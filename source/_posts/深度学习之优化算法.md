---
title: 深度学习之优化算法
top: false
cover: false
toc: true
mathjax: true
date: 2019-11-28 17:00:22
password:
summary:
tags: 
- DL
- python
categories: 深度学习
---



# 一、前言

​	神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化(optimization).

​	使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法(stochastic gradient descent)，简称SGD。 但SGD也有它的缺点，根据不同的问题，也存在比SGD更好的方法。

# 二、SGD

深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.

数学公式
$$
W \leftarrow W - \eta \frac {\partial L}{\partial W}
$$


**代码实现**

```python
class SGD:

    """随机梯度下降法（Stochastic Gradient Descent）"""

    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        """
        更新权重
        :param params: 权重, 字典，params['W1'], ..
        :param grads: 梯度, 字典, grads['w1']
        :return:
        """
        for key in params.keys():
            params[key] -= self.lr * grads[key] 
```



**SGD的缺点**

1. 选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.
2. SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.

​	![](sgd.png)

​	为了改正SGD的缺点，下面我们将使用Momentum、 AdaGrad、Adam这3种方法来取代SGD。

# 三、Momentum

​	Momentum是"动量"的意思。动量方法旨在加速学习，特别是在面对小而连续的梯度但是含有很多噪声的时候。动量模拟了物体运动的惯性，即在更新的时候在一定程度上会考虑之前更新的方向，同时利用当前batch的梯度微调最终的结果。这样则可以在一定程度上增加稳定性，从而更快的学习。

**数学公式**
$$
\upsilon \leftarrow \alpha \upsilon - \eta \frac {\partial L}{\partial W}
$$

$$
W \leftarrow W + \upsilon
$$



**代码实现**

```python
class Momentum:

    """Momentum SGD"""

    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None: # 初始化v
            self.v = {}
            for key, val in params.items():                                
                self.v[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            params[key] += self.v[key]
```



![](mom.png)

​								**基于Momentum的最优化的更新路径**

​	和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。



**特点**

1. 下降初期，使用上一次参数更新，当下降方向一致时能够加速学习。
2. 下降中后期，在局部最小值附近来回震荡，gradient$\rightarrow$0
3. 在梯度改变方向时，能减少更新。
4. 总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。



# 四、AdaGrad

​	在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。

​	在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。

​	和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。

**数学公式**
$$
h \leftarrow h + \frac {\partial L}{\partial W} \cdot \frac {\partial L}{\partial W}
$$

$$
W \leftarrow W - \eta \frac {1}{\sqrt h}\frac {\partial L}{\partial W}
$$

在更新参数时，通过乘以$\frac {1}{\sqrt h}$ ，就可以调整学习的尺度



**代码实现**

```python
class AdaGrad:

    """AdaGrad"""

    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
            
 # 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7
```

​	![](adagrad.png)

​								**基于AdaGrad的最优化的更新路径**

​	由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。

**特点**

1. 前期放大梯度，加速学习，后期约束梯度
2. 适合处理稀疏梯度

**缺点**

​	中后期，分母上梯度的平方的积累将会越来越大，使gradient-->0, 使得训练提前结束。



# 五、RMSProp

RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度。

**数学公式**
$$
h \leftarrow \alpha h + (1-\alpha)\frac {\partial L}{\partial W} \cdot \frac {\partial L}{\partial W}
$$

$$
W \leftarrow W - \eta \frac {1}{\sqrt h}\frac {\partial L}{\partial W}
$$

**代码实现**

```python
class RMSprop:

    """RMSprop"""

    def __init__(self, lr=0.01, decay_rate = 0.99):
        self.lr = lr
        self.decay_rate = decay_rate
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
            
        for key in params.keys():
            self.h[key] *= self.decay_rate
            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```

**优点**

1. 由于采用了梯度平方的指数加权平均，改进了AdaGrad在深度学习中过早结束的问题
2. 适用于处理非平稳过程，-对RNN效果较好





# 六、Adam

​	Adam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正.

​	![](adam.png)

​	Adam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\beta_1$和二次momentum系数$\beta_2$。根据论文，标准的设定值是$\beta_1$为0.9， $\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。

**特点**

1. 结合了AdaGrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
2.  对内存需求较小（指数加权平均不需要存储大量的值就能平均）
3. 为不同的参数计算不同的自适应学习率
4. 适用于大多非凸优化 - 适用于大数据集和高维空间



# 七、不同算法比较

​	![](v1.webp)



![](v2.webp)



1. 如果数据是稀疏的，就用自适应算法, 即AdaGrad, Adadelta, RMSProp, Adam
2. RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。
3. Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。
4. SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。

# 八、参考

  《深度学习入门: 基于Python的理论与实现》

​	https://blog.csdn.net/qq_28031525/article/details/79535942

​	https://www.cnblogs.com/guoyaohua/p/8542554.html