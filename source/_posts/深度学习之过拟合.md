---
title: 深度学习之过拟合
top: false
cover: false
toc: true
mathjax: true
date: 2019-12-29 21:48:28
password:
summary:
tags: 
- DL
- Python
categories: 深度学习
---





# 一、过拟合

机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。

发生过拟合的原因，主要有以下两个：

- 模型拥有大量参数、表现力强（模型太复杂）
- 训练数据少



# 二、如何减少过拟合

## 1、获取更多数据

​	获取更多的数据，从源头上解决问题，比如数据增强



## 2、正则化

​	该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。

**L1正则化**
$$
L_1(\theta) = loss(\theta) + \lambda\sum_{i=1}^n|w_i|
$$
**L2正则化(权值衰减)**
$$
L_2(\theta) = loss(\theta) + \lambda\sum_{i=1}^nw_i^2
$$

- L1减少的是一个常量，L2减少的是权重的固定比例
- L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。
- 实践中L2正则化通常优于L1正则化



## 3、权重初始化

上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习
的方法。通过减小权重参数的值来抑制过拟合的发生。 

如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 

为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始
值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行
相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 

各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 

Xavier：如果前一层的节点数为n，则初始值使用标准差为$\frac{1}{\sqrt n}$的分布 

Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\frac{2}{\sqrt n}$的高斯分布。当Xavier初始值是$\frac{1}{\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 



## 4、Batch Normalization

如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 

为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。

- 可以使学习快速进行（可以增大学习率）。
- 不那么依赖初始值（对于初始值不用那么神经质）。
- 抑制过拟合（降低Dropout等的必要性）。 

![](02.png)

Batch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 

接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用
数学式可以如下表示。 
$$
y_i \leftarrow \gamma x_i + \beta
$$
这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合
适的值。 

## 5、Dropout

​	如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。

![](01.png)

​	Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机
选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递



## 6、早期停止法

我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合
# 参考



https://www.cnblogs.com/skyfsm/p/8456968.html