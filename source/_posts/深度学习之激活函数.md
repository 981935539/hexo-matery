---
title: 深度学习之激活函数
top: false
cover: false
toc: true
mathjax: true
date: 2020-01-18 16:56:27
password:
summary:
tags: 
- DL
categories: 深度学习
---





# 一、为什么要使用激活函数

​	神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比"阶跃函数"更佳平滑.

​	阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。

​	激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。

​	输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。



# 二、常见的激活函数

深度学习是基于人工神经网络的结构，信号经过非线性的激活函数，传递到下一层神经元，经过该层神经元的激活处理后继续往下传递，如此循环往复，直到输出层。正是由于这些非线性函数的反复叠加，才使得神经网络有足够的非线性拟合，选择不同的激活函数将影响整个深度神经网络的效果。下面简单介绍几种常用的激活函数。

## 1、阶跃函数:  

$$
h(x) =\begin{cases}1, & x > 0 \\0, & x \leq 0\end{cases}
$$

​	![](n2.png)





## 2、sigmoid函数（S函数）

$$
h(x) = \dfrac {1}{1+e^{-x}}
$$

![](n1.png)



Sigmoid函数是传统的神经网络和深度学习领域开始时使用频率最高的激活函数，是便于求导的平滑函数，但是容易出现梯度消失问题（gradient vanishing）。函数输出并不是zero-centered，即Sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢，并且所使用的幂运算相对来讲比较耗时。



## 2、tanh函数

$$
h(x) = \dfrac {e^x - e^{-x}}{e^x + e^{-x}}
$$

![](n5.png)





tanh函数解决了zero-centered的输出问题，但梯度消失的问题和幂运算的问题仍然存在。





## 3、Relu函数

$$
h(x) = \{0, x\}
$$

![](n6.png)

ReLU函数本质上是一个取最大值函数，非全区间可导，但是在计算过程中可以取sub-gradient（次梯度）。

ReLU在正区间内解决了梯度消失问题，只需要判断输入是否大于0，所以计算速度非常快，收敛速度远远快于Sigmoid和tanh函数。但ReLU函数的输出同样不是zero-centered，并且存在**Dead ReLU Problem**，即某些神经元可能永远不会参与计算，导致其相应的参数无法被更新。有两个主要原因可能会导致这种情况产生：参数初始化及学习速率太高，从而导致在训练过程中参数更新过大，使网络进入这种情况。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用Adagrad等自动调节学习率的算法。

尽管存在这两个问题，ReLU目前仍是最常用的activation function，**在搭建人工神经网络的时候推荐优先尝试！**

## 4、Leaky ReLU函数

Leaky ReLU函数表达式：
$$
f(x) = max\{ 0.01x, x\}
$$

Leaky ReLU函数示意图:

![](07.png)

Leaky ReLU函数的提出是为了解决Dead ReLU Problem，将ReLU的前半段设为0.01x而非0。理论上来讲，Leaky ReLU函数有ReLU函数的所有优点，外加不会有Dead ReLU问题，但是在实际操作中并没有完全证明Leaky ReLU函数总是好于ReLU函数。



## 5、指数线性单元ELU函数

$$
h(x) =\begin{cases}\alpha(e^x-1), & x \leq 0 \\x, & x > 0\end{cases}
$$

指数线性单元（Exponential Linear Unit）激活函数由Djork等人提出，被证实有较高的噪声鲁棒性，同时能够使得神经元的平均激活均值趋近为0，对噪声更具有鲁棒性。由于需要计算指数，计算量较大。



## 6、SELU函数

SELU函数表达式为:
$$
h(x) =\lambda \begin{cases}\alpha(e^x-1), & x \leq 0 \\x, & x > 0\end{cases}
$$


自归一化神经网络（Self-Normalizing Neural Networks）中提出只需要把激活函数换成SELU就能使得输入在经过一定层数之后变成固定的分布。SELU是给ELU乘上系数λ，即SELU(x)=λ·ELU(x)。



## 7、softmax函数

Softmax函数可视为Sigmoid函数的泛化形式，其本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于(0，1)之间。一般用于多分类神经网络输出。
$$
\sigma(x) =  \dfrac {e^{a_k}}{ \sum_{i=1}^n e^{a^i}   }
$$
**注意**: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。



改进**: 先进行归一化，再求值

```python
a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a))
#  array([nan, nan, nan])  没有计算正确的值

mi = np.min(a)     # 990                                 
ma = np.max(a)                                 
nor = (a-mi)/(ma-mi)    # 归一化 array([1. , 0.5, 0. ])   
np.exp(nor)/np.sum(np.exp(nor))                 
# array([0.50648039, 0.30719589, 0.18632372])

```

​	一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此**输出层的softmax函数一般会被省略**

​	求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系