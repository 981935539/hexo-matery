{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/matery/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/matery/source/css/gitment.css","path":"css/gitment.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/matery.css","path":"css/matery.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/my.css","path":"css/my.css","modified":1,"renderable":1},{"_id":"themes/matery/source/js/matery.js","path":"js/matery.js","modified":1,"renderable":1},{"_id":"themes/matery/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/logo.png","path":"medias/logo.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/explosion.min.js","path":"libs/others/explosion.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/fireworks.js","path":"libs/others/fireworks.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/snow.js","path":"libs/others/snow.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/text.js","path":"libs/others/text.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/avatar.jpg","path":"medias/avatars/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/babyq.png","path":"medias/avatars/babyq.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/cww97.jpg","path":"medias/avatars/cww97.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/fun4go.png","path":"medias/avatars/fun4go.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/hael.jpg","path":"medias/avatars/hael.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/huaji.jpg","path":"medias/avatars/huaji.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/hzwer.jpg","path":"medias/avatars/hzwer.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/ids2.jpg","path":"medias/avatars/ids2.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/kewlgrl.jpg","path":"medias/avatars/kewlgrl.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/ldy.jpg","path":"medias/avatars/ldy.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/liyucheng.jpg","path":"medias/avatars/liyucheng.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/michael.jpg","path":"medias/avatars/michael.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/mouse.jpg","path":"medias/avatars/mouse.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/myzhihu.png","path":"medias/avatars/myzhihu.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/mpy634.png","path":"medias/avatars/mpy634.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/spacesac.png","path":"medias/avatars/spacesac.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/qiqiang.jpg","path":"medias/avatars/qiqiang.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/sunchangzhi.jpg","path":"medias/avatars/sunchangzhi.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/taowei.jpg","path":"medias/avatars/taowei.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/zhaokangzhe.jpg","path":"medias/avatars/zhaokangzhe.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/reward/alipay.jpg","path":"medias/reward/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jquery/jquery-2.2.0.min.js","path":"libs/jquery/jquery-2.2.0.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/ACE.jpg","path":"medias/avatars/ACE.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/antnlp.ico","path":"medias/avatars/antnlp.ico","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/duyupei.jpg","path":"medias/avatars/duyupei.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/gsy.jpg","path":"medias/avatars/gsy.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/liyangzone.jpg","path":"medias/avatars/liyangzone.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/lijiaqian.png","path":"medias/avatars/lijiaqian.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/qiandongwei.jpg","path":"medias/avatars/qiandongwei.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/zhangting.jpg","path":"medias/avatars/zhangting.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/xuzhongyou.jpg","path":"medias/avatars/xuzhongyou.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/zzw.jpg","path":"medias/avatars/zzw.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/css/font-awesome.min.css","path":"libs/awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/0xbird.png","path":"medias/avatars/0xbird.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/milyyy.jpg","path":"medias/avatars/milyyy.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/mizunashi.png","path":"medias/avatars/mizunashi.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/qianqian.png","path":"medias/avatars/qianqian.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/fly.jpeg","path":"medias/music/avatars/fly.jpeg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/guimixinqiao.jpg","path":"medias/music/avatars/guimixinqiao.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/meixue.jpg","path":"medias/music/avatars/meixue.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/slience.jpg","path":"medias/music/avatars/slience.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/tiantangdemogui.jpg","path":"medias/music/avatars/tiantangdemogui.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/yequ.jpg","path":"medias/music/avatars/yequ.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/yiluxiangbei.jpg","path":"medias/music/avatars/yiluxiangbei.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff","path":"libs/awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff2","path":"libs/awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/jitao.jpg","path":"medias/avatars/jitao.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/lyn-draw.jpg","path":"medias/avatars/lyn-draw.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/mashiro.jpg","path":"medias/avatars/mashiro.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/lzh.png","path":"medias/avatars/lzh.png","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/25.jpg","path":"medias/featureimages/25.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/26.jpg","path":"medias/featureimages/26.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.ttf","path":"libs/awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/24.jpg","path":"medias/featureimages/24.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/FontAwesome.otf","path":"libs/awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.eot","path":"libs/awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/zhangyi.jpg","path":"medias/avatars/zhangyi.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/avatars/daoshu.jpg","path":"medias/music/avatars/daoshu.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/27.jpg","path":"medias/featureimages/27.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatars/jingjing.jpg","path":"medias/avatars/jingjing.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.svg","path":"libs/awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/guimixinqiao.mp3","path":"medias/music/guimixinqiao.mp3","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/Silence.mp3","path":"medias/music/Silence.mp3","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/fly.mp3","path":"medias/music/fly.mp3","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/music/yinlong.mp3","path":"medias/music/yinlong.mp3","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/video/Alizée - La isla bonita (live).mp4","path":"medias/video/Alizée - La isla bonita (live).mp4","modified":1,"renderable":1}],"Cache":[{"_id":"source/404.md","hash":"d97f69ff63501de89cfd341c68e4d6ed5c8a5b3a","modified":1574068345929},{"_id":"source/CNAME","hash":"a65644fbc0f44ca357a50723195d8873e10e7ae7","modified":1573753355544},{"_id":"themes/matery/.gitignore","hash":"eaa3d84cb77d92a21b111fd1e37f53edc1ff9de0","modified":1569474790000},{"_id":"themes/matery/LICENSE","hash":"b314c7ebb7d599944981908b7f3ed33a30e78f3a","modified":1574068345941},{"_id":"themes/matery/README_CN.md","hash":"a94324950e0299bcfcbc106cf2ca65c93e1fe843","modified":1574068345944},{"_id":"themes/matery/_config.yml","hash":"bbfb663b8839d043cd5f4b351470be470a10c363","modified":1574068345945},{"_id":"themes/matery/README.md","hash":"7ef16198a2c5ff580f006582286354caf160c7fe","modified":1574068345942},{"_id":"source/_data/friends.json","hash":"cd94171c87c02b0bccb4a0d3fcc4be8f50c883d0","modified":1574068345930},{"_id":"source/_data/musics.json","hash":"9d26b416b0bc08d24deab6786c677a957f1f5beb","modified":1574068345931},{"_id":"source/_posts/CMake用法总结.md","hash":"18f3bae1d570b2e7be8f216052422f6ee838cdbf","modified":1575276786517},{"_id":"source/_posts/Pyhton高数计算库.md","hash":"bd77c6f6d63f635eeac97949d651e87115ea63da","modified":1575276786517},{"_id":"source/_posts/Hexo-Github博客搭建.md","hash":"f284e2902357ffe96b21c17be01c5da539236cea","modified":1574068345933},{"_id":"source/_posts/Python包管理.md","hash":"cdc4a55ed6969c315c1ad5ddcd641a29a1185fff","modified":1575276786518},{"_id":"source/_posts/Python中with的用法.md","hash":"40d69567d91148c4d1db6a2c7a81cf213b9ddc95","modified":1575276786518},{"_id":"source/_posts/数据结构与算法之树.md","hash":"33505b8037c67cf57b68adf84eae9d9ed48380cf","modified":1574532406889},{"_id":"source/_posts/机器学习之评估指标.md","hash":"ca4aa36c50cdae30c0155c45fc8cb981a13deca3","modified":1578138581659},{"_id":"source/_posts/机器学习概述.md","hash":"f8a68d345b2904154d78dd57e9c4745933734998","modified":1575276786519},{"_id":"source/_posts/深度学习之CNN模型演化.md","hash":"01581c39e06daeefee6bd03f55ef5e6991dbbc26","modified":1578417650854},{"_id":"source/_posts/深度学习之优化算法.md","hash":"019a800940a3011d7594f2e633dbe2180f26f7f9","modified":1575271524184},{"_id":"source/_posts/深度学习之卷积神经网络.md","hash":"334609fd223819d418aa7fe5d1552b9c6bf20b65","modified":1578302112561},{"_id":"source/_posts/深度学习之超参数.md","hash":"d36512226f5c34e963ff834b6a54041ccbdda9a0","modified":1577720920922},{"_id":"source/_posts/深度学习之过拟合.md","hash":"bf177227faac28915fd6e24277f6cf72859d8aea","modified":1577637401571},{"_id":"source/_posts/神经网络简介.md","hash":"7ec034f5da1f82fc81312bf0020e501b578ea4e8","modified":1578229502387},{"_id":"source/about/index.md","hash":"5db2abfcfe88a2ad589afe148659084d90efa73f","modified":1574068345934},{"_id":"source/categories/index.md","hash":"76889deb16e0d61d5c585f26a0e69f01de6cab74","modified":1574068345937},{"_id":"source/archives/index.md","hash":"a62b7d9b8a8bdf966ec5c823e71581d2b185156e","modified":1574068345935},{"_id":"source/contact/index.md","hash":"b105cb6d9ad6231edd92336898ebe7bc540d8141","modified":1574068345938},{"_id":"source/tags/index.md","hash":"98697833897097a5c65521ea02baa1c6f85948d8","modified":1574068345940},{"_id":"source/friends/index.md","hash":"04d400fe15d4bf65875181717bedde844a12d5cb","modified":1574068345939},{"_id":"themes/matery/languages/default.yml","hash":"527c795b8c41fe62bf35603ffebfa6d4a7929a2c","modified":1574068345954},{"_id":"themes/matery/languages/zh-CN.yml","hash":"d92db4b986bb6f0d228e9a8249383103bf56342d","modified":1574068345955},{"_id":"themes/matery/layout/404.ejs","hash":"f08a0f507b36f3652520a41381f71167488405c7","modified":1574068345957},{"_id":"themes/matery/layout/about.ejs","hash":"e87752e59f021b5139b1155a264da11ab469a9aa","modified":1574068346003},{"_id":"themes/matery/layout/categories.ejs","hash":"c431e772d0f7700592228bbd9502793bdc28a893","modified":1574068346005},{"_id":"themes/matery/layout/archive.ejs","hash":"1b5023571894404d75caffa28128fc9c49f9095d","modified":1574068346004},{"_id":"themes/matery/layout/category.ejs","hash":"2d421e10c3b8fd2c4f725e5eaa967c4a1429c707","modified":1574068346006},{"_id":"themes/matery/layout/friends.ejs","hash":"b9b75ff45324da5b4ddbb6e16ea1ecd239dbc310","modified":1574068346008},{"_id":"themes/matery/layout/contact.ejs","hash":"1513c5a40b7cc0b6e5854cf8c3253958bcb486cb","modified":1574068346007},{"_id":"themes/matery/layout/index.ejs","hash":"7fc5a6c4f0229c0be43b7d1315524c468346fbb8","modified":1574068346009},{"_id":"themes/matery/layout/layout.ejs","hash":"2ba4110dc596424b1220a259c8e594da774e7f59","modified":1574068346010},{"_id":"themes/matery/layout/tag.ejs","hash":"5cdf3a1d72f54285ee9cb826fd0e4a0449093215","modified":1574068346012},{"_id":"themes/matery/layout/post.ejs","hash":"9ecae79690293cacdccc172118f1fe481705b1f2","modified":1574068346011},{"_id":"themes/matery/layout/tags.ejs","hash":"851c0ee599e91e7b1d657673859e8b6ff79cf50b","modified":1574068346013},{"_id":"themes/matery/source/favicon.png","hash":"c390f8e9f454148ee2d2c116b7c11d6fca7f88a7","modified":1574068346019},{"_id":"source/_posts/C和C++语法总结.md","hash":"51d1b52a0c32049e48b41955fb7247c5af6b983d","modified":1578155859707},{"_id":"source/_posts/C和C++语法总结/03.png","hash":"aeae02a80ab62faca162cca97e19857a086a0581","modified":1577585951922},{"_id":"source/_posts/C和C++语法总结/04.png","hash":"c839b59f41c5de7d7f9f6257a77997ffe8af802d","modified":1577585994245},{"_id":"source/_posts/C和C++语法总结/06.png","hash":"46b0d3868e2fdba67890f8bad5fd28b685e2757d","modified":1578147965669},{"_id":"source/_posts/数据结构与算法之树/完全二叉树.png","hash":"2d6d292d04291be947a93bd2d4aeb2446b7d6222","modified":1496016800000},{"_id":"source/_posts/数据结构与算法之树/parents.png","hash":"907702c5614b221e64b77d0c26aacc9720e22118","modified":1574496278672},{"_id":"source/_posts/数据结构与算法之树/满二叉树.png","hash":"d02a4692cfb90a30bd4fa94770245e7ff3b01db4","modified":1496016800000},{"_id":"source/_posts/机器学习之评估指标/02.jpg","hash":"779fb969659273ed59e97f93a1bc877250813136","modified":1578125893578},{"_id":"source/_posts/机器学习之评估指标/02.png","hash":"37077bad2f33b918ca0a666fb2ac0a1eeb255d03","modified":1578128390483},{"_id":"source/_posts/机器学习之评估指标/混淆矩阵.png","hash":"1eba56ebb014117ea738613a850a22109974d3cf","modified":1522884897723},{"_id":"source/_posts/机器学习概述/2.jpeg","hash":"173092b1a6a76eaa5e4fb5ad9087f7b8b9120494","modified":1575276786533},{"_id":"source/_posts/机器学习概述/4.jpg","hash":"e0b942360b530eef92eec86d45981cab9e5bfd1a","modified":1575276786535},{"_id":"source/_posts/机器学习概述/3.jpg","hash":"f13f9b3b80010785906a6ebe6f9399afd356a0d9","modified":1575276786534},{"_id":"source/_posts/机器学习概述/6.jpg","hash":"0e1a65fcdd971dbe3fe9741c19d392b70600300b","modified":1575276786538},{"_id":"source/_posts/机器学习概述/7.png","hash":"1eba56ebb014117ea738613a850a22109974d3cf","modified":1575276786539},{"_id":"source/_posts/深度学习之CNN模型演化/c5.png","hash":"83578f86d3c96f5471e06c614cf7fb071753194b","modified":1575271524178},{"_id":"source/_posts/深度学习之CNN模型演化/c7.png","hash":"eb727b1d269affe30df31c4a9d055a5aa1e58501","modified":1575271524181},{"_id":"source/_posts/深度学习之CNN模型演化/x2.png","hash":"7c7a09ccc0030c7d22aa3b9c338f1722593f3ee8","modified":1578413672971},{"_id":"source/_posts/深度学习之CNN模型演化/x5.png","hash":"3e7b766ad7f6c668b9043f245aeff25e83d3e84a","modified":1578414159463},{"_id":"source/_posts/深度学习之CNN模型演化/x6.png","hash":"617c27344814142f5490b3aef508df9f48da11e0","modified":1578414212468},{"_id":"source/_posts/深度学习之卷积神经网络/01.png","hash":"4dca16a8c90dd842b02a292bf3d6b26f9fc08568","modified":1578290097025},{"_id":"source/_posts/深度学习之卷积神经网络/03.png","hash":"e48fa9452ddb4daac4b46373bbbafd01178f9068","modified":1578291183884},{"_id":"source/_posts/深度学习之过拟合/02.png","hash":"71786bdbed3727cef0e7028e8af524c30aa02378","modified":1577637124925},{"_id":"source/_posts/神经网络简介/b3.png","hash":"3c4fa914d21f51c4f10a9b441170300d158ab7b3","modified":1575276786567},{"_id":"source/_posts/神经网络简介/b7.png","hash":"036cf59a22c91fb52cd71b0e9bc8b15580b3afc4","modified":1575276786573},{"_id":"source/_posts/神经网络简介/n2.png","hash":"3502bf8a9ac8a3bcfbb00fae94dfce1e8f80692d","modified":1575276786575},{"_id":"source/_posts/神经网络简介/n3.png","hash":"d9272b300fab6c901ea489eb79497c3f658eade6","modified":1575276786575},{"_id":"source/_posts/神经网络简介/n4.png","hash":"35e198e9b5d0f00bab3650d300d9b39c40d04d63","modified":1575276786576},{"_id":"source/_posts/神经网络简介/n6.png","hash":"b97ab82cdc9465e2b6635957fec4bba17b20cc34","modified":1575276786578},{"_id":"source/_posts/神经网络简介/n8.png","hash":"008ae53509146a013956ab5897d643f4ac4f1097","modified":1575276786581},{"_id":"source/_posts/神经网络简介/p2.png","hash":"ca1e929ac5952ab33cab736d7874095296866f0d","modified":1575276786582},{"_id":"source/_posts/神经网络简介/p1.png","hash":"7533bd14e06c3b57533a5a0f7f0a5b1c8509e982","modified":1575276786581},{"_id":"source/_posts/神经网络简介/p3.png","hash":"658997c8847a75b37a29776ce55401ff0168a892","modified":1575276786582},{"_id":"source/_posts/神经网络简介/p4.png","hash":"6a7acfd4673930c5a3f4829702c7cbbd5ece1354","modified":1575276786583},{"_id":"source/_posts/神经网络简介/p5.png","hash":"2b6f16c1753169312dfc3f48d338994f5c911454","modified":1575276786584},{"_id":"source/_posts/神经网络简介/p6.png","hash":"3f070cca640255dd80630401af5aeba929dc01d8","modified":1575276786584},{"_id":"themes/matery/layout/_partial/back-top.ejs","hash":"cb99dc352397ec5d0765794d7b8884972e61973b","modified":1574068345958},{"_id":"themes/matery/layout/_partial/bg-cover-content.ejs","hash":"6bf708dbd705b486bc464e9be8e8834bbd692850","modified":1574068345959},{"_id":"themes/matery/layout/_partial/bg-cover.ejs","hash":"d5a7b9bb96e04c0a3485dd873748f19c50a6a04f","modified":1574068345959},{"_id":"themes/matery/layout/_partial/disqus.ejs","hash":"42dda8e67f7f09d148347887e52f18aea546df26","modified":1574068345961},{"_id":"themes/matery/layout/_partial/footer.ejs","hash":"7a155149fbafc55c178ee709677ed24b2da4dc8e","modified":1574068345962},{"_id":"themes/matery/layout/_partial/gitalk.ejs","hash":"a3a140e6aeeb6f289e4b821a577ef548267f3de1","modified":1574068345963},{"_id":"themes/matery/layout/_partial/gitment.ejs","hash":"d8c40dbc8106b5bc53ceb727ad968c1d8f234261","modified":1574068345965},{"_id":"themes/matery/layout/_partial/github-link.ejs","hash":"fd4034bca2eb3987dcf113e6477260bee97eb1e7","modified":1574068345964},{"_id":"themes/matery/layout/_partial/google-analytics.ejs","hash":"890c8f04c1f4905dfceb3ea9fd6efdd040d79c01","modified":1574068345966},{"_id":"themes/matery/layout/_partial/head.ejs","hash":"764d20ae433f558e81249ff0c8105ac53e6d3f59","modified":1574068345967},{"_id":"themes/matery/layout/_partial/header.ejs","hash":"821e1af65990521c9e0288178d8e5b18c73a9cab","modified":1574068345969},{"_id":"themes/matery/layout/_partial/index-cover.ejs","hash":"d4042e5521ceb5f3255cd4455ac7ccd227fee6df","modified":1574068345970},{"_id":"themes/matery/layout/_partial/livere.ejs","hash":"42728561c09589f79b698eb059ab4def53ed3642","modified":1574068345971},{"_id":"themes/matery/layout/_partial/mobile-nav.ejs","hash":"e761f0104fbf431671bbe6bebc91ca82f737f4d2","modified":1574068345972},{"_id":"themes/matery/layout/_partial/navigation.ejs","hash":"3a82fcb6f31d69971cb564985842c14ac02cdca0","modified":1574068345973},{"_id":"themes/matery/layout/_partial/paging.ejs","hash":"dfdeea9c59d157acb851d4bf44bf95f81787523c","modified":1574068345975},{"_id":"themes/matery/layout/_partial/post-cover.ejs","hash":"166c0b9753f3f913bd801e82ad5b268004be198d","modified":1574068345976},{"_id":"themes/matery/layout/_partial/post-detail.ejs","hash":"3f208f33e4e12becdb8323e6e64e20ad60c3fb2a","modified":1574068345978},{"_id":"themes/matery/layout/_partial/post-detail-toc.ejs","hash":"82cb8090cde663fa7ad67418a802997b3057e957","modified":1574068345977},{"_id":"themes/matery/layout/_partial/post-statis.ejs","hash":"3b42900247d5ea4ea5b68e2be44420a0d54785ad","modified":1574068345980},{"_id":"themes/matery/layout/_partial/prev-next.ejs","hash":"4e73f10eacb5d00a0681cb44fe5c039cd8ab03cd","modified":1574068345981},{"_id":"themes/matery/layout/_partial/reprint-statement.ejs","hash":"f85a222ec3f9bc27eb7978015e63a16514b38791","modified":1574068345982},{"_id":"themes/matery/layout/_partial/reward.ejs","hash":"73624d9db81e87ff0c12310bb873fbd0b5221021","modified":1574068345983},{"_id":"themes/matery/layout/_partial/search.ejs","hash":"e859fe6e0259e0c123cb7ceda6e4cac836318ffc","modified":1574068345984},{"_id":"themes/matery/layout/_partial/share.ejs","hash":"0f2e1e27d21492cf228e786daead985b1e1dcea4","modified":1574068345985},{"_id":"themes/matery/layout/_partial/social-link.ejs","hash":"55272fab7a3303e94f1839ebd7ca6cf9965fb328","modified":1574068345986},{"_id":"themes/matery/layout/_partial/valine.ejs","hash":"c3039180ddb2eb17e724b8441e5f93e79859aef7","modified":1574068345987},{"_id":"themes/matery/layout/_widget/category-cloud.ejs","hash":"b2b22d4fc4e46b051f67216c391f629f4ff552b5","modified":1574068345989},{"_id":"themes/matery/layout/_widget/category-radar.ejs","hash":"5284712d84bbaa4f0d88026ac3ec5a8c13e00056","modified":1574068345990},{"_id":"themes/matery/layout/_widget/dream.ejs","hash":"2af85ddef6e61a44cbe10e8f6272a8324681ee3f","modified":1574068345991},{"_id":"themes/matery/layout/_widget/music.ejs","hash":"fc50cb4bbc1f4d0e4c9f5941f1c3c74bea742db7","modified":1574068345992},{"_id":"themes/matery/layout/_widget/my-gallery.ejs","hash":"9ea672db65f1e5b8fad1ffafb1614f25adc97e63","modified":1574068345993},{"_id":"themes/matery/layout/_widget/my-projects.ejs","hash":"785cb588a31215876f6737213054ba0e8552fff0","modified":1574068345994},{"_id":"themes/matery/layout/_widget/my-skills.ejs","hash":"c6f713316ce75ad08ac5d1587bd8ce42e894e9ae","modified":1574068345995},{"_id":"themes/matery/layout/_widget/post-calendar.ejs","hash":"4608af6151f0e32f668c89f09343748340021478","modified":1574068345996},{"_id":"themes/matery/layout/_widget/post-charts.ejs","hash":"0aaf0a111b9aa07ff37f6286eeac5506283f47f8","modified":1574068345998},{"_id":"themes/matery/layout/_widget/recommend.ejs","hash":"d439d86818de179d64965d4f7f5fa56147fd9221","modified":1574068345999},{"_id":"themes/matery/layout/_widget/tag-cloud.ejs","hash":"6310903eb0e434d6f9a59ca669aab7fae38d4797","modified":1574068346000},{"_id":"themes/matery/layout/_widget/tag-wordcloud.ejs","hash":"bf604fe9c435f0fb9a559cac9c35772579b590e8","modified":1574068346001},{"_id":"themes/matery/layout/_widget/video.ejs","hash":"05f5e2acace5730cdf7bed650375ad88f6b5d1b7","modified":1574068346002},{"_id":"themes/matery/source/css/gitment.css","hash":"d5ef623065d1fbc897119f7b70ccf7563e329917","modified":1574068346015},{"_id":"themes/matery/source/css/matery.css","hash":"b92bd5093424c4cc62f37175f1b0a54c4611a802","modified":1574068346016},{"_id":"themes/matery/source/css/my-gitalk.css","hash":"4e3e855767ac5a48b13af1d6a42df13d8975e03f","modified":1574068346017},{"_id":"themes/matery/source/css/my.css","hash":"37683a9f11c68903a53e2b8593ca8c095a721896","modified":1574068346018},{"_id":"themes/matery/source/js/matery.js","hash":"208b7806caa943c115aa0825c9c72a0781404775","modified":1574068346020},{"_id":"themes/matery/source/js/search.js","hash":"77ecae23dd3edd8ad962c5b12954652bb2f7a1b6","modified":1574068346022},{"_id":"themes/matery/source/medias/logo.png","hash":"870634d4c96a7f0b2cac0511b10820f120d308f4","modified":1574068346090},{"_id":"source/_posts/机器学习之评估指标/01.jpg","hash":"95a0394e61fbb984e06a27c570e7f4b905a6d79e","modified":1578125067443},{"_id":"source/_posts/深度学习之CNN模型演化/09.png","hash":"0f0844b8e45f5bb0b6a7fa6369b4fa1f8f30127f","modified":1578369063986},{"_id":"source/_posts/深度学习之CNN模型演化/c4.png","hash":"4e9bc1aab682d3f025a1bb3e24c08717681fce0b","modified":1575271524177},{"_id":"source/_posts/深度学习之CNN模型演化/v1.png","hash":"952e699dee413ed85038b851afb1ee3f963607b7","modified":1578398019297},{"_id":"source/_posts/深度学习之CNN模型演化/x1.png","hash":"3e34d37d12f9d30cea508396e4a57dc620988d83","modified":1578413644117},{"_id":"source/_posts/深度学习之CNN模型演化/x4.png","hash":"85100fb7cb45eaa8c0dd710995c564c2d4d1cf6e","modified":1578413715225},{"_id":"source/_posts/深度学习之CNN模型演化/x3.png","hash":"ca6d1b606f55b113735de34abecc208b84d0ccc1","modified":1578413695264},{"_id":"source/_posts/深度学习之优化算法/adagrad.png","hash":"fefc823290dd8d812402ee15ce760300396397ed","modified":1575276786541},{"_id":"source/_posts/深度学习之优化算法/adam.png","hash":"290c511ec704b78be7046f8aedcf360fe2590466","modified":1575276786542},{"_id":"source/_posts/深度学习之优化算法/mom.png","hash":"2ffa733e3350dc5c9aa82ec6bdc337e725e359a1","modified":1575276786543},{"_id":"source/_posts/深度学习之优化算法/sgd.png","hash":"448ccc4a31140723215f1e19c1f6f22459e50da3","modified":1575276786544},{"_id":"source/_posts/深度学习之卷积神经网络/02.png","hash":"712724b5b38f164bfc77c7824906ac0f53e246e1","modified":1578291070595},{"_id":"source/_posts/深度学习之卷积神经网络/04.png","hash":"e0a98854339f95d217d258632e97e7efeae44d9a","modified":1578291402546},{"_id":"source/_posts/深度学习之卷积神经网络/05.png","hash":"8d5444b2830bd5a22b2da630cb989879782680ed","modified":1578291710297},{"_id":"source/_posts/深度学习之卷积神经网络/07.png","hash":"eee27e8efac5424cc2d7e1b8f9b81c0bf325990a","modified":1578292397220},{"_id":"source/_posts/深度学习之卷积神经网络/09.png","hash":"e2f6608a155f878109acfc6a5a7c09f7648ad1ef","modified":1578297600993},{"_id":"source/_posts/深度学习之卷积神经网络/08.png","hash":"5d26ad0e6b1c14c945e05997da1a13858ad181bd","modified":1578295751996},{"_id":"source/_posts/深度学习之超参数/02.png","hash":"55dd713300a05bb6012cfb2fc7a4020e28e5c2be","modified":1577709081377},{"_id":"source/_posts/神经网络简介/b1.png","hash":"47d1f5d99bb05129afe28f5cb2f34cb4c2090447","modified":1575276786565},{"_id":"source/_posts/神经网络简介/b4.png","hash":"78705f41b20e6f7b79ee8cdad0dafe2e8b77c3d7","modified":1575276786569},{"_id":"source/_posts/神经网络简介/b6.png","hash":"11359ca35005106fda3be6e9b4f0dc3801d759cc","modified":1575276786572},{"_id":"source/_posts/神经网络简介/n1.png","hash":"c5047ddc9239d1f5e04c5f9715288c0ed8590af0","modified":1575276786574},{"_id":"source/_posts/神经网络简介/p7.png","hash":"b76424f96dd4dd408abec21d32dc0f9b0e203bc1","modified":1575276786585},{"_id":"source/_posts/Hexo-Github博客搭建/2.png","hash":"fdd7232fa05654f710600819ee48222046e4d3b2","modified":1573760775976},{"_id":"source/_posts/机器学习概述/1.png","hash":"a66f7bf41a0ea2de65c7c1545e65bcdbb2b7ec7b","modified":1575276786520},{"_id":"source/_posts/深度学习之CNN模型演化/c3.png","hash":"ce5ad96d7bb50611e433675bceac8a2bc3dabce1","modified":1575271524176},{"_id":"source/_posts/深度学习之CNN模型演化/c6.png","hash":"0290feffed0d2beec8dc7fc663d8eee6afcd8b18","modified":1575271524180},{"_id":"source/_posts/深度学习之CNN模型演化/v13.png","hash":"91b33f8cdc3bb1cc2d702c573fdfb9a5f604c092","modified":1578402114441},{"_id":"source/_posts/深度学习之CNN模型演化/v3.png","hash":"9f3f31a4cc2db5a736c8b274f7a07262b6ebfd70","modified":1578398075378},{"_id":"source/_posts/深度学习之CNN模型演化/v4.png","hash":"c66535b3f16b45e0bd66993d1c26c16a77d5630c","modified":1578399350034},{"_id":"source/_posts/深度学习之CNN模型演化/v2.png","hash":"6df07d2bf4ce2d33ab0f4d4e620e3417b2bf3f27","modified":1578398053449},{"_id":"source/_posts/深度学习之CNN模型演化/v5.png","hash":"fe42fc395ad15990d4d86f1ea2db311b03433578","modified":1578399381365},{"_id":"source/_posts/深度学习之CNN模型演化/v6.png","hash":"cf07083c672516eaa6d539334319dba65f0d2880","modified":1578399514350},{"_id":"source/_posts/深度学习之CNN模型演化/v7.png","hash":"0bd8e6dda03817ec93dd2f1ec2d033a537e847fd","modified":1578399865664},{"_id":"source/_posts/深度学习之CNN模型演化/v8.png","hash":"4dba23f203e557ebe2a16f36cb7169c3b74e26ab","modified":1578399885952},{"_id":"source/_posts/深度学习之CNN模型演化/v9.png","hash":"6b5986768f253a27b3c986b423eddc08655a62bb","modified":1578399907732},{"_id":"source/_posts/深度学习之超参数/01.png","hash":"f7e6239e05819b9626dd4ba9033dd95abcfa3680","modified":1577709017933},{"_id":"source/_posts/神经网络简介/n5.png","hash":"331a4f4d9db15f42e443c88301a0b38aa1227683","modified":1575276786577},{"_id":"themes/matery/source/libs/animate/animate.min.css","hash":"5dfcbcee866e9dc564916416281885f3e320871e","modified":1574068346023},{"_id":"themes/matery/source/libs/aos/aos.js","hash":"5a8e6d07ffa55642418ab3fd4b263aa08284b77a","modified":1574068346025},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","hash":"7f4f8913f2d46ade2def5134e2cc8684a4b87939","modified":1574068346026},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","hash":"33810b2b757fc4327bc1d3b83bb5e0d3dc1fec5b","modified":1574068346034},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","hash":"5d52d3b34fceb9d7e11f1beaf7ed380b4249dec4","modified":1574068346035},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","hash":"021898a16279ac2ffe75af4f902fab2a0a39f11a","modified":1574068346043},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","hash":"a0625d8b432af8bdc820f8768d36cde439e7257c","modified":1574068346046},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"26849509f196a2d21bbfd15696e5d5153163b8f1","modified":1574068346048},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","hash":"4e6538c8312aeeab845d361c37a8c1a0931241f0","modified":1574068346049},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","hash":"f81cd7bfcf7aa2d043bd3e6077df42656fc44b82","modified":1574068346054},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1569474790000},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1569474790000},{"_id":"themes/matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1569474790000},{"_id":"themes/matery/source/libs/others/explosion.min.js","hash":"5b76fa72a85cfb27d54b00128393ece773d65386","modified":1574068346059},{"_id":"themes/matery/source/libs/others/fireworks.js","hash":"e9c74f2dd3953d4d8dec44e9977574d00702e84d","modified":1574068346060},{"_id":"themes/matery/source/libs/others/snow.js","hash":"b393f069781eef788a0ae66b2681cece8fea2851","modified":1574068346061},{"_id":"themes/matery/source/libs/others/text.js","hash":"fdf18f65977e4bc358dfb5fb0b7c98492ae72efd","modified":1574068346062},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","hash":"f646f2bb75bcd1eb65b2788ac7bf15d4fd243ce9","modified":1574068346065},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/avatar.jpg","hash":"f6e148b65284c04f5989197691f62b686d26796d","modified":1574068346087},{"_id":"themes/matery/source/medias/avatars/babyq.png","hash":"be5432588003e5a52c02e690622eec72b5f7346c","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/cww97.jpg","hash":"6af987cafc55d8d031534dd5e0f722fff19f70ec","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/fun4go.png","hash":"0f4333973a972a629cfbabf601bc7c192b65376c","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/hael.jpg","hash":"e66ccedab38bb2e8fc45fac024e234ab8e7b9d54","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/huaji.jpg","hash":"86be7eed2a491455ccfe3e7da46366ff477765ca","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/hzwer.jpg","hash":"53a66bb5e65d2abd5b7412edf094c1e0b1094492","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/ids2.jpg","hash":"2c8d3ac6ab5ac6196bac83766fde975daca91c32","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/kewlgrl.jpg","hash":"3af0fd1029a1511bb3c0e90871e41b35e714b01f","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/ldy.jpg","hash":"906ef214d1f2fe52a663738340ad5623f826bd82","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/liyucheng.jpg","hash":"12055a27fa667c87d2319475968056e1a8ad0f08","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/michael.jpg","hash":"331a2ab20c299196f5a3089b8445fc8f55346cb6","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/mouse.jpg","hash":"2eae273885b9859150a1f98f74b3df12ca9a207c","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/myzhihu.png","hash":"ef2bcf774e4068d66aefdb9d7fe2293b88060eab","modified":1574068346089},{"_id":"themes/matery/source/medias/avatars/mpy634.png","hash":"30f88e09c02b37c2dc684d4ee3237e327bb23f8b","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/spacesac.png","hash":"ff1bdb058f1f0499312da1a082ba97d78590db1a","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/qiqiang.jpg","hash":"081459866f922d9558a88cd4d7155d91fa730322","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/sunchangzhi.jpg","hash":"bbe2a15fd474ab62dbd14fea72deb1113a4fb005","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/taowei.jpg","hash":"e58b03b70656aa7a27238be38dac3896d9d16f10","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/zhaokangzhe.jpg","hash":"c8242bd13f08a9ddb97e26f216bc729b12ed9058","modified":1569474790000},{"_id":"themes/matery/source/medias/reward/alipay.jpg","hash":"3523f42e0c7cce1585f1243000008177320e073c","modified":1574068346262},{"_id":"source/_posts/Hexo-Github博客搭建/1.png","hash":"6c41f7bc3663f2dbef648b18777bf24fef8470d9","modified":1573750846862},{"_id":"source/_posts/深度学习之CNN模型演化/c2.png","hash":"3ca9d480d07ea517b19b8177f202b1f0c1aef8c7","modified":1575271524174},{"_id":"source/_posts/深度学习之CNN模型演化/v10.png","hash":"c3a7f895567e7059157c9260daaec60390526c9d","modified":1578400399439},{"_id":"source/_posts/深度学习之CNN模型演化/v12.png","hash":"2847cacce2f2bce82dd1c319fbb4e13473961738","modified":1578402095604},{"_id":"source/_posts/深度学习之卷积神经网络/06.png","hash":"4edea7ccb24bd4bfde4b60461e5a3b5193956536","modified":1578292016201},{"_id":"source/_posts/神经网络简介/b2.png","hash":"706b48bb59935a4576148354f1dd07ea71cc5f37","modified":1575276786567},{"_id":"themes/matery/source/libs/aos/aos.css","hash":"ded9739f803d114c9168d3351fded72b3b478b4c","modified":1574068346024},{"_id":"themes/matery/source/libs/gitment/gitment.js","hash":"5a13983930b019450e4fe01a407c64b3dd316be4","modified":1574068346047},{"_id":"themes/matery/source/libs/jquery/jquery-2.2.0.min.js","hash":"7a551393b8360731104fdef1af36a6f3638f5855","modified":1574068346051},{"_id":"themes/matery/source/medias/avatars/ACE.jpg","hash":"eda5008f174b2e074c415459f061328707e96da9","modified":1574068346085},{"_id":"themes/matery/source/medias/avatars/antnlp.ico","hash":"29475f350b989331cebd702a315f020917d06ed8","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/duyupei.jpg","hash":"3c02ed4cf57dc37e4f4b8314bf5094833a854cb0","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/gsy.jpg","hash":"6a175e2ba56a2280d40a2e654b559be41c3a0a48","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/liyangzone.jpg","hash":"febab557e4c0d859ab4cc14b57d8106f5e3fccfb","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/lijiaqian.png","hash":"9d96b3838acfae9a23b6e290fcfafceff0419c63","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/qiandongwei.jpg","hash":"6873551596a4513d01898ad866c4073c68270c57","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/zhangting.jpg","hash":"10ee25ae3531f046a8bd3696c1cc8a16f0f25e1b","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/xuzhongyou.jpg","hash":"1db4dfaf23cf250f222a398326562d4170d3aaa1","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/zzw.jpg","hash":"5d385b5732644b07b937a4919abc83cb95e14513","modified":1569474790000},{"_id":"themes/matery/source/medias/reward/wechat.png","hash":"70de6c7a69361ee7e978714934c085cc2612ca8c","modified":1573754275338},{"_id":"source/_posts/C和C++语法总结/02.png","hash":"c901c070814411eb4ecbd5f607b543b3a1ad0fea","modified":1577448228574},{"_id":"source/_posts/机器学习概述/5.jpg","hash":"52c9a54b1edeab646595551c6ba85f7f6b0582e3","modified":1575276786538},{"_id":"source/_posts/神经网络简介/n7.png","hash":"ca0372249e26cc843ccf2941fdc6cf069563d44a","modified":1575276786580},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","hash":"70c0c4a9bf698747b7c058c21287ad617355e5dd","modified":1574068346028},{"_id":"themes/matery/source/libs/awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1574068346029},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","hash":"3480f00d284c812d623ed16a9e0ead3fb964c72e","modified":1574068346052},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","hash":"1142b47de219dddfba2e712cd3189dec0c8b7bee","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","hash":"fbfdbe06aebf7d0c00da175a4810cf888d128f11","modified":1569474790000},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","hash":"39150b45ec5fc03155b7ebeaa44f1829281788e2","modified":1569474790000},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","hash":"2c27939768606603bee3b5e6c8a722596a667e60","modified":1574068346056},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","hash":"c843f0dc497314574c608ca28cc742bb041786d5","modified":1574068346058},{"_id":"themes/matery/source/libs/share/css/share.min.css","hash":"7126de5cec8371e580b7b1f22512da0985cc39e5","modified":1574068346063},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","hash":"337b4f156f6d8f4beb32c32a3db46fef361cff74","modified":1574068346064},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1569474790000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1569474790000},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","hash":"16ce82901ca0e302cf47a35fb10f59009a5e7eb9","modified":1569474790000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1569474790000},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","hash":"4df722bafde2c5d8faaace0d1f894798385a8793","modified":1569474790000},{"_id":"themes/matery/source/libs/valine/Valine.min.js","hash":"f1558f12d96a352e490166d543a8e821dd3bb2bc","modified":1574068346067},{"_id":"themes/matery/source/medias/avatars/0xbird.png","hash":"f9d597dfcb49e1e2be06138b24028291f5638610","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/milyyy.jpg","hash":"ac2826d9c28346efeb967df01465a2c74d9041fe","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/mizunashi.png","hash":"5fc300701d3b4250a307ed70e3a3aa0d5395c808","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/qianqian.png","hash":"fed254c4e7eb58ee22d647acb83f1d08f4508f8f","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/16.jpg","hash":"0d8315876e8285abec48ce71797a14734fdc980c","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/17.jpg","hash":"5f27406bd8661bef44291601edc2784b5af63c23","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/2.jpg","hash":"c86ede693175156922041c5cdb31b9f1432a0e8a","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/6.jpg","hash":"7bdf83e0822794a7294fe554c5d21edda1e0c28f","modified":1569474790000},{"_id":"themes/matery/source/medias/music/avatars/fly.jpeg","hash":"2fd387f2058b5d60d174230ae3dd67c91414f7ae","modified":1574068346113},{"_id":"themes/matery/source/medias/music/avatars/guimixinqiao.jpg","hash":"818ef6fe415bf9cb1e4d7be70e27ed86b3bc2896","modified":1574068346114},{"_id":"themes/matery/source/medias/music/avatars/meixue.jpg","hash":"8e05a54d3629b4241512d8d938d397bbeb2cb123","modified":1574068346115},{"_id":"themes/matery/source/medias/music/avatars/slience.jpg","hash":"434659b743eee2c7fcd4f7025a7a977a5e15ae83","modified":1574068346115},{"_id":"themes/matery/source/medias/music/avatars/tiantangdemogui.jpg","hash":"f005578ddb4d3d731838db89a708f39f18d50e60","modified":1569474790000},{"_id":"themes/matery/source/medias/music/avatars/yequ.jpg","hash":"103beb9ab33434b434fa37a30aecdb29db633024","modified":1569474790000},{"_id":"themes/matery/source/medias/music/avatars/yiluxiangbei.jpg","hash":"01b12e3aca7385a88412c12539e1a608a78896fa","modified":1569474790000},{"_id":"source/_posts/深度学习之CNN模型演化/c1.png","hash":"c51bc76c3cdf23b0af41319864139a8f6fbf8af4","modified":1575271524154},{"_id":"source/_posts/神经网络简介/b5.png","hash":"028c395fd01f1987d28ea3d220bdbf0649222b14","modified":1575276786571},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1569474790000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1569474790000},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","hash":"82276be41d2001e820020a219b90ad5b026302d1","modified":1574068346037},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","hash":"f63c7c489524ccb5d95e74fcd6618116c58fb305","modified":1574068346045},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"f8cd48e1fff82ecd54a7ce3e69de8dba7c92d113","modified":1574068346053},{"_id":"themes/matery/source/libs/valine/av-min.js","hash":"04c6b2782ce4610c429563110f6a20a47432fc4c","modified":1574068346068},{"_id":"themes/matery/source/medias/avatars/jitao.jpg","hash":"5934b9baccebccbc2be2ead5d84ad32dd41f9559","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/lyn-draw.jpg","hash":"837d5d5df4dcb086d2da114d0d85084b4ec18768","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/mashiro.jpg","hash":"250e911c16eeb6acb1e6214ad3e6a3d762850a8e","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/lzh.png","hash":"8ffcbf19d6b38b891dbe408d9a4e9513b56f247e","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/1.jpg","hash":"e177d97cfbb0071d2ac15e99d15666cf09d5eacb","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/20.jpg","hash":"6dcacca5f1d93dec5c12cab74516e415243c0a64","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/25.jpg","hash":"35a945ed6f2f8a0d055f9da6e90b974b7f30c5b0","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/26.jpg","hash":"7e6241a0793404fb3668b1030795e5031e0f1ba7","modified":1569474790000},{"_id":"source/_posts/C和C++语法总结/05.png","hash":"64f3ffca32a5dcf37c512c0fc0a5285656c6719c","modified":1577939274921},{"_id":"source/_posts/机器学习概述/12.png","hash":"7b2558bbe577e8bcc6d16ca3ac3f242c94f15e36","modified":1575276786525},{"_id":"source/_posts/机器学习概述/11.png","hash":"07996f90697e0f01aaab1c32f66a5b88ad4f554e","modified":1575276786523},{"_id":"source/_posts/机器学习概述/13.png","hash":"2423cfee022f5818c8929357875b8b76b9d4666b","modified":1575276786528},{"_id":"source/_posts/深度学习之CNN模型演化/v11.png","hash":"91ee08fe34a00eec9fb5a0606d1ad31a4b21e153","modified":1578400431146},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/12.jpg","hash":"c72b487cf410eb99697ac6e416b1db9c10fa92ce","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/15.jpg","hash":"18cc714b6693d0336c1bbaef47ed79cc8a1d95e7","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/18.jpg","hash":"5efc3856cd2da8863e9c808005c3724c09843c3c","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/24.jpg","hash":"44d047bc0d29d6ae3e1d65b6cb95143d0d1b6ec2","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/7.jpg","hash":"d41c54ff1c41f5bfd4ccf5af303229059577c351","modified":1569474790000},{"_id":"source/_posts/深度学习之CNN模型演化/c8.png","hash":"87d76ecfeaea1edc68470c21895f91a2820ed5b9","modified":1575271524183},{"_id":"themes/matery/source/libs/awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1569474790000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/zhangyi.jpg","hash":"c9130036aac9a7ac8d62e33550a9d64896cdc364","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/0.jpg","hash":"39a1cb44e4ab3006577fefcbeecaddd1a2f62cfa","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/11.jpg","hash":"3fa19adaa3c19d58e34be573f1c3233c9b57b242","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/13.jpg","hash":"575cb26b73b220a3fea685108812466fb8b68e62","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/14.jpg","hash":"f515cb01de6d2b1981f50f0487e9b5beb99baf43","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/19.jpg","hash":"7a07b854a602cb9c8fa0db80ebd2c9b989f3de22","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/21.jpg","hash":"c04a6292dc85cc47a9ec370e1bee81e0e7e65652","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/22.jpg","hash":"43d1db28c3f3f953c5fd399fd959f6e92ee88fe7","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/3.jpg","hash":"447bf84c1d9bd71491545922c8735b5b41e2121f","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/8.jpg","hash":"4cedcb5a87fa5fbc1f343c9180d4e52bb4331220","modified":1569474790000},{"_id":"themes/matery/source/medias/music/avatars/daoshu.jpg","hash":"eee120fdf5ccbe86aa7d51826c4c773e76e6357f","modified":1569474790000},{"_id":"themes/matery/source/medias/banner/1.jpg","hash":"cd3fc47d2042a3277e4b375ad084365abdc28f5d","modified":1569474790000},{"_id":"themes/matery/source/medias/banner/2.jpg","hash":"c2980f75f2c047d0957e3c8227b3f8d84e67f752","modified":1569474790000},{"_id":"themes/matery/source/medias/banner/4.jpg","hash":"73bae0e6812c46509b91e3155bd12ce8640b245a","modified":1569474790000},{"_id":"themes/matery/source/medias/banner/6.jpg","hash":"5be9274e63d6ac02607e3d659fd32532291385fa","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/10.jpg","hash":"6339e06736ff6618d97cca389d7887439c87415a","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/27.jpg","hash":"f4d026ea224b9d216357d4b42fbe9f002bcaa2b0","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/4.jpg","hash":"f46e3860fbcbb63cd76023d91b7a4715e27fb231","modified":1569474790000},{"_id":"source/_posts/C和C++语法总结/01.png","hash":"1055b6f98c5b0c9e4ece8f625b263cfb07784a87","modified":1577447331387},{"_id":"themes/matery/source/medias/banner/5.jpg","hash":"6f5795e2fa01a6a7f09e3419941a0fc147ea83c5","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/23.jpg","hash":"8cc1c127ca8ff09d4a07f5c3b845ef2340a1106f","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/5.jpg","hash":"4c001e19790f5ac73259138ad1713338ea34c664","modified":1569474790000},{"_id":"themes/matery/source/medias/featureimages/9.jpg","hash":"1829b3464d5715136d3feec2849c7716541e83ed","modified":1569474790000},{"_id":"themes/matery/source/medias/avatars/jingjing.jpg","hash":"bfcab0139edb2509de984cb0a9b156879c355158","modified":1569474790000},{"_id":"themes/matery/source/medias/banner/0.jpg","hash":"4d6f31f86966584360bcdbfecb6f6a2ec94f944d","modified":1569474790000},{"_id":"source/_posts/数据结构与算法之树/binarytree.png","hash":"9d81839de778b34556c4714dade31c0ed3df6920","modified":1574530064125},{"_id":"source/_posts/数据结构与算法之树/childs01.png","hash":"e9fba2d76ffb64f3e50275040e7878c0bedb419e","modified":1574527041817},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1574068346033},{"_id":"themes/matery/source/medias/banner/3.jpg","hash":"5c494c93ad30b8f71c7865b42b3cf613279a914e","modified":1569474790000},{"_id":"source/_posts/机器学习概述/14.png","hash":"9cf32545ac6b37c6dd3c758dd1b0981f7f76794e","modified":1575276786532},{"_id":"source/_posts/数据结构与算法之树/childs02.png","hash":"e691deb2654b5ecc8b1c273ccf964b2f204aaf13","modified":1574527144703},{"_id":"source/_posts/深度学习之过拟合/01.png","hash":"73210b1388e0912f4b15f1a177ac159e4b302417","modified":1577633938525},{"_id":"source/_posts/数据结构与算法之树/parchild.png","hash":"a87c82f3d2293157765b4ccc8cd680995b95fc5e","modified":1574527769808},{"_id":"source/_posts/数据结构与算法之树/childs03.png","hash":"2f58a47ce4b81a5be9ae4c1d75dd3d140c690f67","modified":1574527499499},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","hash":"8789b5e4daf0029a6c88f238f10e54d01c4fce82","modified":1574068346042},{"_id":"source/_posts/深度学习之优化算法/v1.webp","hash":"3a6bac28183e028c40486004bd7855de3834903f","modified":1575276786550},{"_id":"source/_posts/深度学习之优化算法/v2.webp","hash":"7b3ff955b54e123bcb0a6c668ebf1cb0ec226abe","modified":1575276786563},{"_id":"themes/matery/source/medias/music/guimixinqiao.mp3","hash":"45ecabe519c61149502828b40e141609bb74f059","modified":1574068346194},{"_id":"themes/matery/source/medias/music/Silence.mp3","hash":"eb2309bdbfd37705efec6141da8509b0a9c908f2","modified":1574068346112},{"_id":"themes/matery/source/medias/music/fly.mp3","hash":"34b524b1ab440c6d1cd365c4daa55d4500ef5567","modified":1574068346174},{"_id":"themes/matery/source/medias/music/yinlong.mp3","hash":"39e0a0b952f8974ec4fb0f92af0a5b894ebbeb7c","modified":1574068346259},{"_id":"themes/matery/source/medias/video/Alizée - La isla bonita (live).mp4","hash":"8a3ee30931dd391d209114952c6906f7fcffa7d5","modified":1574068346386}],"Category":[{"name":"C++","_id":"ck5454trv0005zsv54or2iw1b"},{"name":"Python","_id":"ck5454ts5000czsv5nf2rp8gh"},{"name":"随笔","_id":"ck5454tsc000jzsv5pa78y7d8"},{"name":"数据结构与算法","_id":"ck5454tsy0010zsv57q845ngx"},{"name":"机器学习","_id":"ck5454tt50015zsv589ouwyv6"},{"name":"深度学习","_id":"ck5454tti001dzsv58fqvy6e7"}],"Data":[{"_id":"friends","data":[]},{"_id":"musics","data":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}],"Page":[{"title":"404","date":"2019-07-19T08:41:10.000Z","type":"404","layout":"404","description":"你来到了没有知识的荒原 :(","_content":"","source":"404.md","raw":"---\ntitle: 404\ndate: 2019-07-19 16:41:10\ntype: \"404\"\nlayout: \"404\"\ndescription: \"你来到了没有知识的荒原 :(\"\n---\n","updated":"2019-11-18T09:12:25.929Z","path":"404.html","comments":1,"_id":"ck5454tpc0000zsv52s3nzki4","content":"","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":""},{"title":"about","date":"2019-07-19T08:41:10.000Z","type":"about","layout":"about","_content":"\n\n# 教育经历\n\n\n# 获得荣誉\n\n\n# 联系方式\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2019-07-19 16:41:10\ntype: \"about\"\nlayout: \"about\"\n---\n\n\n# 教育经历\n\n\n# 获得荣誉\n\n\n# 联系方式\n","updated":"2019-11-18T09:12:25.934Z","path":"about/index.html","comments":1,"_id":"ck5454trs0002zsv5ujusisdc","content":"<h1 id=\"教育经历\"><a href=\"#教育经历\" class=\"headerlink\" title=\"教育经历\"></a>教育经历</h1><h1 id=\"获得荣誉\"><a href=\"#获得荣誉\" class=\"headerlink\" title=\"获得荣誉\"></a>获得荣誉</h1><h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1>","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"教育经历\"><a href=\"#教育经历\" class=\"headerlink\" title=\"教育经历\"></a>教育经历</h1><h1 id=\"获得荣誉\"><a href=\"#获得荣誉\" class=\"headerlink\" title=\"获得荣誉\"></a>获得荣誉</h1><h1 id=\"联系方式\"><a href=\"#联系方式\" class=\"headerlink\" title=\"联系方式\"></a>联系方式</h1>"},{"title":"archives","date":"2019-07-19T08:39:20.000Z","type":"archives","layout":"archives","_content":"","source":"archives/index.md","raw":"---\ntitle: archives\ndate: 2019-07-19 16:39:20\ntype: \"archives\"\nlayout: \"archives\"\n---","updated":"2019-11-18T09:12:25.935Z","path":"archives/index.html","comments":1,"_id":"ck5454tru0004zsv54tg0erly","content":"","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":""},{"title":"contact","date":"2019-07-26T09:17:02.000Z","type":"contact","layout":"contact","_content":"\n# 欢迎留言\n\n\n# 友链交换\n","source":"contact/index.md","raw":"---\ntitle: contact\ndate: 2019-07-26 17:17:02\ntype: \"contact\"\nlayout: \"contact\"\n---\n\n# 欢迎留言\n\n\n# 友链交换\n","updated":"2019-11-18T09:12:25.938Z","path":"contact/index.html","comments":1,"_id":"ck5454try0008zsv5hjt90wh8","content":"<h1 id=\"欢迎留言\"><a href=\"#欢迎留言\" class=\"headerlink\" title=\"欢迎留言\"></a>欢迎留言</h1><h1 id=\"友链交换\"><a href=\"#友链交换\" class=\"headerlink\" title=\"友链交换\"></a>友链交换</h1>","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"欢迎留言\"><a href=\"#欢迎留言\" class=\"headerlink\" title=\"欢迎留言\"></a>欢迎留言</h1><h1 id=\"友链交换\"><a href=\"#友链交换\" class=\"headerlink\" title=\"友链交换\"></a>友链交换</h1>"},{"title":"tags","date":"2019-07-19T08:40:27.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2019-07-19 16:40:27\ntype: \"tags\"\nlayout: \"tags\"\n---","updated":"2019-11-18T09:12:25.940Z","path":"tags/index.html","comments":1,"_id":"ck5454ts3000azsv5o2nlenhe","content":"","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":""},{"title":"categories","date":"2019-07-19T08:39:20.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2019-07-19 16:39:20\ntype: \"categories\"\nlayout: \"categories\"\n---","updated":"2019-11-18T09:12:25.937Z","path":"categories/index.html","comments":1,"_id":"ck5454ts6000ezsv5pgccqlt6","content":"","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":""},{"title":"friends","date":"2019-07-19T08:42:10.000Z","type":"friends","layout":"friends","_content":"\n# 赞赏名单\n\n\n# 友链交换\n\n","source":"friends/index.md","raw":"---\ntitle: friends\ndate: 2019-07-19 16:42:10\ntype: \"friends\"\nlayout: \"friends\"\n---\n\n# 赞赏名单\n\n\n# 友链交换\n\n","updated":"2019-11-18T09:12:25.939Z","path":"friends/index.html","comments":1,"_id":"ck5454ts9000gzsv5hdfujoe5","content":"<h1 id=\"赞赏名单\"><a href=\"#赞赏名单\" class=\"headerlink\" title=\"赞赏名单\"></a>赞赏名单</h1><h1 id=\"友链交换\"><a href=\"#友链交换\" class=\"headerlink\" title=\"友链交换\"></a>友链交换</h1>","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"赞赏名单\"><a href=\"#赞赏名单\" class=\"headerlink\" title=\"赞赏名单\"></a>赞赏名单</h1><h1 id=\"友链交换\"><a href=\"#友链交换\" class=\"headerlink\" title=\"友链交换\"></a>友链交换</h1>"}],"Post":[{"title":"CMake用法总结","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-18T08:18:09.000Z","password":null,"summary":null,"_content":"\n# 前言\n\n\n\n# 一、`CMake`的作用\n\n大家都知道, 源文件的编译步骤为:\n\n+ 预处理: 宏定义展开, 头文件展开, 条件编译\n+ 编译: 检查语法, 生成编译文件\n+ 汇编: 将汇编文件生成目标文件(二进制文件)\n+ 链接: 将目标文件链接成目标程序\n\n但如果源文件太多，一个一个编译就会特别麻烦，为什么不批处理编译源文件呢，于是就有了make工具，它是一个自动化编译工具，你可以使用一条命令实现完全编译。还可以指定文件编译的顺序。但是使用make编译源码，需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile，所以编写makefile文件也是一个程序员所必备的技能。\n 对于一个大工程，编写makefile实在是件复杂的事，于是人们又想，为什么不设计一个工具，读入所有源文件之后，自动生成makefile呢，于是就出现了`cmake`工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。所以在编程的世界里没有捷径可走，还是要脚踏实地的。\n\n 原文件－－camkelist ---cmake ---makefile ---make ---生成可执行文件\n\n# 二、`CMake基本语法规则`\n\n1. 变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名\n\n2. 指令(参数1  参数2  ...)\n\n   参数使用括弧括起，参数之间使用空格或分号分开\n\n3. 指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令\n\n4. 关于双引号的疑惑\n\n   ```shell\n   SET(SRC_LIST main.c)也可以写成 SET(SRC_LIST “main.c”)\n   是没有区别的，但是假设一个源文件的文件名是 fu nc.c(文件名中间包含了空格)。这时候就必须使用双引号，如果写成了 SET(SRC_LIST fu nc.c)，就会出现错误，提示你找不到 fu 文件和 nc.c 文件。这种情况，就必须写成:SET(SRC_LIST “fu nc.c”)\n   ```\n\n   \n\n# 三、内部构建与外部构建\n\n内部构建就是在项目跟目录直接编译\n\n引出了我们对外部编译的探讨，外部编译的过程如下：\n\n1. 首先，请清除 t1 目录中除 main.c CmakeLists.txt 之外的所有中间文件，最关键的是 CMakeCache.txt。\n2. 在 t1 目录中建立 build 目录，当然你也可以在任何地方建立 build 目录，不一定必须在工程目录中。\n3. 进入 build 目录，运行 cmake ..(注意,..代表父目录，因为父目录存在我们需要的CMakeLists.txt，如果你在其他地方建立了 build 目录，需要运行 cmake <工程的全路径>)，查看一下 build 目录，就会发现了生成了编译需要的 Makefile 以及其他的中间文件.\n4. 运行 make 构建工程，就会在当前目录(build 目录)中获得目标文件 hello。\n5. 上述过程就是所谓的 out-of-source 外部编译，一个最大的好处是，对于原有的工程没有任何影响，所有动作全部发生在编译目录。通过这一点，也足以说服我们全部采用外部编译方式构建工程。\n6. 这里需要特别注意的是：\n   通过外部编译进行工程构建，HELLO_SOURCE_DIR 仍然指代工程路径，即/backup/cmake/t1, 而 HELLO_BINARY_DIR 则指代编译路径，即/backup/cmake/t1/build\n\n#　四、安装库和INSTALL指令\n\n有两种安装方式，一种是从代码编译后直接 make install 安装，一种是cmake的install 指令安装。\n\n## 1、`make install`\n\n```shell\nDESTDIR=\ninstall:\n\tmkdir -p $(DESTDIR)/usr/bin\n\tinstall -m 755 hello $(DESTDIR)/usr/bin\n你可以通过:\n\tmake install\n将 hello 直接安装到/usr/bin 目录，也可以通过 make install\nDESTDIR=/tmp/test 将他安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。稍微复杂一点的是还需要定义 PREFIX，一般 autotools 工程，会运行这样的指令:\n./configure –prefix=/usr \n或者./configure --prefix=/usr/local \n来指定PREFIX\n比如上面的 Makefile 就可以改写成:\nDESTDIR=\nPREFIX=/usr\ninstall:\n\tmkdir -p $(DESTDIR)/$(PREFIX)/bin\n\tinstall -m 755 hello $(DESTDIR)/$(PREFIX)/bin\n```\n\n\n\n## 2、`cmake INSTALL`指令安装\n\n这里需要引入一个新的 cmake 指令 INSTALL 和一个非常有用的变量\nCMAKE_INSTALL_PREFIX。CMAKE_INSTALL_PREFIX 变量类似于 configure 脚本的 –prefix，常见的使用方法看起来是这个样子：\n\t`cmake -DCMAKE_INSTALL_PREFIX=/usr ..`\nINSTALL 指令用于定义安装规则，安装的内容可以包括目标二进制、动态库、静态库以及文件、目录、脚本等。\n\nINSTALL 指令包含了各种安装类型，我们需要一个个分开解释：\n目标文件的安装：\n\n```\nINSTALL(TARGETS targets...\n\t[[ARCHIVE|LIBRARY|RUNTIME]\n\t[DESTINATION <dir>]\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[OPTIONAL]\n] [...])\n```\n\n\n\n参数中的 TARGETS 后面跟的就是我们通过 ADD_EXECUTABLE 或者 ADD_LIBRARY 定义的\n目标文件，可能是可执行二进制、动态库、静态库。\n目标类型也就相对应的有三种，ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME\n特指可执行目标二进制。\nDESTINATION 定义了安装的路径，如果路径以/开头，那么指的是绝对路径，这时候\nCMAKE_INSTALL_PREFIX 其实就无效了。如果你希望使用 CMAKE_INSTALL_PREFIX 来\n定义安装路径，就要写成相对路径，即不要以/开头，那么安装后的路径就是\n${CMAKE_INSTALL_PREFIX}/<DESTINATION 定义的路径>\n举个简单的例子：\n\n```shell\nINSTALL(TARGETS myrun mylib mystaticlib\n\tRUNTIME DESTINATION bin\n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION libstatic\n)\n```\n\n上面的例子会将：\n可执行二进制 myrun 安装到${CMAKE_INSTALL_PREFIX}/bin 目录\n动态库 libmylib 安装到${CMAKE_INSTALL_PREFIX}/lib 目录\n静态库 libmystaticlib 安装到${CMAKE_INSTALL_PREFIX}/libstatic 目录\n特别注意的是你不需要关心 TARGETS 具体生成的路径，只需要写上 TARGETS 名称就可以\n了。  \n\n普通文件的安装：\n\n```shell\nINSTALL(FILES files... DESTINATION <dir>\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[RENAME <name>] [OPTIONAL])\n```\n\n\n\n可用于安装一般文件，并可以指定访问权限，文件名是此指令所在路径下的相对路径。如果\n默认不定义权限 PERMISSIONS，安装后的权限为：\nOWNER_WRITE, OWNER_READ, GROUP_READ,和 WORLD_READ，即 644 权限。\n非目标文件的可执行程序安装(比如脚本之类)：\n\n```\nINSTALL(PROGRAMS files... DESTINATION <dir>\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[RENAME <name>] [OPTIONAL])\n```\n\n跟上面的 FILES 指令使用方法一样，唯一的不同是安装后权限为:\nOWNER_EXECUTE, GROUP_EXECUTE, 和 WORLD_EXECUTE，即 755 权限\n目录的安装：\n\n```shell\nINSTALL(DIRECTORY dirs... DESTINATION <dir>\n\t[FILE_PERMISSIONS permissions...]\n\t[DIRECTORY_PERMISSIONS permissions...]\n\t[USE_SOURCE_PERMISSIONS]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[[PATTERN <pattern> | REGEX <regex>]\n\t[EXCLUDE] [PERMISSIONS permissions...]] [...])\n```\n\n\n这里主要介绍其中的 DIRECTORY、PATTERN 以及 PERMISSIONS 参数。\n\nDIRECTORY 后面连接的是所在 Source 目录的相对路径，但务必注意：abc 和 abc/有很大的区别。\n如果目录名不以/结尾，那么这个目录将被安装为目标路径下的 abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。\nPATTERN 用于使用正则表达式进行过滤，PERMISSIONS 用于指定 PATTERN 过滤后的文件权限。\n我们来看一个例子:\n\n```shell\nINSTALL(DIRECTORY icons scripts/ DESTINATION \tshare/myproj\nPATTERN \"CVS\" EXCLUDE\nPATTERN \"scripts/*\"\nPERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ\nGROUP_EXECUTE GROUP_READ)\n\n```\n\n这条指令的执行结果是：\n将 icons 目录安装到 <prefix>/share/myproj，将 scripts/中的内容安装到<prefix>/share/myproj不包含目录名为 CVS 的目录，对于 scripts/*  文件指定权限为 OWNER_EXECUTE   OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ.\n\n安装时 CMAKE 脚本的执行：\n\n```\nINSTALL([[SCRIPT <file>] [CODE <code>]] [...])\nSCRIPT 参数用于在安装时调用 cmake 脚本文件（也就是<abc>.cmake 文件）\nCODE 参数用于执行 CMAKE 指令，必须以双引号括起来。比如：\nINSTALL(CODE \"MESSAGE(\\\"Sample install message.\\\")\")\n```\n\n\n\n# 五、静态库和动态库构建\n\n## 1、ADD_LIBRARY指令\n\n```shell\nADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n\t[EXCLUDE_FROM_ALL]\n\tsource1 source2 ... sourceN)\n# 不需要写全lib<libname>.so, 只需要填写<libname>,cmake系统会自动为你生成，lib<libname>.X\n\n# 类型有三种:\n\tSHARED，动态库\t.so\n\tSTATIC，静态库\t.a\n\tMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\n\t\n#EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。\n```\n\n\n## 2、指定库的生成路径\n\n​\t两种方法\n\n1. ADD_SUBDIRECTORY指令来指定一个编译输出位置\n2. 在CMakeLists.txt中添加　SET(LIBRARY_OUTPUT_PATH <路径>)来指定一个新的位置\n\n\n## 3、同时生成动态库和静态库\n\n因为ADD_SUBDIRECTORY的TARGET(libname)是唯一的，所以生成动态库和静态库不能指定相同的名称，想要有相同的名称需要用到SET_TARGET_PROPERTIES指令。\n\nSET_TARGET_PROPERTIES，其基本语法是：\n\n```shell\nSET_TARGET_PROPERTIES(target1 target2 ...\n\tPROPERTIES prop1 value1\n\tprop2 value2 ...)\n# 举例\nADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})　# 动态库\nADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) # 静态库\nSET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \"hello\")\n```\n\n这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。\n\n与他对应的指令是：\n\tGET_TARGET_PROPERTY(VAR target property)\n\n举例\n\n```shell\nGET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME)\nMESSAGE(STATUS “This is the hello_static\nOUTPUT_NAME:”${OUTPUT_VALUE})\n# 如果没有这个属性定义，则返回 NOTFOUND.\n```\n\n## 4、动态库版本号\n\n```shell\nSET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1)\n# VERSION 指代动态库版本，SOVERSION 指代 API 版本。\n# 在 build/lib 目录会生成：\n    libhello.so.1.2\n    libhello.so.1->libhello.so.1.2\n    libhello.so -> libhello.so.1\n```\n\n\n\n# 六、使用共享库和头文件\n\n## 1.`INCLUDE_DIRECTORIES`指令\n\n`INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)`\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１. CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。\n２. 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。\n\n## 2. `LINK_DIRECTORIES`和 `TARGET_LINK_LIBRARIES`\n\n```shell\nLINK_DIRECTORIES(directory1 directory2 ...)\n# 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制，在编译时就需要指定一下这些共享库的路径。\n# TARGET_LINK_LIBRARIES 的全部语法是:\nTARGET_LINK_LIBRARIES(target library1\n\t<debug | optimized> library2\n...)\n# 这个指令可以用来为 target 添加需要链接的共享库\n```\n\n## 3. `FIND`系列指令\n\n1. 特殊的环境变量` CMAKE_INCLUDE_PATH` 和`CMAKE_LIBRARY_PATH`\n\n   务必注意，这两个是环境变量而不是 cmake 变量\n\n2. `CMAKE_INCLUDE_PATH`和`CMAKE_LIBRARY_PATH`是配合`FIND_PATH`和`FIND_LIBRARY`指令使用的\n\n3. find_path指令\n\n   ```shell\n   find_path (<VAR> NAMES name)\n   # <VAR>查找的库文件路径报存在变量VAR中\n   # 默认搜索路径为`CMAKE_INCLUDE_PATH`\n   \n   find_path (<VAR> NAMES name PATHS paths... [NO_DEFAULT_PATH])\n   #　指定搜索路径\n   # NO_DEFAULT_PATH　不使用默认搜索路径　\n   # 举例\n   为了将程序更智能一点，我们可以使用 CMAKE_INCLUDE_PATH 来进行，使用 bash 的方法\n   如下：export CMAKE_INCLUDE_PATH=/usr/include/hello\n   然后在头文件中将 INCLUDE_DIRECTORIES(/usr/include/hello)替换为：\n   FIND_PATH(myHeader hello.h)\n   IF(myHeader)\n   \tINCLUDE_DIRECTORIES(${myHeader})\n   ENDIF(myHeader)\n   ```\n\n## 4. 共享库和头文件指令总结\n\n1. **FIND_PATH** 查找头文件所在目录\n2. **INCLUDE_DIRECTORIES**　添加头文件目录\n3. **FIND_LIBRARY** 查找库文件所在目录\n4. **LINK_DIRECTORIES**   添加库文件目录\n5. **LINK_LIBRARIES**　添加需要链接的库文件路径，注意这里是全路径\n6. **TARGET_LINK_LIBRARIES **　给TARGET链接库\n\n\n\n# 七、Find模块\n\n## 1.Find模块使用\n\n```shell\nFIND_PACKAGE(XXX)\nIF(XXX_FOUND)\n\tINCLUDE_DIRECTORIES(${XXX_INCLUDE_DIR})\n\tTARGET_LINK_LIBRARIES(xxxtest ${XXX_LIBRARY})\nELSE(XXX_FOUND)\n\tMESSAGE(FATAL_ERROR ”XXX library not found”)\nENDIF(XXX_FOUND)\n```\n\n对于系统预定义的 Find<name>.cmake 模块，使用方法一般如上例所示：\n每一个模块都会定义以下几个变量\n\t• <name>_FOUND\n\t• <name>_INCLUDE_DIR or <name>_INCLUDES\n\t• <name>_LIBRARY or <name>_LIBRARIES\n你可以通过<name>_FOUND 来判断模块是否被找到，如果没有找到，按照工程的需要关闭某些特性、给出提醒或者中止编译\n\n\n\n\n\n## 2.find_package指令\n\n```shell\nfind_package(<PackageName> [QUIET] [REQUIRED] [[COMPONENTS] [components...]]\n             [OPTIONAL_COMPONENTS components...]\n             [NO_POLICY_SCOPE])\n             \n# 查找并从外部项目加载设置，\n# <PackageName>_FOUND 将设置为指示是否找到该软件包, 如果查找到，该变量为true\n# [QUIET], 设置该变量，不会打印任何消息，且\t\t   <PackageName>_FIND_QUIETLY为true\n# [REQUIRED] 设置该变量，如果找不到软件包，该选项将停止处理并显示一条错误消息，且设置<PackageName>_FIND_REQUIRED为true,不过不指定该参数，即使没有找到，也能编译通过\n```\n\nfind_package采用两种模式搜索库：\n\n-  **Module模式**：搜索**CMAKE_MODULE_PATH**指定路径下的**FindXXX.cmake**文件，执行该文件从而找到XXX库。其中，具体查找库并给**XXX_INCLUDE_DIRS**和**XXX_LIBRARIES**两个变量赋值的操作由FindXXX.cmake模块完成。\n-  **Config模式**：搜索**XXX_DIR**指定路径下的**XXXConfig.cmake**文件，执行该文件从而找到XXX库。其中具体查找库并给**XXX_INCLUDE_DIRS**和**XXX_LIBRARIES**两个变量赋值的操作由XXXConfig.cmake模块完成。\n\n两种模式看起来似乎差不多，不过cmake默认采取**Module**模式，如果Module模式未找到库，才会采取Config模式。如果**XXX_DIR**路径下找不到XXXConfig.cmake或`<lower-case-package-name>`config.cmake文件，则会找/usr/local/lib/cmake/XXX/中的XXXConfig.cmake文件。总之，Config模式是一个备选策略。通常，库安装时会拷贝一份XXXConfig.cmake到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。\n\n总结：CMake搜索的顺序为: 首先在`CMAKE_MODULE_PATH`中搜索名为`Find<PackageName>.cmake`的文件，然后在`<PackageName>_DIR`名为`PackageName>Config.cmake`或`<lower-case-package-name>-config.cmake`的文件，如果还是找不到，则会去`/usr/local/lib/cmake`中查找`Find<PackageName>.cmake`文件。\n\n所以我们可以通过`CMAKE_MODULE_PATH`或`<PackageName>_DIR`变量指定cmake文件路径。\n\n## 3.自定义Find模块\n\n```shell\n# 查找HELLO的头文件目录\nFIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello\n/usr/local/include/hello)\n# 查找HELLO的动态库\nFIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib\n/usr/local/lib)\nIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\n\tSET(HELLO_FOUND TRUE)\nENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\nIF (HELLO_FOUND)\n\t# 如果不指定QUIET参数，就打印信息\n\tIF (NOT HELLO_FIND_QUIETLY)\n\t\tMESSAGE(STATUS \"Found Hello: ${HELLO_LIBRARY}\")\n\tENDIF (NOT HELLO_FIND_QUIETLY)\nELSE (HELLO_FOUND)\n\t# 如果设置了REQUIRED参数就报错\n\tIF (HELLO_FIND_REQUIRED)\n\t\tMESSAGE(FATAL_ERROR \"Could not find hello library\")\n\tENDIF (HELLO_FIND_REQUIRED)\nENDIF (HELLO_FOUND)\n```\n\n# 八、`CMake`常用变量\n\n## 1.`cmake` 变量引用的方式：\n\n使用${}进行变量的引用。在 IF 等语句中，是直接使用变量名而不通过${}取值\n\n## 2.`cmake` 自定义变量的方式：\n\n主要有隐式定义和显式定义两种，前面举了一个隐式定义的例子，就是 PROJECT 指令，他会隐式的定义<projectname>_BINARY_DIR 和<projectname>_SOURCE_DIR 两个变量。\n显式定义的例子我们前面也提到了，使用 SET 指令，就可以构建一个自定义变量了。比如:\n\nSET(HELLO_SRC main.SOURCE_PATHc)，就PROJECT_BINARY_DIR 可以通过${HELLO_SRC}来引用这个自定义变量了.\n\n## 3.`cmake` 常用变量\n\n### 1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/<projectname>_BINARY_DIR_\n\n这三个变量指代的内容是一致的，如果是 in source 编译，指得就是工程顶层目录，如果是 out-of-source 编译，指的是工程编译发生的目录。PROJECT_BINARY_DIR 跟其他指令稍有区别，现在，你可以理解为他们是一致的。\n\n### 2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/<projectname>_SOURCE_DIR\n\n这三个变量指代的内容是一致的，不论采用何种编译方式，都是工程顶层目录。\n\n### 3. CMAKE_CURRENT_SOURCE_DIR\n\n指的是**当前处理的** CMakeLists.txt 所在的路径\n\n### 4. CMAKE_CURRRENT_BINARY_DIR\n\n如果是 in-source 编译，它跟 CMAKE_CURRENT_SOURCE_DIR 一致，如果是 out-ofsource 编译，他指的是 target 编译目录。\n使用我们上面提到的 ADD_SUBDIRECTORY(src bin)可以更改这个变量的值。\n使用 SET(EXECUTABLE_OUTPUT_PATH <新路径>)并不会对这个变量造成影响，它仅仅修改了最终目标文件存放的路径。\n\n### ５. CMAKE_CURRENT_LIST_FILE\n\n​\t输出调用这个变量的 CMakeLists.txt 的完整路径\n\n### 6. CMAKE_CURRENT_LIST_LINE\n\n​\t输出这个变量所在的行\n\n### 7. CMAKE_MODULE_PATH\n\n这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂，有可能会自己编写一些 cmake 模块，这些 cmake 模块是随你的工程发布的，为了让 cmake 在处理CMakeLists.txt 时找到这些模块，你需要通过 SET 指令，将自己的 cmake 模块路径设\n置一下。比如\nSET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n这时候你就可以通过 INCLUDE 指令来调用自己的模块了。\n\n### 8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH\n\n分别用来重新定义最终结果的存放目录，前面我们已经提到了这两个变量。\n\n### 9. PROJECT_NAME\n\n返回通过 PROJECT 指令定义的项目名称。\n\n## 4. cmake 调用环境变量的方式\n\n使用$ENV{NAME}指令就可以调用系统的环境变量了。\n比如MESSAGE(STATUS “HOME dir: $ENV{HOME}”)\n设置环境变量的方式是：SET(ENV{变量名} 值)\n\n### 1. CMAKE_INCLUDE_CURRENT_DIR\n\n自动添加 CMAKE_CURRENT_BINARY_DIR 和 CMAKE_CURRENT_SOURCE_DIR 到当前处理\n的 CMakeLists.txt。相当于在每个 CMakeLists.txt 加入：\nINCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR}\n${CMAKE_CURRENT_SOURCE_DIR})\n\n### 2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE\n\n将工程提供的头文件目录始终至于系统头文件目录的前面，当你定义的头文件确实跟系统发生冲突时可以提供一些帮助。\n\n### 3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。\n\n## 5. 系统信息\n\n1. CMAKE_MAJOR_VERSION，CMAKE 主版本号，比如 2.4.6 中的 2\n\n2. CMAKE_MINOR_VERSION，CMAKE 次版本号，比如 2.4.6 中的 4\n\n3. CMAKE_PATCH_VERSION，CMAKE 补丁等级，比如 2.4.6 中的 6\n\n4. CMAKE_SYSTEM，系统名称，比如 Linux-2.6.22\n\n5. CMAKE_SYSTEM_NAME，不包含版本的系统名，比如 Linux\n\n6. CMAKE_SYSTEM_VERSION，系统版本，比如 2.6.22\n\n7. CMAKE_SYSTEM_PROCESSOR，处理器名称，比如 i686.\n\n8. UNIX，在所有的类 UNIX 平台为 TRUE，包括 OS X 和 cygwin\n\n9. WIN32，在所有的 win32 平台为 TRUE，包括 cygwin\n\n   \n\n## 6.主要的开关选项：\n\n1. CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS，用来控制 IF ELSE 语句的书写方式，在\n   下一节语法部分会讲到。\n\n2. BUILD_SHARED_LIBS\n   这个开关用来控制默认的库编译方式，如果不进行设置，使用 ADD_LIBRARY 并没有指定库类型的情况下，默认编译生成的库都是静态库。\n   如果 SET(BUILD_SHARED_LIBS ON)后，默认生成的为动态\n\n3. CMAKE_C_FLAGS\n   设置 C 编译选项，也可以通过指令 ADD_DEFINITIONS()添加。\n\n4. CMAKE_CXX_FLAGS\n   设置 C++编译选项，也可以通过指令 ADD_DEFINITIONS()添加。\n\n   \n\n# 九、`CMake`常用指令\n\n## 1. 基本指令\n\n### MESSAGE\n\n```shell\nmessage([<mode>] \"message to display\" ...)\n可选<mode>关键字确定消息的类型:\nFATAL_ERROR\t立即终止所有 cmake 过程\nSEND_ERROR 产生错误，生成过程被跳过\nWARNING\nAUTHOR_WARNING\nNOTICE\nSTATUS\t输出前缀为—的信息\nVERBOSE\nDEBUG\nTRACE\n```\n\n### PROJECT\n\n```shell\nproject(<PROJECT-NAME> [<language-name>...])\nproject(<PROJECT-NAME>\n        [VERSION <major>[.<minor>[.<patch>[.<tweak>]]]]\n        [LANGUAGES <language-name>...])\n        \n设置项目的名称，并将其存储在变量中 PROJECT_NAME。从顶层调用时， CMakeLists.txt还将项目名称存储在变量CMAKE_PROJECT_NAME中。\n\n同时设置变量\n\nPROJECT_SOURCE_DIR， <PROJECT-NAME>_SOURCE_DIR\nPROJECT_BINARY_DIR， <PROJECT-NAME>_BINARY_DIR\n\nhttps://cmake.org/cmake/help/v3.15/command/project.html\n```\n\n\n\n### SET\n\n```shell\n将普通变量，缓存变量或环境变量设置为给定值。\n指定<value>...占位符的此命令的签名期望零个或多个参数。多个参数将以分号分隔的列表形式加入，以形成要设置的实际变量值。零参数将导致未设置普通变量。unset() 命令显式取消设置变量。\n1、设置正常变量\nset(<variable> <value>... [PARENT_SCOPE])\n<variable>在当前函数或目录范围内设置给定值。\n如果PARENT_SCOPE给出了该选项，则将在当前作用域上方的作用域中设置变量。\n2、设置缓存变量\nset(<variable> <value>... CACHE <type> <docstring> [FORCE])\n3、设置环境变量\nset(ENV{<variable>} [<value>])\n\n```\n\n\n\n### add_executable\n\n```shell\n使用指定的源文件生成可执行文件\nadd_executable(<name> [WIN32] [MACOSX_BUNDLE]\n               [EXCLUDE_FROM_ALL]\n               [source1] [source2 ...])\n<name>可执行文件名, <name>与逻辑目标名称相对应，并且在项目中必须是全局唯一的。构建的可执行文件的实际文件名是基于本机平台（例如<name>.exe或<name>）的约定构造的 。\n默认情况下，将在与调用命令的源树目录相对应的构建树目录中创建可执行文件。\n               \n```\n\n### add_subdirectory\n\n```shell\n在构建中添加一个子目录。\nadd_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])\n将一个子目录添加到构建中。source_dir指定源CMakeLists.txt和代码文件所在的目录。binary_dir指定了输出文件放置的目录以及编译输出的路径。EXCLUDE_FROM_ALL 参数的含义是将这个目录从编译过程中排除，比如，工程的 example，可能就需要工程构建完成后，再进入 example 目录单独进行构建(当然，你也可以通过定义依赖来解决此类问题)。\n如果没有指定binary_dir,那么编译结果(包括中间结果)都将存放在\nbuild/source_dir 目录(这个目录跟原有的 source_dir 目录对应)，指定binary_dir 目录后，相当于在编译时将 source_dir 重命名为binary_dir，所有的中间结果和目标二进制都将存放在binary_dir 目录。\n```\n\n### subdirs\n\n```shell\n构建多个子目录\nsubdirs(dir1 dir2 ...[EXCLUDE_FROM_ALL exclude_dir1 exclude_dir2 ...]\n        [PREORDER] )\n        \n            \n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录)，我们都可以通过 SET 指令重新定义EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 变量\n来指定最终的目标二进制的位置(指最终生成的 hello 或者最终的共享库，不包含编译生成的中间文件)\nSET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\nSET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\n在第一节我们提到了<projectname>_BINARY_DIR 和 PROJECT_BINARY_DIR 变量，他们指的编译发生的当前目录，如果是内部编译，就相当于 PROJECT_SOURCE_DIR 也就是工程代码所在目录，如果是外部编译，指的是外部编译所在目录，也就是本例中的两个指令分别定义了：可执行二进制的输出路径为 build/bin 和库的输出路径为 build/lib.\n```\n\n\n\n### add_library\n\n```shell\nADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n[EXCLUDE_FROM_ALL]\nsource1 source2 ... sourceN)\n你不需要写全 libhello.so，只需要填写 hello 即可，cmake 系统会自动为你生成\nlibhello.X\n类型有三种:\nSHARED，动态库\nSTATIC，静态库\nMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\nEXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手\n工构建。\n```\n\n\n\n### include_directories\n\n```shell\n将include目录添加到构建中\ninclude_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...])\n将给定目录添加到编译器用于搜索头文件的路径中。\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径\n中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１，CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以\n将添加的头文件搜索路径放在已有路径的前面。\n２，通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。\n```\n\n### target_link_libraries & link_directories\n\n```shell\nTARGET_LINK_LIBRARIES(target library1\n<debug | optimized> library2\n...)\n这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样\n可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误，我们需要作的是向\nsrc/CMakeLists.txt 中添加如下指令：\nTARGET_LINK_LIBRARIES(main hello)\n也可以写成\nTARGET_LINK_LIBRARIES(main libhello.so)\n```\n\n### ADD_DEFINITIONS\n\n```shell\n向 C/C++编译器添加-D 定义，比如:\nADD_DEFINITIONS(-DENABLE_DEBUG -DABC)，参数之间用空格分割。\n如果你的代码中定义了#ifdef ENABLE_DEBUG #endif，这个代码块就会生效。如果要添加其他的编译器开关，可以通过 CMAKE_C_FLAGS 变量和 CMAKE_CXX_FLAGS 变量设置。\n```\n\n\n\n### ADD_DEPENDENCIES\n\n```shell\n定义 target 依赖的其他 target，确保在编译本 target 之前，其他的 target 已经被构建。\nADD_DEPENDENCIES(target-name depend-target1\ndepend-target2 ...)\n```\n\n\n\n### ADD_TEST 与 ENABLE_TESTING 指令。\n\n```shell\nENABLE_TESTING 指令用来控制 Makefile 是否构建 test 目标，涉及工程所有目录。语法很简单，没有任何参数，ENABLE_TESTING()，一般情况这个指令放在工程的主CMakeLists.txt 中.\nADD_TEST 指令的语法是:\n\t`ADD_TEST(testname Exename arg1 arg2 ...)`\ntestname 是自定义的 test 名称，Exename 可以是构建的目标文件也可以是外部脚本等等。后面连接传递给可执行文件的参数。如果没有在同一个 CMakeLists.txt 中打开\n\tENABLE_TESTING()指令，任何 ADD_TEST 都是无效的。\n比如我们前面的 Helloworld 例子，可以在工程主 CMakeLists.txt 中添加\n\nADD_TEST(mytest ${PROJECT_BINARY_DIR}/bin/main)\nENABLE_TESTING()\n生成 Makefile 后，就可以运行 make test 来执行测试了。\n```\n\n\n\n### AUX_SOURCE_DIRECTORY\n\n```shell\n基本语法是：\nAUX_SOURCE_DIRECTORY(dir VARIABLE)\n作用是发现一个目录下所有的源代码文件并将列表存储在一个变量中，这个指令临时被用来\n自动构建源文件列表。因为目前 cmake 还不能自动发现新添加的源文件。\n比如\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nADD_EXECUTABLE(main ${SRC_LIST})\n你也可以通过后面提到的 FOREACH 指令来处理这个 LIST\n```\n\n\n\n###　CMAKE_MINIMUM_REQUIRED\n\n```sehll\n其语法为 CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n比如 CMAKE_MINIMUM_REQUIRED(VERSION 2.5 FATAL_ERROR)\n如果 cmake 版本小与 2.5，则出现严重错误，整个过程中止。\n```\n\n\n\n### EXEC_PROGRAM\n\n在 CMakeLists.txt 处理过程中执行命令，并不会在生成的 Makefile 中执行。具体语法为：\n\n```shell\nEXEC_PROGRAM(Executable [directory in which to run]\n[ARGS <arguments to executable>]\n[OUTPUT_VARIABLE <var>]\n[RETURN_VALUE <var>])\n```\n\n用于在指定的目录运行某个程序，通过 ARGS 添加参数，如果要获取输出和返回值，可通过OUTPUT_VARIABLE 和 RETURN_VALUE 分别定义两个变量.\n这个指令可以帮助你在 CMakeLists.txt 处理过程中支持任何命令，比如根据系统情况去修改代码文件等等。\n举个简单的例子，我们要在 src 目录执行 ls 命令，并把结果和返回值存下来。\n可以直接在 src/CMakeLists.txt 中添加：\nEXEC_PROGRAM(ls ARGS \"*.c\" OUTPUT_VARIABLE LS_OUTPUT RETURN_VALUE LS_RVALUE)\nIF(not LS_RVALUE)\n\tMESSAGE(STATUS \"ls result: \" ${LS_OUTPUT})\nENDIF(not LS_RVALUE)\n在 cmake 生成 Makefile 的过程中，就会执行 ls 命令，如果返回 0，则说明成功执行，\n那么就输出 ls *.c 的结果。关于 IF 语句，后面的控制指令会提到。\n\n### FILE 指令\n\n文件操作指令，基本语法为:\n\n```shell\nFILE(WRITE filename \"message to write\"... )\nFILE(APPEND filename \"message to write\"... )\nFILE(READ filename variable)\nFILE(GLOB variable [RELATIVE path] [globbing\nexpressions]...)\nFILE(GLOB_RECURSE variable [RELATIVE path]\n[globbing expressions]...)\nFILE(REMOVE [directory]...)\nFILE(REMOVE_RECURSE [directory]...)\nFILE(MAKE_DIRECTORY [directory]...)\nFILE(RELATIVE_PATH variable directory file)\nFILE(TO_CMAKE_PATH path result)\nFILE(TO_NATIVE_PATH path result)\n```\n\n\n\n这里的语法都比较简单，不在展开介绍了。\n\n### INCLUDE 指令\n\n```shell\n用来载入 CMakeLists.txt 文件，也用于载入预定义的 cmake 模块.\n\tINCLUDE(file1 [OPTIONAL])\n\tINCLUDE(module [OPTIONAL])\nOPTIONAL 参数的作用是文件不存在也不会产生错误。\n你可以指定载入一个文件，如果定义的是一个模块，那么将在 CMAKE_MODULE_PATH 中搜索这个模块并载入。\n载入的内容将在处理到 INCLUDE 语句是直接执行。\n```\n\n\n\n## 2. 控制指令：\n\n### 1. IF 指令\n\n基本语法为：\n\n```shell\nIF(expression)\n\n# THEN section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nELSE(expression)\n\n# ELSE section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDIF(expression)\n```\n\n\n\n另外一个指令是 ELSEIF，总体把握一个原则，凡是出现 IF 的地方一定要有对应的\nENDIF.出现 ELSEIF 的地方，ENDIF 是可选的。\n表达式的使用方法如下:\nIF(var)，如果变量不是：空，0，N, NO, OFF, FALSE, NOTFOUND 或\n<var>_NOTFOUND 时，表达式为真。\nIF(NOT var )，与上述条件相反。\nIF(var1 AND var2)，当两个变量都为真是为真。\nIF(var1 OR var2)，当两个变量其中一个为真时为真。\nIF(COMMAND cmd)，当给定的 cmd 确实是命令并可以调用是为真。\nIF(EXISTS dir)或者 IF(EXISTS file)，当目录名或者文件名存在时为真。\nIF(file1 IS_NEWER_THAN file2)，当 file1 比 file2 新，或者 file1/file2 其中有一个不存在时为真，文件名请使用完整路径。\nIF(IS_DIRECTORY dirname)，当 dirname 是目录时，为真。\nIF(variable MATCHES regex)\nIF(string MATCHES regex)\n当给定的变量或者字符串能够匹配正则表达式 regex 时为真。比如：\nIF(\"hello\" MATCHES \"ell\")\nMESSAGE(\"true\")\nENDIF(\"hello\" MATCHES \"ell\")\nIF(variable LESS number)\nIF(string LESS number)\nIF(variable GREATER number)\nIF(string GREATER number)\nIF(variable EQUAL number)\nIF(string EQUAL number)\n数字比较表达式\nIF(variable STRLESS string)\nIF(string STRLESS string)\nIF(variable STRGREATER string)\nIF(string STRGREATER string)\nIF(variable STREQUAL string)\nIF(string STREQUAL string)\n按照字母序的排列进行比较.\nIF(DEFINED variable)，如果变量被定义，为真。\n一个小例子，用来判断平台差异：\nIF(WIN32)\nMESSAGE(STATUS “This is windows.”)\n#作一些 Windows 相关的操作\nELSE(WIN32)\nMESSAGE(STATUS “This is not windows”)\n#作一些非 Windows 相关的操作\nENDIF(WIN32)\n上述代码用来控制在不同的平台进行不同的控制，但是，阅读起来却并不是那么舒服，\nELSE(WIN32)之类的语句很容易引起歧义。\n这就用到了我们在“常用变量”一节提到的 CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS 开\n关。\n可以 SET(CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS ON)\n这时候就可以写成:\nIF(WIN32)\nELSE()\nENDIF()\n如果配合 ELSEIF 使用，可能的写法是这样:\nIF(WIN32)\n#do something related to WIN32\nELSEIF(UNIX)\n#do something related to UNIX\nELSEIF(APPLE)\n#do something related to APPLE\nENDIF(WIN32)\n\n### 2. WHILE\n\nWHILE 指令的语法是：\n\n```shell\nWHILE(condition)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDWHILE(condition)\n```\n\n\n\n其真假判断条件可以参考 IF 指令。\n\n### 3. FOREACH\n\nFOREACH 指令的使用方法有三种形式：\n\n```shell\n1，列表\nFOREACH(loop_var arg1 arg2 ...)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDFOREACH(loop_var)\n像我们前面使用的 AUX_SOURCE_DIRECTORY 的例子\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nFOREACH(F ${SRC_LIST})\nMESSAGE(${F})\nENDFOREACH(F)\n2，范围\nFOREACH(loop_var RANGE total)\nENDFOREACH(loop_var)\n从 0 到 total 以１为步进\n举例如下：\nFOREACH(VAR RANGE 10)\nMESSAGE(${VAR})\nENDFOREACH(VAR)\n最终得到的输出是：\n0 1 2 3 4 5 6 7 8 9\n10\n３，范围和步进\nFOREACH(loop_var RANGE start stop [step])\nENDFOREACH(loop_var)\n从 start 开始到 stop 结束，以 step 为步进，\n举例如下\nFOREACH(A RANGE 5 15 3)\nMESSAGE(${A})\nENDFOREACH(A)\n最终得到的结果是：\n5 8\n11\n14\n这个指令需要注意的是，知道遇到 ENDFOREACH 指令，整个语句块才会得到真正的执行。\n```\n\n\n\n# 十、`CMakeLists`配置模板\n\n## １.基本配置\n\n```shell\ncmake_minimum_required(VERSION 3.14)\nproject(XXX_Project)\n\n# 设置CMAKE版本\nset(CMAKE_CXX_STANDARD 14)\n\n# 设置输出目录为 build/Debug/bin build/Debug/lib\n# 并缓存路径\nset(OUTPUT_DIRECTORY_ROOT ${CMAKE_CURRENT_SOURCE_DIR}/build/${CMAKE_BUILD_TYPE})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/bin\" CACHE PATH \"Runtime directory\" FORCE)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Library directory\" FORCE)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Archive directory\" FORCE)\n\n# 添加src子目录\nadd_subdirectory(src)\n```\n\n## ２.依赖库相关配置\n\n**`OPenCV`依赖库**\n\n将`OpenCV`依赖库下的`share/OpenCV`中，`OpenCVConfig.cmake`复制一份叫`FindOpenCV.cmake`，然后在根目录的CMakeLists.txt添加如下配置\n\n```shell\n#　添加make文件搜索路径\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ~/3rdparty/OpenCV-3.4.7/share/OpenCV)\n\n# 查找cmake文件，并初始化变量\nfind_package(OpenCV REQUIRED)\n# 添加头文件搜索路径\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n\n# 给执行程序添加链接库\nadd_executable(XXXXMain main.cpp)\ntarget_link_libraries(XXXXMain ${OpenCV_LIBS})\n```\n\n\n\n# 十一、参考\n\n1. [http://file.ncnynl.com/ros/CMake%20Practice.pdf](http://file.ncnynl.com/ros/CMake Practice.pdf)\n2. https://cmake.org/cmake/help/latest/guide/tutorial/index.html","source":"_posts/CMake用法总结.md","raw":"---\ntitle: CMake用法总结\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-18 16:18:09\npassword:\nsummary:\ntags:\n- CMake\n- Make\n- C++\ncategories: C++\n---\n\n# 前言\n\n\n\n# 一、`CMake`的作用\n\n大家都知道, 源文件的编译步骤为:\n\n+ 预处理: 宏定义展开, 头文件展开, 条件编译\n+ 编译: 检查语法, 生成编译文件\n+ 汇编: 将汇编文件生成目标文件(二进制文件)\n+ 链接: 将目标文件链接成目标程序\n\n但如果源文件太多，一个一个编译就会特别麻烦，为什么不批处理编译源文件呢，于是就有了make工具，它是一个自动化编译工具，你可以使用一条命令实现完全编译。还可以指定文件编译的顺序。但是使用make编译源码，需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile，所以编写makefile文件也是一个程序员所必备的技能。\n 对于一个大工程，编写makefile实在是件复杂的事，于是人们又想，为什么不设计一个工具，读入所有源文件之后，自动生成makefile呢，于是就出现了`cmake`工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。所以在编程的世界里没有捷径可走，还是要脚踏实地的。\n\n 原文件－－camkelist ---cmake ---makefile ---make ---生成可执行文件\n\n# 二、`CMake基本语法规则`\n\n1. 变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名\n\n2. 指令(参数1  参数2  ...)\n\n   参数使用括弧括起，参数之间使用空格或分号分开\n\n3. 指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令\n\n4. 关于双引号的疑惑\n\n   ```shell\n   SET(SRC_LIST main.c)也可以写成 SET(SRC_LIST “main.c”)\n   是没有区别的，但是假设一个源文件的文件名是 fu nc.c(文件名中间包含了空格)。这时候就必须使用双引号，如果写成了 SET(SRC_LIST fu nc.c)，就会出现错误，提示你找不到 fu 文件和 nc.c 文件。这种情况，就必须写成:SET(SRC_LIST “fu nc.c”)\n   ```\n\n   \n\n# 三、内部构建与外部构建\n\n内部构建就是在项目跟目录直接编译\n\n引出了我们对外部编译的探讨，外部编译的过程如下：\n\n1. 首先，请清除 t1 目录中除 main.c CmakeLists.txt 之外的所有中间文件，最关键的是 CMakeCache.txt。\n2. 在 t1 目录中建立 build 目录，当然你也可以在任何地方建立 build 目录，不一定必须在工程目录中。\n3. 进入 build 目录，运行 cmake ..(注意,..代表父目录，因为父目录存在我们需要的CMakeLists.txt，如果你在其他地方建立了 build 目录，需要运行 cmake <工程的全路径>)，查看一下 build 目录，就会发现了生成了编译需要的 Makefile 以及其他的中间文件.\n4. 运行 make 构建工程，就会在当前目录(build 目录)中获得目标文件 hello。\n5. 上述过程就是所谓的 out-of-source 外部编译，一个最大的好处是，对于原有的工程没有任何影响，所有动作全部发生在编译目录。通过这一点，也足以说服我们全部采用外部编译方式构建工程。\n6. 这里需要特别注意的是：\n   通过外部编译进行工程构建，HELLO_SOURCE_DIR 仍然指代工程路径，即/backup/cmake/t1, 而 HELLO_BINARY_DIR 则指代编译路径，即/backup/cmake/t1/build\n\n#　四、安装库和INSTALL指令\n\n有两种安装方式，一种是从代码编译后直接 make install 安装，一种是cmake的install 指令安装。\n\n## 1、`make install`\n\n```shell\nDESTDIR=\ninstall:\n\tmkdir -p $(DESTDIR)/usr/bin\n\tinstall -m 755 hello $(DESTDIR)/usr/bin\n你可以通过:\n\tmake install\n将 hello 直接安装到/usr/bin 目录，也可以通过 make install\nDESTDIR=/tmp/test 将他安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。稍微复杂一点的是还需要定义 PREFIX，一般 autotools 工程，会运行这样的指令:\n./configure –prefix=/usr \n或者./configure --prefix=/usr/local \n来指定PREFIX\n比如上面的 Makefile 就可以改写成:\nDESTDIR=\nPREFIX=/usr\ninstall:\n\tmkdir -p $(DESTDIR)/$(PREFIX)/bin\n\tinstall -m 755 hello $(DESTDIR)/$(PREFIX)/bin\n```\n\n\n\n## 2、`cmake INSTALL`指令安装\n\n这里需要引入一个新的 cmake 指令 INSTALL 和一个非常有用的变量\nCMAKE_INSTALL_PREFIX。CMAKE_INSTALL_PREFIX 变量类似于 configure 脚本的 –prefix，常见的使用方法看起来是这个样子：\n\t`cmake -DCMAKE_INSTALL_PREFIX=/usr ..`\nINSTALL 指令用于定义安装规则，安装的内容可以包括目标二进制、动态库、静态库以及文件、目录、脚本等。\n\nINSTALL 指令包含了各种安装类型，我们需要一个个分开解释：\n目标文件的安装：\n\n```\nINSTALL(TARGETS targets...\n\t[[ARCHIVE|LIBRARY|RUNTIME]\n\t[DESTINATION <dir>]\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[OPTIONAL]\n] [...])\n```\n\n\n\n参数中的 TARGETS 后面跟的就是我们通过 ADD_EXECUTABLE 或者 ADD_LIBRARY 定义的\n目标文件，可能是可执行二进制、动态库、静态库。\n目标类型也就相对应的有三种，ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME\n特指可执行目标二进制。\nDESTINATION 定义了安装的路径，如果路径以/开头，那么指的是绝对路径，这时候\nCMAKE_INSTALL_PREFIX 其实就无效了。如果你希望使用 CMAKE_INSTALL_PREFIX 来\n定义安装路径，就要写成相对路径，即不要以/开头，那么安装后的路径就是\n${CMAKE_INSTALL_PREFIX}/<DESTINATION 定义的路径>\n举个简单的例子：\n\n```shell\nINSTALL(TARGETS myrun mylib mystaticlib\n\tRUNTIME DESTINATION bin\n\tLIBRARY DESTINATION lib\n\tARCHIVE DESTINATION libstatic\n)\n```\n\n上面的例子会将：\n可执行二进制 myrun 安装到${CMAKE_INSTALL_PREFIX}/bin 目录\n动态库 libmylib 安装到${CMAKE_INSTALL_PREFIX}/lib 目录\n静态库 libmystaticlib 安装到${CMAKE_INSTALL_PREFIX}/libstatic 目录\n特别注意的是你不需要关心 TARGETS 具体生成的路径，只需要写上 TARGETS 名称就可以\n了。  \n\n普通文件的安装：\n\n```shell\nINSTALL(FILES files... DESTINATION <dir>\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[RENAME <name>] [OPTIONAL])\n```\n\n\n\n可用于安装一般文件，并可以指定访问权限，文件名是此指令所在路径下的相对路径。如果\n默认不定义权限 PERMISSIONS，安装后的权限为：\nOWNER_WRITE, OWNER_READ, GROUP_READ,和 WORLD_READ，即 644 权限。\n非目标文件的可执行程序安装(比如脚本之类)：\n\n```\nINSTALL(PROGRAMS files... DESTINATION <dir>\n\t[PERMISSIONS permissions...]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[RENAME <name>] [OPTIONAL])\n```\n\n跟上面的 FILES 指令使用方法一样，唯一的不同是安装后权限为:\nOWNER_EXECUTE, GROUP_EXECUTE, 和 WORLD_EXECUTE，即 755 权限\n目录的安装：\n\n```shell\nINSTALL(DIRECTORY dirs... DESTINATION <dir>\n\t[FILE_PERMISSIONS permissions...]\n\t[DIRECTORY_PERMISSIONS permissions...]\n\t[USE_SOURCE_PERMISSIONS]\n\t[CONFIGURATIONS [Debug|Release|...]]\n\t[COMPONENT <component>]\n\t[[PATTERN <pattern> | REGEX <regex>]\n\t[EXCLUDE] [PERMISSIONS permissions...]] [...])\n```\n\n\n这里主要介绍其中的 DIRECTORY、PATTERN 以及 PERMISSIONS 参数。\n\nDIRECTORY 后面连接的是所在 Source 目录的相对路径，但务必注意：abc 和 abc/有很大的区别。\n如果目录名不以/结尾，那么这个目录将被安装为目标路径下的 abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。\nPATTERN 用于使用正则表达式进行过滤，PERMISSIONS 用于指定 PATTERN 过滤后的文件权限。\n我们来看一个例子:\n\n```shell\nINSTALL(DIRECTORY icons scripts/ DESTINATION \tshare/myproj\nPATTERN \"CVS\" EXCLUDE\nPATTERN \"scripts/*\"\nPERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ\nGROUP_EXECUTE GROUP_READ)\n\n```\n\n这条指令的执行结果是：\n将 icons 目录安装到 <prefix>/share/myproj，将 scripts/中的内容安装到<prefix>/share/myproj不包含目录名为 CVS 的目录，对于 scripts/*  文件指定权限为 OWNER_EXECUTE   OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ.\n\n安装时 CMAKE 脚本的执行：\n\n```\nINSTALL([[SCRIPT <file>] [CODE <code>]] [...])\nSCRIPT 参数用于在安装时调用 cmake 脚本文件（也就是<abc>.cmake 文件）\nCODE 参数用于执行 CMAKE 指令，必须以双引号括起来。比如：\nINSTALL(CODE \"MESSAGE(\\\"Sample install message.\\\")\")\n```\n\n\n\n# 五、静态库和动态库构建\n\n## 1、ADD_LIBRARY指令\n\n```shell\nADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n\t[EXCLUDE_FROM_ALL]\n\tsource1 source2 ... sourceN)\n# 不需要写全lib<libname>.so, 只需要填写<libname>,cmake系统会自动为你生成，lib<libname>.X\n\n# 类型有三种:\n\tSHARED，动态库\t.so\n\tSTATIC，静态库\t.a\n\tMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\n\t\n#EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。\n```\n\n\n## 2、指定库的生成路径\n\n​\t两种方法\n\n1. ADD_SUBDIRECTORY指令来指定一个编译输出位置\n2. 在CMakeLists.txt中添加　SET(LIBRARY_OUTPUT_PATH <路径>)来指定一个新的位置\n\n\n## 3、同时生成动态库和静态库\n\n因为ADD_SUBDIRECTORY的TARGET(libname)是唯一的，所以生成动态库和静态库不能指定相同的名称，想要有相同的名称需要用到SET_TARGET_PROPERTIES指令。\n\nSET_TARGET_PROPERTIES，其基本语法是：\n\n```shell\nSET_TARGET_PROPERTIES(target1 target2 ...\n\tPROPERTIES prop1 value1\n\tprop2 value2 ...)\n# 举例\nADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})　# 动态库\nADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) # 静态库\nSET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \"hello\")\n```\n\n这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。\n\n与他对应的指令是：\n\tGET_TARGET_PROPERTY(VAR target property)\n\n举例\n\n```shell\nGET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME)\nMESSAGE(STATUS “This is the hello_static\nOUTPUT_NAME:”${OUTPUT_VALUE})\n# 如果没有这个属性定义，则返回 NOTFOUND.\n```\n\n## 4、动态库版本号\n\n```shell\nSET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1)\n# VERSION 指代动态库版本，SOVERSION 指代 API 版本。\n# 在 build/lib 目录会生成：\n    libhello.so.1.2\n    libhello.so.1->libhello.so.1.2\n    libhello.so -> libhello.so.1\n```\n\n\n\n# 六、使用共享库和头文件\n\n## 1.`INCLUDE_DIRECTORIES`指令\n\n`INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)`\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１. CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。\n２. 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。\n\n## 2. `LINK_DIRECTORIES`和 `TARGET_LINK_LIBRARIES`\n\n```shell\nLINK_DIRECTORIES(directory1 directory2 ...)\n# 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制，在编译时就需要指定一下这些共享库的路径。\n# TARGET_LINK_LIBRARIES 的全部语法是:\nTARGET_LINK_LIBRARIES(target library1\n\t<debug | optimized> library2\n...)\n# 这个指令可以用来为 target 添加需要链接的共享库\n```\n\n## 3. `FIND`系列指令\n\n1. 特殊的环境变量` CMAKE_INCLUDE_PATH` 和`CMAKE_LIBRARY_PATH`\n\n   务必注意，这两个是环境变量而不是 cmake 变量\n\n2. `CMAKE_INCLUDE_PATH`和`CMAKE_LIBRARY_PATH`是配合`FIND_PATH`和`FIND_LIBRARY`指令使用的\n\n3. find_path指令\n\n   ```shell\n   find_path (<VAR> NAMES name)\n   # <VAR>查找的库文件路径报存在变量VAR中\n   # 默认搜索路径为`CMAKE_INCLUDE_PATH`\n   \n   find_path (<VAR> NAMES name PATHS paths... [NO_DEFAULT_PATH])\n   #　指定搜索路径\n   # NO_DEFAULT_PATH　不使用默认搜索路径　\n   # 举例\n   为了将程序更智能一点，我们可以使用 CMAKE_INCLUDE_PATH 来进行，使用 bash 的方法\n   如下：export CMAKE_INCLUDE_PATH=/usr/include/hello\n   然后在头文件中将 INCLUDE_DIRECTORIES(/usr/include/hello)替换为：\n   FIND_PATH(myHeader hello.h)\n   IF(myHeader)\n   \tINCLUDE_DIRECTORIES(${myHeader})\n   ENDIF(myHeader)\n   ```\n\n## 4. 共享库和头文件指令总结\n\n1. **FIND_PATH** 查找头文件所在目录\n2. **INCLUDE_DIRECTORIES**　添加头文件目录\n3. **FIND_LIBRARY** 查找库文件所在目录\n4. **LINK_DIRECTORIES**   添加库文件目录\n5. **LINK_LIBRARIES**　添加需要链接的库文件路径，注意这里是全路径\n6. **TARGET_LINK_LIBRARIES **　给TARGET链接库\n\n\n\n# 七、Find模块\n\n## 1.Find模块使用\n\n```shell\nFIND_PACKAGE(XXX)\nIF(XXX_FOUND)\n\tINCLUDE_DIRECTORIES(${XXX_INCLUDE_DIR})\n\tTARGET_LINK_LIBRARIES(xxxtest ${XXX_LIBRARY})\nELSE(XXX_FOUND)\n\tMESSAGE(FATAL_ERROR ”XXX library not found”)\nENDIF(XXX_FOUND)\n```\n\n对于系统预定义的 Find<name>.cmake 模块，使用方法一般如上例所示：\n每一个模块都会定义以下几个变量\n\t• <name>_FOUND\n\t• <name>_INCLUDE_DIR or <name>_INCLUDES\n\t• <name>_LIBRARY or <name>_LIBRARIES\n你可以通过<name>_FOUND 来判断模块是否被找到，如果没有找到，按照工程的需要关闭某些特性、给出提醒或者中止编译\n\n\n\n\n\n## 2.find_package指令\n\n```shell\nfind_package(<PackageName> [QUIET] [REQUIRED] [[COMPONENTS] [components...]]\n             [OPTIONAL_COMPONENTS components...]\n             [NO_POLICY_SCOPE])\n             \n# 查找并从外部项目加载设置，\n# <PackageName>_FOUND 将设置为指示是否找到该软件包, 如果查找到，该变量为true\n# [QUIET], 设置该变量，不会打印任何消息，且\t\t   <PackageName>_FIND_QUIETLY为true\n# [REQUIRED] 设置该变量，如果找不到软件包，该选项将停止处理并显示一条错误消息，且设置<PackageName>_FIND_REQUIRED为true,不过不指定该参数，即使没有找到，也能编译通过\n```\n\nfind_package采用两种模式搜索库：\n\n-  **Module模式**：搜索**CMAKE_MODULE_PATH**指定路径下的**FindXXX.cmake**文件，执行该文件从而找到XXX库。其中，具体查找库并给**XXX_INCLUDE_DIRS**和**XXX_LIBRARIES**两个变量赋值的操作由FindXXX.cmake模块完成。\n-  **Config模式**：搜索**XXX_DIR**指定路径下的**XXXConfig.cmake**文件，执行该文件从而找到XXX库。其中具体查找库并给**XXX_INCLUDE_DIRS**和**XXX_LIBRARIES**两个变量赋值的操作由XXXConfig.cmake模块完成。\n\n两种模式看起来似乎差不多，不过cmake默认采取**Module**模式，如果Module模式未找到库，才会采取Config模式。如果**XXX_DIR**路径下找不到XXXConfig.cmake或`<lower-case-package-name>`config.cmake文件，则会找/usr/local/lib/cmake/XXX/中的XXXConfig.cmake文件。总之，Config模式是一个备选策略。通常，库安装时会拷贝一份XXXConfig.cmake到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。\n\n总结：CMake搜索的顺序为: 首先在`CMAKE_MODULE_PATH`中搜索名为`Find<PackageName>.cmake`的文件，然后在`<PackageName>_DIR`名为`PackageName>Config.cmake`或`<lower-case-package-name>-config.cmake`的文件，如果还是找不到，则会去`/usr/local/lib/cmake`中查找`Find<PackageName>.cmake`文件。\n\n所以我们可以通过`CMAKE_MODULE_PATH`或`<PackageName>_DIR`变量指定cmake文件路径。\n\n## 3.自定义Find模块\n\n```shell\n# 查找HELLO的头文件目录\nFIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello\n/usr/local/include/hello)\n# 查找HELLO的动态库\nFIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib\n/usr/local/lib)\nIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\n\tSET(HELLO_FOUND TRUE)\nENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\nIF (HELLO_FOUND)\n\t# 如果不指定QUIET参数，就打印信息\n\tIF (NOT HELLO_FIND_QUIETLY)\n\t\tMESSAGE(STATUS \"Found Hello: ${HELLO_LIBRARY}\")\n\tENDIF (NOT HELLO_FIND_QUIETLY)\nELSE (HELLO_FOUND)\n\t# 如果设置了REQUIRED参数就报错\n\tIF (HELLO_FIND_REQUIRED)\n\t\tMESSAGE(FATAL_ERROR \"Could not find hello library\")\n\tENDIF (HELLO_FIND_REQUIRED)\nENDIF (HELLO_FOUND)\n```\n\n# 八、`CMake`常用变量\n\n## 1.`cmake` 变量引用的方式：\n\n使用${}进行变量的引用。在 IF 等语句中，是直接使用变量名而不通过${}取值\n\n## 2.`cmake` 自定义变量的方式：\n\n主要有隐式定义和显式定义两种，前面举了一个隐式定义的例子，就是 PROJECT 指令，他会隐式的定义<projectname>_BINARY_DIR 和<projectname>_SOURCE_DIR 两个变量。\n显式定义的例子我们前面也提到了，使用 SET 指令，就可以构建一个自定义变量了。比如:\n\nSET(HELLO_SRC main.SOURCE_PATHc)，就PROJECT_BINARY_DIR 可以通过${HELLO_SRC}来引用这个自定义变量了.\n\n## 3.`cmake` 常用变量\n\n### 1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/<projectname>_BINARY_DIR_\n\n这三个变量指代的内容是一致的，如果是 in source 编译，指得就是工程顶层目录，如果是 out-of-source 编译，指的是工程编译发生的目录。PROJECT_BINARY_DIR 跟其他指令稍有区别，现在，你可以理解为他们是一致的。\n\n### 2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/<projectname>_SOURCE_DIR\n\n这三个变量指代的内容是一致的，不论采用何种编译方式，都是工程顶层目录。\n\n### 3. CMAKE_CURRENT_SOURCE_DIR\n\n指的是**当前处理的** CMakeLists.txt 所在的路径\n\n### 4. CMAKE_CURRRENT_BINARY_DIR\n\n如果是 in-source 编译，它跟 CMAKE_CURRENT_SOURCE_DIR 一致，如果是 out-ofsource 编译，他指的是 target 编译目录。\n使用我们上面提到的 ADD_SUBDIRECTORY(src bin)可以更改这个变量的值。\n使用 SET(EXECUTABLE_OUTPUT_PATH <新路径>)并不会对这个变量造成影响，它仅仅修改了最终目标文件存放的路径。\n\n### ５. CMAKE_CURRENT_LIST_FILE\n\n​\t输出调用这个变量的 CMakeLists.txt 的完整路径\n\n### 6. CMAKE_CURRENT_LIST_LINE\n\n​\t输出这个变量所在的行\n\n### 7. CMAKE_MODULE_PATH\n\n这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂，有可能会自己编写一些 cmake 模块，这些 cmake 模块是随你的工程发布的，为了让 cmake 在处理CMakeLists.txt 时找到这些模块，你需要通过 SET 指令，将自己的 cmake 模块路径设\n置一下。比如\nSET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n这时候你就可以通过 INCLUDE 指令来调用自己的模块了。\n\n### 8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH\n\n分别用来重新定义最终结果的存放目录，前面我们已经提到了这两个变量。\n\n### 9. PROJECT_NAME\n\n返回通过 PROJECT 指令定义的项目名称。\n\n## 4. cmake 调用环境变量的方式\n\n使用$ENV{NAME}指令就可以调用系统的环境变量了。\n比如MESSAGE(STATUS “HOME dir: $ENV{HOME}”)\n设置环境变量的方式是：SET(ENV{变量名} 值)\n\n### 1. CMAKE_INCLUDE_CURRENT_DIR\n\n自动添加 CMAKE_CURRENT_BINARY_DIR 和 CMAKE_CURRENT_SOURCE_DIR 到当前处理\n的 CMakeLists.txt。相当于在每个 CMakeLists.txt 加入：\nINCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR}\n${CMAKE_CURRENT_SOURCE_DIR})\n\n### 2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE\n\n将工程提供的头文件目录始终至于系统头文件目录的前面，当你定义的头文件确实跟系统发生冲突时可以提供一些帮助。\n\n### 3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。\n\n## 5. 系统信息\n\n1. CMAKE_MAJOR_VERSION，CMAKE 主版本号，比如 2.4.6 中的 2\n\n2. CMAKE_MINOR_VERSION，CMAKE 次版本号，比如 2.4.6 中的 4\n\n3. CMAKE_PATCH_VERSION，CMAKE 补丁等级，比如 2.4.6 中的 6\n\n4. CMAKE_SYSTEM，系统名称，比如 Linux-2.6.22\n\n5. CMAKE_SYSTEM_NAME，不包含版本的系统名，比如 Linux\n\n6. CMAKE_SYSTEM_VERSION，系统版本，比如 2.6.22\n\n7. CMAKE_SYSTEM_PROCESSOR，处理器名称，比如 i686.\n\n8. UNIX，在所有的类 UNIX 平台为 TRUE，包括 OS X 和 cygwin\n\n9. WIN32，在所有的 win32 平台为 TRUE，包括 cygwin\n\n   \n\n## 6.主要的开关选项：\n\n1. CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS，用来控制 IF ELSE 语句的书写方式，在\n   下一节语法部分会讲到。\n\n2. BUILD_SHARED_LIBS\n   这个开关用来控制默认的库编译方式，如果不进行设置，使用 ADD_LIBRARY 并没有指定库类型的情况下，默认编译生成的库都是静态库。\n   如果 SET(BUILD_SHARED_LIBS ON)后，默认生成的为动态\n\n3. CMAKE_C_FLAGS\n   设置 C 编译选项，也可以通过指令 ADD_DEFINITIONS()添加。\n\n4. CMAKE_CXX_FLAGS\n   设置 C++编译选项，也可以通过指令 ADD_DEFINITIONS()添加。\n\n   \n\n# 九、`CMake`常用指令\n\n## 1. 基本指令\n\n### MESSAGE\n\n```shell\nmessage([<mode>] \"message to display\" ...)\n可选<mode>关键字确定消息的类型:\nFATAL_ERROR\t立即终止所有 cmake 过程\nSEND_ERROR 产生错误，生成过程被跳过\nWARNING\nAUTHOR_WARNING\nNOTICE\nSTATUS\t输出前缀为—的信息\nVERBOSE\nDEBUG\nTRACE\n```\n\n### PROJECT\n\n```shell\nproject(<PROJECT-NAME> [<language-name>...])\nproject(<PROJECT-NAME>\n        [VERSION <major>[.<minor>[.<patch>[.<tweak>]]]]\n        [LANGUAGES <language-name>...])\n        \n设置项目的名称，并将其存储在变量中 PROJECT_NAME。从顶层调用时， CMakeLists.txt还将项目名称存储在变量CMAKE_PROJECT_NAME中。\n\n同时设置变量\n\nPROJECT_SOURCE_DIR， <PROJECT-NAME>_SOURCE_DIR\nPROJECT_BINARY_DIR， <PROJECT-NAME>_BINARY_DIR\n\nhttps://cmake.org/cmake/help/v3.15/command/project.html\n```\n\n\n\n### SET\n\n```shell\n将普通变量，缓存变量或环境变量设置为给定值。\n指定<value>...占位符的此命令的签名期望零个或多个参数。多个参数将以分号分隔的列表形式加入，以形成要设置的实际变量值。零参数将导致未设置普通变量。unset() 命令显式取消设置变量。\n1、设置正常变量\nset(<variable> <value>... [PARENT_SCOPE])\n<variable>在当前函数或目录范围内设置给定值。\n如果PARENT_SCOPE给出了该选项，则将在当前作用域上方的作用域中设置变量。\n2、设置缓存变量\nset(<variable> <value>... CACHE <type> <docstring> [FORCE])\n3、设置环境变量\nset(ENV{<variable>} [<value>])\n\n```\n\n\n\n### add_executable\n\n```shell\n使用指定的源文件生成可执行文件\nadd_executable(<name> [WIN32] [MACOSX_BUNDLE]\n               [EXCLUDE_FROM_ALL]\n               [source1] [source2 ...])\n<name>可执行文件名, <name>与逻辑目标名称相对应，并且在项目中必须是全局唯一的。构建的可执行文件的实际文件名是基于本机平台（例如<name>.exe或<name>）的约定构造的 。\n默认情况下，将在与调用命令的源树目录相对应的构建树目录中创建可执行文件。\n               \n```\n\n### add_subdirectory\n\n```shell\n在构建中添加一个子目录。\nadd_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])\n将一个子目录添加到构建中。source_dir指定源CMakeLists.txt和代码文件所在的目录。binary_dir指定了输出文件放置的目录以及编译输出的路径。EXCLUDE_FROM_ALL 参数的含义是将这个目录从编译过程中排除，比如，工程的 example，可能就需要工程构建完成后，再进入 example 目录单独进行构建(当然，你也可以通过定义依赖来解决此类问题)。\n如果没有指定binary_dir,那么编译结果(包括中间结果)都将存放在\nbuild/source_dir 目录(这个目录跟原有的 source_dir 目录对应)，指定binary_dir 目录后，相当于在编译时将 source_dir 重命名为binary_dir，所有的中间结果和目标二进制都将存放在binary_dir 目录。\n```\n\n### subdirs\n\n```shell\n构建多个子目录\nsubdirs(dir1 dir2 ...[EXCLUDE_FROM_ALL exclude_dir1 exclude_dir2 ...]\n        [PREORDER] )\n        \n            \n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录)，我们都可以通过 SET 指令重新定义EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 变量\n来指定最终的目标二进制的位置(指最终生成的 hello 或者最终的共享库，不包含编译生成的中间文件)\nSET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\nSET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\n在第一节我们提到了<projectname>_BINARY_DIR 和 PROJECT_BINARY_DIR 变量，他们指的编译发生的当前目录，如果是内部编译，就相当于 PROJECT_SOURCE_DIR 也就是工程代码所在目录，如果是外部编译，指的是外部编译所在目录，也就是本例中的两个指令分别定义了：可执行二进制的输出路径为 build/bin 和库的输出路径为 build/lib.\n```\n\n\n\n### add_library\n\n```shell\nADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n[EXCLUDE_FROM_ALL]\nsource1 source2 ... sourceN)\n你不需要写全 libhello.so，只需要填写 hello 即可，cmake 系统会自动为你生成\nlibhello.X\n类型有三种:\nSHARED，动态库\nSTATIC，静态库\nMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\nEXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手\n工构建。\n```\n\n\n\n### include_directories\n\n```shell\n将include目录添加到构建中\ninclude_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...])\n将给定目录添加到编译器用于搜索头文件的路径中。\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径\n中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１，CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以\n将添加的头文件搜索路径放在已有路径的前面。\n２，通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。\n```\n\n### target_link_libraries & link_directories\n\n```shell\nTARGET_LINK_LIBRARIES(target library1\n<debug | optimized> library2\n...)\n这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样\n可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误，我们需要作的是向\nsrc/CMakeLists.txt 中添加如下指令：\nTARGET_LINK_LIBRARIES(main hello)\n也可以写成\nTARGET_LINK_LIBRARIES(main libhello.so)\n```\n\n### ADD_DEFINITIONS\n\n```shell\n向 C/C++编译器添加-D 定义，比如:\nADD_DEFINITIONS(-DENABLE_DEBUG -DABC)，参数之间用空格分割。\n如果你的代码中定义了#ifdef ENABLE_DEBUG #endif，这个代码块就会生效。如果要添加其他的编译器开关，可以通过 CMAKE_C_FLAGS 变量和 CMAKE_CXX_FLAGS 变量设置。\n```\n\n\n\n### ADD_DEPENDENCIES\n\n```shell\n定义 target 依赖的其他 target，确保在编译本 target 之前，其他的 target 已经被构建。\nADD_DEPENDENCIES(target-name depend-target1\ndepend-target2 ...)\n```\n\n\n\n### ADD_TEST 与 ENABLE_TESTING 指令。\n\n```shell\nENABLE_TESTING 指令用来控制 Makefile 是否构建 test 目标，涉及工程所有目录。语法很简单，没有任何参数，ENABLE_TESTING()，一般情况这个指令放在工程的主CMakeLists.txt 中.\nADD_TEST 指令的语法是:\n\t`ADD_TEST(testname Exename arg1 arg2 ...)`\ntestname 是自定义的 test 名称，Exename 可以是构建的目标文件也可以是外部脚本等等。后面连接传递给可执行文件的参数。如果没有在同一个 CMakeLists.txt 中打开\n\tENABLE_TESTING()指令，任何 ADD_TEST 都是无效的。\n比如我们前面的 Helloworld 例子，可以在工程主 CMakeLists.txt 中添加\n\nADD_TEST(mytest ${PROJECT_BINARY_DIR}/bin/main)\nENABLE_TESTING()\n生成 Makefile 后，就可以运行 make test 来执行测试了。\n```\n\n\n\n### AUX_SOURCE_DIRECTORY\n\n```shell\n基本语法是：\nAUX_SOURCE_DIRECTORY(dir VARIABLE)\n作用是发现一个目录下所有的源代码文件并将列表存储在一个变量中，这个指令临时被用来\n自动构建源文件列表。因为目前 cmake 还不能自动发现新添加的源文件。\n比如\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nADD_EXECUTABLE(main ${SRC_LIST})\n你也可以通过后面提到的 FOREACH 指令来处理这个 LIST\n```\n\n\n\n###　CMAKE_MINIMUM_REQUIRED\n\n```sehll\n其语法为 CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n比如 CMAKE_MINIMUM_REQUIRED(VERSION 2.5 FATAL_ERROR)\n如果 cmake 版本小与 2.5，则出现严重错误，整个过程中止。\n```\n\n\n\n### EXEC_PROGRAM\n\n在 CMakeLists.txt 处理过程中执行命令，并不会在生成的 Makefile 中执行。具体语法为：\n\n```shell\nEXEC_PROGRAM(Executable [directory in which to run]\n[ARGS <arguments to executable>]\n[OUTPUT_VARIABLE <var>]\n[RETURN_VALUE <var>])\n```\n\n用于在指定的目录运行某个程序，通过 ARGS 添加参数，如果要获取输出和返回值，可通过OUTPUT_VARIABLE 和 RETURN_VALUE 分别定义两个变量.\n这个指令可以帮助你在 CMakeLists.txt 处理过程中支持任何命令，比如根据系统情况去修改代码文件等等。\n举个简单的例子，我们要在 src 目录执行 ls 命令，并把结果和返回值存下来。\n可以直接在 src/CMakeLists.txt 中添加：\nEXEC_PROGRAM(ls ARGS \"*.c\" OUTPUT_VARIABLE LS_OUTPUT RETURN_VALUE LS_RVALUE)\nIF(not LS_RVALUE)\n\tMESSAGE(STATUS \"ls result: \" ${LS_OUTPUT})\nENDIF(not LS_RVALUE)\n在 cmake 生成 Makefile 的过程中，就会执行 ls 命令，如果返回 0，则说明成功执行，\n那么就输出 ls *.c 的结果。关于 IF 语句，后面的控制指令会提到。\n\n### FILE 指令\n\n文件操作指令，基本语法为:\n\n```shell\nFILE(WRITE filename \"message to write\"... )\nFILE(APPEND filename \"message to write\"... )\nFILE(READ filename variable)\nFILE(GLOB variable [RELATIVE path] [globbing\nexpressions]...)\nFILE(GLOB_RECURSE variable [RELATIVE path]\n[globbing expressions]...)\nFILE(REMOVE [directory]...)\nFILE(REMOVE_RECURSE [directory]...)\nFILE(MAKE_DIRECTORY [directory]...)\nFILE(RELATIVE_PATH variable directory file)\nFILE(TO_CMAKE_PATH path result)\nFILE(TO_NATIVE_PATH path result)\n```\n\n\n\n这里的语法都比较简单，不在展开介绍了。\n\n### INCLUDE 指令\n\n```shell\n用来载入 CMakeLists.txt 文件，也用于载入预定义的 cmake 模块.\n\tINCLUDE(file1 [OPTIONAL])\n\tINCLUDE(module [OPTIONAL])\nOPTIONAL 参数的作用是文件不存在也不会产生错误。\n你可以指定载入一个文件，如果定义的是一个模块，那么将在 CMAKE_MODULE_PATH 中搜索这个模块并载入。\n载入的内容将在处理到 INCLUDE 语句是直接执行。\n```\n\n\n\n## 2. 控制指令：\n\n### 1. IF 指令\n\n基本语法为：\n\n```shell\nIF(expression)\n\n# THEN section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nELSE(expression)\n\n# ELSE section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDIF(expression)\n```\n\n\n\n另外一个指令是 ELSEIF，总体把握一个原则，凡是出现 IF 的地方一定要有对应的\nENDIF.出现 ELSEIF 的地方，ENDIF 是可选的。\n表达式的使用方法如下:\nIF(var)，如果变量不是：空，0，N, NO, OFF, FALSE, NOTFOUND 或\n<var>_NOTFOUND 时，表达式为真。\nIF(NOT var )，与上述条件相反。\nIF(var1 AND var2)，当两个变量都为真是为真。\nIF(var1 OR var2)，当两个变量其中一个为真时为真。\nIF(COMMAND cmd)，当给定的 cmd 确实是命令并可以调用是为真。\nIF(EXISTS dir)或者 IF(EXISTS file)，当目录名或者文件名存在时为真。\nIF(file1 IS_NEWER_THAN file2)，当 file1 比 file2 新，或者 file1/file2 其中有一个不存在时为真，文件名请使用完整路径。\nIF(IS_DIRECTORY dirname)，当 dirname 是目录时，为真。\nIF(variable MATCHES regex)\nIF(string MATCHES regex)\n当给定的变量或者字符串能够匹配正则表达式 regex 时为真。比如：\nIF(\"hello\" MATCHES \"ell\")\nMESSAGE(\"true\")\nENDIF(\"hello\" MATCHES \"ell\")\nIF(variable LESS number)\nIF(string LESS number)\nIF(variable GREATER number)\nIF(string GREATER number)\nIF(variable EQUAL number)\nIF(string EQUAL number)\n数字比较表达式\nIF(variable STRLESS string)\nIF(string STRLESS string)\nIF(variable STRGREATER string)\nIF(string STRGREATER string)\nIF(variable STREQUAL string)\nIF(string STREQUAL string)\n按照字母序的排列进行比较.\nIF(DEFINED variable)，如果变量被定义，为真。\n一个小例子，用来判断平台差异：\nIF(WIN32)\nMESSAGE(STATUS “This is windows.”)\n#作一些 Windows 相关的操作\nELSE(WIN32)\nMESSAGE(STATUS “This is not windows”)\n#作一些非 Windows 相关的操作\nENDIF(WIN32)\n上述代码用来控制在不同的平台进行不同的控制，但是，阅读起来却并不是那么舒服，\nELSE(WIN32)之类的语句很容易引起歧义。\n这就用到了我们在“常用变量”一节提到的 CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS 开\n关。\n可以 SET(CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS ON)\n这时候就可以写成:\nIF(WIN32)\nELSE()\nENDIF()\n如果配合 ELSEIF 使用，可能的写法是这样:\nIF(WIN32)\n#do something related to WIN32\nELSEIF(UNIX)\n#do something related to UNIX\nELSEIF(APPLE)\n#do something related to APPLE\nENDIF(WIN32)\n\n### 2. WHILE\n\nWHILE 指令的语法是：\n\n```shell\nWHILE(condition)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDWHILE(condition)\n```\n\n\n\n其真假判断条件可以参考 IF 指令。\n\n### 3. FOREACH\n\nFOREACH 指令的使用方法有三种形式：\n\n```shell\n1，列表\nFOREACH(loop_var arg1 arg2 ...)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDFOREACH(loop_var)\n像我们前面使用的 AUX_SOURCE_DIRECTORY 的例子\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nFOREACH(F ${SRC_LIST})\nMESSAGE(${F})\nENDFOREACH(F)\n2，范围\nFOREACH(loop_var RANGE total)\nENDFOREACH(loop_var)\n从 0 到 total 以１为步进\n举例如下：\nFOREACH(VAR RANGE 10)\nMESSAGE(${VAR})\nENDFOREACH(VAR)\n最终得到的输出是：\n0 1 2 3 4 5 6 7 8 9\n10\n３，范围和步进\nFOREACH(loop_var RANGE start stop [step])\nENDFOREACH(loop_var)\n从 start 开始到 stop 结束，以 step 为步进，\n举例如下\nFOREACH(A RANGE 5 15 3)\nMESSAGE(${A})\nENDFOREACH(A)\n最终得到的结果是：\n5 8\n11\n14\n这个指令需要注意的是，知道遇到 ENDFOREACH 指令，整个语句块才会得到真正的执行。\n```\n\n\n\n# 十、`CMakeLists`配置模板\n\n## １.基本配置\n\n```shell\ncmake_minimum_required(VERSION 3.14)\nproject(XXX_Project)\n\n# 设置CMAKE版本\nset(CMAKE_CXX_STANDARD 14)\n\n# 设置输出目录为 build/Debug/bin build/Debug/lib\n# 并缓存路径\nset(OUTPUT_DIRECTORY_ROOT ${CMAKE_CURRENT_SOURCE_DIR}/build/${CMAKE_BUILD_TYPE})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/bin\" CACHE PATH \"Runtime directory\" FORCE)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Library directory\" FORCE)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Archive directory\" FORCE)\n\n# 添加src子目录\nadd_subdirectory(src)\n```\n\n## ２.依赖库相关配置\n\n**`OPenCV`依赖库**\n\n将`OpenCV`依赖库下的`share/OpenCV`中，`OpenCVConfig.cmake`复制一份叫`FindOpenCV.cmake`，然后在根目录的CMakeLists.txt添加如下配置\n\n```shell\n#　添加make文件搜索路径\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ~/3rdparty/OpenCV-3.4.7/share/OpenCV)\n\n# 查找cmake文件，并初始化变量\nfind_package(OpenCV REQUIRED)\n# 添加头文件搜索路径\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n\n# 给执行程序添加链接库\nadd_executable(XXXXMain main.cpp)\ntarget_link_libraries(XXXXMain ${OpenCV_LIBS})\n```\n\n\n\n# 十一、参考\n\n1. [http://file.ncnynl.com/ros/CMake%20Practice.pdf](http://file.ncnynl.com/ros/CMake Practice.pdf)\n2. https://cmake.org/cmake/help/latest/guide/tutorial/index.html","slug":"CMake用法总结","published":1,"updated":"2019-12-02T08:53:06.517Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tro0001zsv5lpyfdiwc","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><h1 id=\"一、CMake的作用\"><a href=\"#一、CMake的作用\" class=\"headerlink\" title=\"一、CMake的作用\"></a>一、<code>CMake</code>的作用</h1><p>大家都知道, 源文件的编译步骤为:</p>\n<ul>\n<li>预处理: 宏定义展开, 头文件展开, 条件编译</li>\n<li>编译: 检查语法, 生成编译文件</li>\n<li>汇编: 将汇编文件生成目标文件(二进制文件)</li>\n<li>链接: 将目标文件链接成目标程序</li>\n</ul>\n<p>但如果源文件太多，一个一个编译就会特别麻烦，为什么不批处理编译源文件呢，于是就有了make工具，它是一个自动化编译工具，你可以使用一条命令实现完全编译。还可以指定文件编译的顺序。但是使用make编译源码，需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile，所以编写makefile文件也是一个程序员所必备的技能。<br> 对于一个大工程，编写makefile实在是件复杂的事，于是人们又想，为什么不设计一个工具，读入所有源文件之后，自动生成makefile呢，于是就出现了<code>cmake</code>工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。所以在编程的世界里没有捷径可走，还是要脚踏实地的。</p>\n<p> 原文件－－camkelist —cmake —makefile —make —生成可执行文件</p>\n<h1 id=\"二、CMake基本语法规则\"><a href=\"#二、CMake基本语法规则\" class=\"headerlink\" title=\"二、CMake基本语法规则\"></a>二、<code>CMake基本语法规则</code></h1><ol>\n<li><p>变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名</p>\n</li>\n<li><p>指令(参数1  参数2  …)</p>\n<p>参数使用括弧括起，参数之间使用空格或分号分开</p>\n</li>\n<li><p>指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令</p>\n</li>\n<li><p>关于双引号的疑惑</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">SET(SRC_LIST main.c)也可以写成 SET(SRC_LIST “main.c”)\n是没有区别的，但是假设一个源文件的文件名是 fu nc.c(文件名中间包含了空格)。这时候就必须使用双引号，如果写成了 SET(SRC_LIST fu nc.c)，就会出现错误，提示你找不到 fu 文件和 nc.c 文件。这种情况，就必须写成:SET(SRC_LIST “fu nc.c”)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<h1 id=\"三、内部构建与外部构建\"><a href=\"#三、内部构建与外部构建\" class=\"headerlink\" title=\"三、内部构建与外部构建\"></a>三、内部构建与外部构建</h1><p>内部构建就是在项目跟目录直接编译</p>\n<p>引出了我们对外部编译的探讨，外部编译的过程如下：</p>\n<ol>\n<li>首先，请清除 t1 目录中除 main.c CmakeLists.txt 之外的所有中间文件，最关键的是 CMakeCache.txt。</li>\n<li>在 t1 目录中建立 build 目录，当然你也可以在任何地方建立 build 目录，不一定必须在工程目录中。</li>\n<li>进入 build 目录，运行 cmake ..(注意,..代表父目录，因为父目录存在我们需要的CMakeLists.txt，如果你在其他地方建立了 build 目录，需要运行 cmake &lt;工程的全路径&gt;)，查看一下 build 目录，就会发现了生成了编译需要的 Makefile 以及其他的中间文件.</li>\n<li>运行 make 构建工程，就会在当前目录(build 目录)中获得目标文件 hello。</li>\n<li>上述过程就是所谓的 out-of-source 外部编译，一个最大的好处是，对于原有的工程没有任何影响，所有动作全部发生在编译目录。通过这一点，也足以说服我们全部采用外部编译方式构建工程。</li>\n<li>这里需要特别注意的是：<br>通过外部编译进行工程构建，HELLO_SOURCE_DIR 仍然指代工程路径，即/backup/cmake/t1, 而 HELLO_BINARY_DIR 则指代编译路径，即/backup/cmake/t1/build</li>\n</ol>\n<p>#　四、安装库和INSTALL指令</p>\n<p>有两种安装方式，一种是从代码编译后直接 make install 安装，一种是cmake的install 指令安装。</p>\n<h2 id=\"1、make-install\"><a href=\"#1、make-install\" class=\"headerlink\" title=\"1、make install\"></a>1、<code>make install</code></h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">DESTDIR=\ninstall:\n    mkdir -p $(DESTDIR)/usr/bin\n    install -m 755 hello $(DESTDIR)/usr/bin\n你可以通过:\n    make install\n将 hello 直接安装到/usr/bin 目录，也可以通过 make install\nDESTDIR=/tmp/test 将他安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。稍微复杂一点的是还需要定义 PREFIX，一般 autotools 工程，会运行这样的指令:\n./configure –prefix=/usr \n或者./configure --prefix=/usr/local \n来指定PREFIX\n比如上面的 Makefile 就可以改写成:\nDESTDIR=\nPREFIX=/usr\ninstall:\n    mkdir -p $(DESTDIR)/$(PREFIX)/bin\n    install -m 755 hello $(DESTDIR)/$(PREFIX)/bin<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"2、cmake-INSTALL指令安装\"><a href=\"#2、cmake-INSTALL指令安装\" class=\"headerlink\" title=\"2、cmake INSTALL指令安装\"></a>2、<code>cmake INSTALL</code>指令安装</h2><p>这里需要引入一个新的 cmake 指令 INSTALL 和一个非常有用的变量<br>CMAKE_INSTALL_PREFIX。CMAKE_INSTALL_PREFIX 变量类似于 configure 脚本的 –prefix，常见的使用方法看起来是这个样子：<br>    <code>cmake -DCMAKE_INSTALL_PREFIX=/usr ..</code><br>INSTALL 指令用于定义安装规则，安装的内容可以包括目标二进制、动态库、静态库以及文件、目录、脚本等。</p>\n<p>INSTALL 指令包含了各种安装类型，我们需要一个个分开解释：<br>目标文件的安装：</p>\n<pre><code>INSTALL(TARGETS targets...\n    [[ARCHIVE|LIBRARY|RUNTIME]\n    [DESTINATION &lt;dir&gt;]\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [OPTIONAL]\n] [...])</code></pre><p>参数中的 TARGETS 后面跟的就是我们通过 ADD_EXECUTABLE 或者 ADD_LIBRARY 定义的<br>目标文件，可能是可执行二进制、动态库、静态库。<br>目标类型也就相对应的有三种，ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME<br>特指可执行目标二进制。<br>DESTINATION 定义了安装的路径，如果路径以/开头，那么指的是绝对路径，这时候<br>CMAKE_INSTALL_PREFIX 其实就无效了。如果你希望使用 CMAKE_INSTALL_PREFIX 来<br>定义安装路径，就要写成相对路径，即不要以/开头，那么安装后的路径就是<br>${CMAKE_INSTALL_PREFIX}/&lt;DESTINATION 定义的路径&gt;<br>举个简单的例子：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">INSTALL(TARGETS myrun mylib mystaticlib\n    RUNTIME DESTINATION bin\n    LIBRARY DESTINATION lib\n    ARCHIVE DESTINATION libstatic\n)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>上面的例子会将：<br>可执行二进制 myrun 安装到${CMAKE_INSTALL_PREFIX}/bin 目录<br>动态库 libmylib 安装到${CMAKE_INSTALL_PREFIX}/lib 目录<br>静态库 libmystaticlib 安装到${CMAKE_INSTALL_PREFIX}/libstatic 目录<br>特别注意的是你不需要关心 TARGETS 具体生成的路径，只需要写上 TARGETS 名称就可以<br>了。  </p>\n<p>普通文件的安装：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">INSTALL(FILES files... DESTINATION <dir>\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT <component>]\n    [RENAME <name>] [OPTIONAL])<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>可用于安装一般文件，并可以指定访问权限，文件名是此指令所在路径下的相对路径。如果<br>默认不定义权限 PERMISSIONS，安装后的权限为：<br>OWNER_WRITE, OWNER_READ, GROUP_READ,和 WORLD_READ，即 644 权限。<br>非目标文件的可执行程序安装(比如脚本之类)：</p>\n<pre><code>INSTALL(PROGRAMS files... DESTINATION &lt;dir&gt;\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [RENAME &lt;name&gt;] [OPTIONAL])</code></pre><p>跟上面的 FILES 指令使用方法一样，唯一的不同是安装后权限为:<br>OWNER_EXECUTE, GROUP_EXECUTE, 和 WORLD_EXECUTE，即 755 权限<br>目录的安装：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">INSTALL(DIRECTORY dirs... DESTINATION <dir>\n    [FILE_PERMISSIONS permissions...]\n    [DIRECTORY_PERMISSIONS permissions...]\n    [USE_SOURCE_PERMISSIONS]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT <component>]\n    [[PATTERN <pattern> | REGEX <regex>]\n    [EXCLUDE] [PERMISSIONS permissions...]] [...])<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>这里主要介绍其中的 DIRECTORY、PATTERN 以及 PERMISSIONS 参数。</p>\n<p>DIRECTORY 后面连接的是所在 Source 目录的相对路径，但务必注意：abc 和 abc/有很大的区别。<br>如果目录名不以/结尾，那么这个目录将被安装为目标路径下的 abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。<br>PATTERN 用于使用正则表达式进行过滤，PERMISSIONS 用于指定 PATTERN 过滤后的文件权限。<br>我们来看一个例子:</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">INSTALL(DIRECTORY icons scripts/ DESTINATION     share/myproj\nPATTERN \"CVS\" EXCLUDE\nPATTERN \"scripts/*\"\nPERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ\nGROUP_EXECUTE GROUP_READ)\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>这条指令的执行结果是：<br>将 icons 目录安装到 <prefix>/share/myproj，将 scripts/中的内容安装到<prefix>/share/myproj不包含目录名为 CVS 的目录，对于 scripts/*  文件指定权限为 OWNER_EXECUTE   OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ.</prefix></prefix></p>\n<p>安装时 CMAKE 脚本的执行：</p>\n<pre><code>INSTALL([[SCRIPT &lt;file&gt;] [CODE &lt;code&gt;]] [...])\nSCRIPT 参数用于在安装时调用 cmake 脚本文件（也就是&lt;abc&gt;.cmake 文件）\nCODE 参数用于执行 CMAKE 指令，必须以双引号括起来。比如：\nINSTALL(CODE &quot;MESSAGE(\\&quot;Sample install message.\\&quot;)&quot;)</code></pre><h1 id=\"五、静态库和动态库构建\"><a href=\"#五、静态库和动态库构建\" class=\"headerlink\" title=\"五、静态库和动态库构建\"></a>五、静态库和动态库构建</h1><h2 id=\"1、ADD-LIBRARY指令\"><a href=\"#1、ADD-LIBRARY指令\" class=\"headerlink\" title=\"1、ADD_LIBRARY指令\"></a>1、ADD_LIBRARY指令</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">ADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n    [EXCLUDE_FROM_ALL]\n    source1 source2 ... sourceN)\n# 不需要写全lib<libname>.so, 只需要填写<libname>,cmake系统会自动为你生成，lib<libname>.X\n\n# 类型有三种:\n    SHARED，动态库    .so\n    STATIC，静态库    .a\n    MODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\n\n#EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"2、指定库的生成路径\"><a href=\"#2、指定库的生成路径\" class=\"headerlink\" title=\"2、指定库的生成路径\"></a>2、指定库的生成路径</h2><p>​    两种方法</p>\n<ol>\n<li>ADD_SUBDIRECTORY指令来指定一个编译输出位置</li>\n<li>在CMakeLists.txt中添加　SET(LIBRARY_OUTPUT_PATH &lt;路径&gt;)来指定一个新的位置</li>\n</ol>\n<h2 id=\"3、同时生成动态库和静态库\"><a href=\"#3、同时生成动态库和静态库\" class=\"headerlink\" title=\"3、同时生成动态库和静态库\"></a>3、同时生成动态库和静态库</h2><p>因为ADD_SUBDIRECTORY的TARGET(libname)是唯一的，所以生成动态库和静态库不能指定相同的名称，想要有相同的名称需要用到SET_TARGET_PROPERTIES指令。</p>\n<p>SET_TARGET_PROPERTIES，其基本语法是：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">SET_TARGET_PROPERTIES(target1 target2 ...\n    PROPERTIES prop1 value1\n    prop2 value2 ...)\n# 举例\nADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})　# 动态库\nADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) # 静态库\nSET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME \"hello\")<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。</p>\n<p>与他对应的指令是：<br>    GET_TARGET_PROPERTY(VAR target property)</p>\n<p>举例</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME)\nMESSAGE(STATUS “This is the hello_static\nOUTPUT_NAME:”${OUTPUT_VALUE})\n# 如果没有这个属性定义，则返回 NOTFOUND.<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"4、动态库版本号\"><a href=\"#4、动态库版本号\" class=\"headerlink\" title=\"4、动态库版本号\"></a>4、动态库版本号</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1)\n# VERSION 指代动态库版本，SOVERSION 指代 API 版本。\n# 在 build/lib 目录会生成：\n    libhello.so.1.2\n    libhello.so.1->libhello.so.1.2\n    libhello.so -> libhello.so.1<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"六、使用共享库和头文件\"><a href=\"#六、使用共享库和头文件\" class=\"headerlink\" title=\"六、使用共享库和头文件\"></a>六、使用共享库和头文件</h1><h2 id=\"1-INCLUDE-DIRECTORIES指令\"><a href=\"#1-INCLUDE-DIRECTORIES指令\" class=\"headerlink\" title=\"1.INCLUDE_DIRECTORIES指令\"></a>1.<code>INCLUDE_DIRECTORIES</code>指令</h2><p><code>INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)</code><br>这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的<br>后面，你可以通过两种方式来进行控制搜索路径添加的方式：<br>１. CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。<br>２. 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。</p>\n<h2 id=\"2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES\"><a href=\"#2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES\" class=\"headerlink\" title=\"2. LINK_DIRECTORIES和 TARGET_LINK_LIBRARIES\"></a>2. <code>LINK_DIRECTORIES</code>和 <code>TARGET_LINK_LIBRARIES</code></h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">LINK_DIRECTORIES(directory1 directory2 ...)\n# 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制，在编译时就需要指定一下这些共享库的路径。\n# TARGET_LINK_LIBRARIES 的全部语法是:\nTARGET_LINK_LIBRARIES(target library1\n    <debug | optimized> library2\n...)\n# 这个指令可以用来为 target 添加需要链接的共享库<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"3-FIND系列指令\"><a href=\"#3-FIND系列指令\" class=\"headerlink\" title=\"3. FIND系列指令\"></a>3. <code>FIND</code>系列指令</h2><ol>\n<li><p>特殊的环境变量<code>CMAKE_INCLUDE_PATH</code> 和<code>CMAKE_LIBRARY_PATH</code></p>\n<p>务必注意，这两个是环境变量而不是 cmake 变量</p>\n</li>\n<li><p><code>CMAKE_INCLUDE_PATH</code>和<code>CMAKE_LIBRARY_PATH</code>是配合<code>FIND_PATH</code>和<code>FIND_LIBRARY</code>指令使用的</p>\n</li>\n<li><p>find_path指令</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">find_path (<VAR> NAMES name)\n# <VAR>查找的库文件路径报存在变量VAR中\n# 默认搜索路径为`CMAKE_INCLUDE_PATH`\n\nfind_path (<VAR> NAMES name PATHS paths... [NO_DEFAULT_PATH])\n#　指定搜索路径\n# NO_DEFAULT_PATH　不使用默认搜索路径　\n# 举例\n为了将程序更智能一点，我们可以使用 CMAKE_INCLUDE_PATH 来进行，使用 bash 的方法\n如下：export CMAKE_INCLUDE_PATH=/usr/include/hello\n然后在头文件中将 INCLUDE_DIRECTORIES(/usr/include/hello)替换为：\nFIND_PATH(myHeader hello.h)\nIF(myHeader)\n    INCLUDE_DIRECTORIES(${myHeader})\nENDIF(myHeader)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<h2 id=\"4-共享库和头文件指令总结\"><a href=\"#4-共享库和头文件指令总结\" class=\"headerlink\" title=\"4. 共享库和头文件指令总结\"></a>4. 共享库和头文件指令总结</h2><ol>\n<li><strong>FIND_PATH</strong> 查找头文件所在目录</li>\n<li><strong>INCLUDE_DIRECTORIES</strong>　添加头文件目录</li>\n<li><strong>FIND_LIBRARY</strong> 查找库文件所在目录</li>\n<li><strong>LINK_DIRECTORIES</strong>   添加库文件目录</li>\n<li><strong>LINK_LIBRARIES</strong>　添加需要链接的库文件路径，注意这里是全路径</li>\n<li><em><em>TARGET_LINK_LIBRARIES </em></em>　给TARGET链接库</li>\n</ol>\n<h1 id=\"七、Find模块\"><a href=\"#七、Find模块\" class=\"headerlink\" title=\"七、Find模块\"></a>七、Find模块</h1><h2 id=\"1-Find模块使用\"><a href=\"#1-Find模块使用\" class=\"headerlink\" title=\"1.Find模块使用\"></a>1.Find模块使用</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">FIND_PACKAGE(XXX)\nIF(XXX_FOUND)\n    INCLUDE_DIRECTORIES(${XXX_INCLUDE_DIR})\n    TARGET_LINK_LIBRARIES(xxxtest ${XXX_LIBRARY})\nELSE(XXX_FOUND)\n    MESSAGE(FATAL_ERROR ”XXX library not found”)\nENDIF(XXX_FOUND)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>对于系统预定义的 Find<name>.cmake 模块，使用方法一般如上例所示：<br>每一个模块都会定义以下几个变量<br>    • <name>_FOUND<br>    • <name>_INCLUDE_DIR or <name>_INCLUDES<br>    • <name>_LIBRARY or <name>_LIBRARIES<br>你可以通过<name>_FOUND 来判断模块是否被找到，如果没有找到，按照工程的需要关闭某些特性、给出提醒或者中止编译</name></name></name></name></name></name></name></p>\n<h2 id=\"2-find-package指令\"><a href=\"#2-find-package指令\" class=\"headerlink\" title=\"2.find_package指令\"></a>2.find_package指令</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">find_package(<PackageName> [QUIET] [REQUIRED] [[COMPONENTS] [components...]]\n             [OPTIONAL_COMPONENTS components...]\n             [NO_POLICY_SCOPE])\n\n# 查找并从外部项目加载设置，\n# <PackageName>_FOUND 将设置为指示是否找到该软件包, 如果查找到，该变量为true\n# [QUIET], 设置该变量，不会打印任何消息，且           <PackageName>_FIND_QUIETLY为true\n# [REQUIRED] 设置该变量，如果找不到软件包，该选项将停止处理并显示一条错误消息，且设置<PackageName>_FIND_REQUIRED为true,不过不指定该参数，即使没有找到，也能编译通过<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>find_package采用两种模式搜索库：</p>\n<ul>\n<li><strong>Module模式</strong>：搜索<strong>CMAKE_MODULE_PATH</strong>指定路径下的<strong>FindXXX.cmake</strong>文件，执行该文件从而找到XXX库。其中，具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由FindXXX.cmake模块完成。</li>\n<li><strong>Config模式</strong>：搜索<strong>XXX_DIR</strong>指定路径下的<strong>XXXConfig.cmake</strong>文件，执行该文件从而找到XXX库。其中具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由XXXConfig.cmake模块完成。</li>\n</ul>\n<p>两种模式看起来似乎差不多，不过cmake默认采取<strong>Module</strong>模式，如果Module模式未找到库，才会采取Config模式。如果<strong>XXX_DIR</strong>路径下找不到XXXConfig.cmake或<code>&lt;lower-case-package-name&gt;</code>config.cmake文件，则会找/usr/local/lib/cmake/XXX/中的XXXConfig.cmake文件。总之，Config模式是一个备选策略。通常，库安装时会拷贝一份XXXConfig.cmake到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。</p>\n<p>总结：CMake搜索的顺序为: 首先在<code>CMAKE_MODULE_PATH</code>中搜索名为<code>Find&lt;PackageName&gt;.cmake</code>的文件，然后在<code>&lt;PackageName&gt;_DIR</code>名为<code>PackageName&gt;Config.cmake</code>或<code>&lt;lower-case-package-name&gt;-config.cmake</code>的文件，如果还是找不到，则会去<code>/usr/local/lib/cmake</code>中查找<code>Find&lt;PackageName&gt;.cmake</code>文件。</p>\n<p>所以我们可以通过<code>CMAKE_MODULE_PATH</code>或<code>&lt;PackageName&gt;_DIR</code>变量指定cmake文件路径。</p>\n<h2 id=\"3-自定义Find模块\"><a href=\"#3-自定义Find模块\" class=\"headerlink\" title=\"3.自定义Find模块\"></a>3.自定义Find模块</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\"># 查找HELLO的头文件目录\nFIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello\n/usr/local/include/hello)\n# 查找HELLO的动态库\nFIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib\n/usr/local/lib)\nIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\n    SET(HELLO_FOUND TRUE)\nENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\nIF (HELLO_FOUND)\n    # 如果不指定QUIET参数，就打印信息\n    IF (NOT HELLO_FIND_QUIETLY)\n        MESSAGE(STATUS \"Found Hello: ${HELLO_LIBRARY}\")\n    ENDIF (NOT HELLO_FIND_QUIETLY)\nELSE (HELLO_FOUND)\n    # 如果设置了REQUIRED参数就报错\n    IF (HELLO_FIND_REQUIRED)\n        MESSAGE(FATAL_ERROR \"Could not find hello library\")\n    ENDIF (HELLO_FIND_REQUIRED)\nENDIF (HELLO_FOUND)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"八、CMake常用变量\"><a href=\"#八、CMake常用变量\" class=\"headerlink\" title=\"八、CMake常用变量\"></a>八、<code>CMake</code>常用变量</h1><h2 id=\"1-cmake-变量引用的方式：\"><a href=\"#1-cmake-变量引用的方式：\" class=\"headerlink\" title=\"1.cmake 变量引用的方式：\"></a>1.<code>cmake</code> 变量引用的方式：</h2><p>使用${}进行变量的引用。在 IF 等语句中，是直接使用变量名而不通过${}取值</p>\n<h2 id=\"2-cmake-自定义变量的方式：\"><a href=\"#2-cmake-自定义变量的方式：\" class=\"headerlink\" title=\"2.cmake 自定义变量的方式：\"></a>2.<code>cmake</code> 自定义变量的方式：</h2><p>主要有隐式定义和显式定义两种，前面举了一个隐式定义的例子，就是 PROJECT 指令，他会隐式的定义<projectname>_BINARY_DIR 和<projectname>_SOURCE_DIR 两个变量。<br>显式定义的例子我们前面也提到了，使用 SET 指令，就可以构建一个自定义变量了。比如:</projectname></projectname></p>\n<p>SET(HELLO_SRC main.SOURCE_PATHc)，就PROJECT_BINARY_DIR 可以通过${HELLO_SRC}来引用这个自定义变量了.</p>\n<h2 id=\"3-cmake-常用变量\"><a href=\"#3-cmake-常用变量\" class=\"headerlink\" title=\"3.cmake 常用变量\"></a>3.<code>cmake</code> 常用变量</h2><h3 id=\"1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR\"><a href=\"#1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR\" class=\"headerlink\" title=\"1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/_BINARY_DIR_\"></a>1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/<projectname>_BINARY_DIR_</projectname></h3><p>这三个变量指代的内容是一致的，如果是 in source 编译，指得就是工程顶层目录，如果是 out-of-source 编译，指的是工程编译发生的目录。PROJECT_BINARY_DIR 跟其他指令稍有区别，现在，你可以理解为他们是一致的。</p>\n<h3 id=\"2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR\"><a href=\"#2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR\" class=\"headerlink\" title=\"2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/_SOURCE_DIR\"></a>2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/<projectname>_SOURCE_DIR</projectname></h3><p>这三个变量指代的内容是一致的，不论采用何种编译方式，都是工程顶层目录。</p>\n<h3 id=\"3-CMAKE-CURRENT-SOURCE-DIR\"><a href=\"#3-CMAKE-CURRENT-SOURCE-DIR\" class=\"headerlink\" title=\"3. CMAKE_CURRENT_SOURCE_DIR\"></a>3. CMAKE_CURRENT_SOURCE_DIR</h3><p>指的是<strong>当前处理的</strong> CMakeLists.txt 所在的路径</p>\n<h3 id=\"4-CMAKE-CURRRENT-BINARY-DIR\"><a href=\"#4-CMAKE-CURRRENT-BINARY-DIR\" class=\"headerlink\" title=\"4. CMAKE_CURRRENT_BINARY_DIR\"></a>4. CMAKE_CURRRENT_BINARY_DIR</h3><p>如果是 in-source 编译，它跟 CMAKE_CURRENT_SOURCE_DIR 一致，如果是 out-ofsource 编译，他指的是 target 编译目录。<br>使用我们上面提到的 ADD_SUBDIRECTORY(src bin)可以更改这个变量的值。<br>使用 SET(EXECUTABLE_OUTPUT_PATH &lt;新路径&gt;)并不会对这个变量造成影响，它仅仅修改了最终目标文件存放的路径。</p>\n<h3 id=\"５-CMAKE-CURRENT-LIST-FILE\"><a href=\"#５-CMAKE-CURRENT-LIST-FILE\" class=\"headerlink\" title=\"５. CMAKE_CURRENT_LIST_FILE\"></a>５. CMAKE_CURRENT_LIST_FILE</h3><p>​    输出调用这个变量的 CMakeLists.txt 的完整路径</p>\n<h3 id=\"6-CMAKE-CURRENT-LIST-LINE\"><a href=\"#6-CMAKE-CURRENT-LIST-LINE\" class=\"headerlink\" title=\"6. CMAKE_CURRENT_LIST_LINE\"></a>6. CMAKE_CURRENT_LIST_LINE</h3><p>​    输出这个变量所在的行</p>\n<h3 id=\"7-CMAKE-MODULE-PATH\"><a href=\"#7-CMAKE-MODULE-PATH\" class=\"headerlink\" title=\"7. CMAKE_MODULE_PATH\"></a>7. CMAKE_MODULE_PATH</h3><p>这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂，有可能会自己编写一些 cmake 模块，这些 cmake 模块是随你的工程发布的，为了让 cmake 在处理CMakeLists.txt 时找到这些模块，你需要通过 SET 指令，将自己的 cmake 模块路径设<br>置一下。比如<br>SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)<br>这时候你就可以通过 INCLUDE 指令来调用自己的模块了。</p>\n<h3 id=\"8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH\"><a href=\"#8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH\" class=\"headerlink\" title=\"8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH\"></a>8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH</h3><p>分别用来重新定义最终结果的存放目录，前面我们已经提到了这两个变量。</p>\n<h3 id=\"9-PROJECT-NAME\"><a href=\"#9-PROJECT-NAME\" class=\"headerlink\" title=\"9. PROJECT_NAME\"></a>9. PROJECT_NAME</h3><p>返回通过 PROJECT 指令定义的项目名称。</p>\n<h2 id=\"4-cmake-调用环境变量的方式\"><a href=\"#4-cmake-调用环境变量的方式\" class=\"headerlink\" title=\"4. cmake 调用环境变量的方式\"></a>4. cmake 调用环境变量的方式</h2><p>使用$ENV{NAME}指令就可以调用系统的环境变量了。<br>比如MESSAGE(STATUS “HOME dir: $ENV{HOME}”)<br>设置环境变量的方式是：SET(ENV{变量名} 值)</p>\n<h3 id=\"1-CMAKE-INCLUDE-CURRENT-DIR\"><a href=\"#1-CMAKE-INCLUDE-CURRENT-DIR\" class=\"headerlink\" title=\"1. CMAKE_INCLUDE_CURRENT_DIR\"></a>1. CMAKE_INCLUDE_CURRENT_DIR</h3><p>自动添加 CMAKE_CURRENT_BINARY_DIR 和 CMAKE_CURRENT_SOURCE_DIR 到当前处理<br>的 CMakeLists.txt。相当于在每个 CMakeLists.txt 加入：<br>INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR}<br>${CMAKE_CURRENT_SOURCE_DIR})</p>\n<h3 id=\"2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE\"><a href=\"#2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE\" class=\"headerlink\" title=\"2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE\"></a>2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE</h3><p>将工程提供的头文件目录始终至于系统头文件目录的前面，当你定义的头文件确实跟系统发生冲突时可以提供一些帮助。</p>\n<h3 id=\"3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。\"><a href=\"#3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。\" class=\"headerlink\" title=\"3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。\"></a>3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。</h3><h2 id=\"5-系统信息\"><a href=\"#5-系统信息\" class=\"headerlink\" title=\"5. 系统信息\"></a>5. 系统信息</h2><ol>\n<li><p>CMAKE_MAJOR_VERSION，CMAKE 主版本号，比如 2.4.6 中的 2</p>\n</li>\n<li><p>CMAKE_MINOR_VERSION，CMAKE 次版本号，比如 2.4.6 中的 4</p>\n</li>\n<li><p>CMAKE_PATCH_VERSION，CMAKE 补丁等级，比如 2.4.6 中的 6</p>\n</li>\n<li><p>CMAKE_SYSTEM，系统名称，比如 Linux-2.6.22</p>\n</li>\n<li><p>CMAKE_SYSTEM_NAME，不包含版本的系统名，比如 Linux</p>\n</li>\n<li><p>CMAKE_SYSTEM_VERSION，系统版本，比如 2.6.22</p>\n</li>\n<li><p>CMAKE_SYSTEM_PROCESSOR，处理器名称，比如 i686.</p>\n</li>\n<li><p>UNIX，在所有的类 UNIX 平台为 TRUE，包括 OS X 和 cygwin</p>\n</li>\n<li><p>WIN32，在所有的 win32 平台为 TRUE，包括 cygwin</p>\n</li>\n</ol>\n<h2 id=\"6-主要的开关选项：\"><a href=\"#6-主要的开关选项：\" class=\"headerlink\" title=\"6.主要的开关选项：\"></a>6.主要的开关选项：</h2><ol>\n<li><p>CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS，用来控制 IF ELSE 语句的书写方式，在<br>下一节语法部分会讲到。</p>\n</li>\n<li><p>BUILD_SHARED_LIBS<br>这个开关用来控制默认的库编译方式，如果不进行设置，使用 ADD_LIBRARY 并没有指定库类型的情况下，默认编译生成的库都是静态库。<br>如果 SET(BUILD_SHARED_LIBS ON)后，默认生成的为动态</p>\n</li>\n<li><p>CMAKE_C_FLAGS<br>设置 C 编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p>\n</li>\n<li><p>CMAKE_CXX_FLAGS<br>设置 C++编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p>\n</li>\n</ol>\n<h1 id=\"九、CMake常用指令\"><a href=\"#九、CMake常用指令\" class=\"headerlink\" title=\"九、CMake常用指令\"></a>九、<code>CMake</code>常用指令</h1><h2 id=\"1-基本指令\"><a href=\"#1-基本指令\" class=\"headerlink\" title=\"1. 基本指令\"></a>1. 基本指令</h2><h3 id=\"MESSAGE\"><a href=\"#MESSAGE\" class=\"headerlink\" title=\"MESSAGE\"></a>MESSAGE</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">message([<mode>] \"message to display\" ...)\n可选<mode>关键字确定消息的类型:\nFATAL_ERROR    立即终止所有 cmake 过程\nSEND_ERROR 产生错误，生成过程被跳过\nWARNING\nAUTHOR_WARNING\nNOTICE\nSTATUS    输出前缀为—的信息\nVERBOSE\nDEBUG\nTRACE<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"PROJECT\"><a href=\"#PROJECT\" class=\"headerlink\" title=\"PROJECT\"></a>PROJECT</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">project(<PROJECT-NAME> [<language-name>...])\nproject(<PROJECT-NAME>\n        [VERSION <major>[.<minor>[.<patch>[.<tweak>]]]]\n        [LANGUAGES <language-name>...])\n\n设置项目的名称，并将其存储在变量中 PROJECT_NAME。从顶层调用时， CMakeLists.txt还将项目名称存储在变量CMAKE_PROJECT_NAME中。\n\n同时设置变量\n\nPROJECT_SOURCE_DIR， <PROJECT-NAME>_SOURCE_DIR\nPROJECT_BINARY_DIR， <PROJECT-NAME>_BINARY_DIR\n\nhttps://cmake.org/cmake/help/v3.15/command/project.html<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"SET\"><a href=\"#SET\" class=\"headerlink\" title=\"SET\"></a>SET</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">将普通变量，缓存变量或环境变量设置为给定值。\n指定<value>...占位符的此命令的签名期望零个或多个参数。多个参数将以分号分隔的列表形式加入，以形成要设置的实际变量值。零参数将导致未设置普通变量。unset() 命令显式取消设置变量。\n1、设置正常变量\nset(<variable> <value>... [PARENT_SCOPE])\n<variable>在当前函数或目录范围内设置给定值。\n如果PARENT_SCOPE给出了该选项，则将在当前作用域上方的作用域中设置变量。\n2、设置缓存变量\nset(<variable> <value>... CACHE <type> <docstring> [FORCE])\n3、设置环境变量\nset(ENV{<variable>} [<value>])\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"add-executable\"><a href=\"#add-executable\" class=\"headerlink\" title=\"add_executable\"></a>add_executable</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">使用指定的源文件生成可执行文件\nadd_executable(<name> [WIN32] [MACOSX_BUNDLE]\n               [EXCLUDE_FROM_ALL]\n               [source1] [source2 ...])\n<name>可执行文件名, <name>与逻辑目标名称相对应，并且在项目中必须是全局唯一的。构建的可执行文件的实际文件名是基于本机平台（例如<name>.exe或<name>）的约定构造的 。\n默认情况下，将在与调用命令的源树目录相对应的构建树目录中创建可执行文件。\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"add-subdirectory\"><a href=\"#add-subdirectory\" class=\"headerlink\" title=\"add_subdirectory\"></a>add_subdirectory</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">在构建中添加一个子目录。\nadd_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])\n将一个子目录添加到构建中。source_dir指定源CMakeLists.txt和代码文件所在的目录。binary_dir指定了输出文件放置的目录以及编译输出的路径。EXCLUDE_FROM_ALL 参数的含义是将这个目录从编译过程中排除，比如，工程的 example，可能就需要工程构建完成后，再进入 example 目录单独进行构建(当然，你也可以通过定义依赖来解决此类问题)。\n如果没有指定binary_dir,那么编译结果(包括中间结果)都将存放在\nbuild/source_dir 目录(这个目录跟原有的 source_dir 目录对应)，指定binary_dir 目录后，相当于在编译时将 source_dir 重命名为binary_dir，所有的中间结果和目标二进制都将存放在binary_dir 目录。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"subdirs\"><a href=\"#subdirs\" class=\"headerlink\" title=\"subdirs\"></a>subdirs</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">构建多个子目录\nsubdirs(dir1 dir2 ...[EXCLUDE_FROM_ALL exclude_dir1 exclude_dir2 ...]\n        [PREORDER] )\n\n\n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录)，我们都可以通过 SET 指令重新定义EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 变量\n来指定最终的目标二进制的位置(指最终生成的 hello 或者最终的共享库，不包含编译生成的中间文件)\nSET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\nSET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\n在第一节我们提到了<projectname>_BINARY_DIR 和 PROJECT_BINARY_DIR 变量，他们指的编译发生的当前目录，如果是内部编译，就相当于 PROJECT_SOURCE_DIR 也就是工程代码所在目录，如果是外部编译，指的是外部编译所在目录，也就是本例中的两个指令分别定义了：可执行二进制的输出路径为 build/bin 和库的输出路径为 build/lib.<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"add-library\"><a href=\"#add-library\" class=\"headerlink\" title=\"add_library\"></a>add_library</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">ADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n[EXCLUDE_FROM_ALL]\nsource1 source2 ... sourceN)\n你不需要写全 libhello.so，只需要填写 hello 即可，cmake 系统会自动为你生成\nlibhello.X\n类型有三种:\nSHARED，动态库\nSTATIC，静态库\nMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\nEXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手\n工构建。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"include-directories\"><a href=\"#include-directories\" class=\"headerlink\" title=\"include_directories\"></a>include_directories</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">将include目录添加到构建中\ninclude_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...])\n将给定目录添加到编译器用于搜索头文件的路径中。\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径\n中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１，CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以\n将添加的头文件搜索路径放在已有路径的前面。\n２，通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"target-link-libraries-amp-link-directories\"><a href=\"#target-link-libraries-amp-link-directories\" class=\"headerlink\" title=\"target_link_libraries &amp; link_directories\"></a>target_link_libraries &amp; link_directories</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">TARGET_LINK_LIBRARIES(target library1\n<debug | optimized> library2\n...)\n这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样\n可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误，我们需要作的是向\nsrc/CMakeLists.txt 中添加如下指令：\nTARGET_LINK_LIBRARIES(main hello)\n也可以写成\nTARGET_LINK_LIBRARIES(main libhello.so)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"ADD-DEFINITIONS\"><a href=\"#ADD-DEFINITIONS\" class=\"headerlink\" title=\"ADD_DEFINITIONS\"></a>ADD_DEFINITIONS</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">向 C/C++编译器添加-D 定义，比如:\nADD_DEFINITIONS(-DENABLE_DEBUG -DABC)，参数之间用空格分割。\n如果你的代码中定义了#ifdef ENABLE_DEBUG #endif，这个代码块就会生效。如果要添加其他的编译器开关，可以通过 CMAKE_C_FLAGS 变量和 CMAKE_CXX_FLAGS 变量设置。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"ADD-DEPENDENCIES\"><a href=\"#ADD-DEPENDENCIES\" class=\"headerlink\" title=\"ADD_DEPENDENCIES\"></a>ADD_DEPENDENCIES</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">定义 target 依赖的其他 target，确保在编译本 target 之前，其他的 target 已经被构建。\nADD_DEPENDENCIES(target-name depend-target1\ndepend-target2 ...)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"ADD-TEST-与-ENABLE-TESTING-指令。\"><a href=\"#ADD-TEST-与-ENABLE-TESTING-指令。\" class=\"headerlink\" title=\"ADD_TEST 与 ENABLE_TESTING 指令。\"></a>ADD_TEST 与 ENABLE_TESTING 指令。</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">ENABLE_TESTING 指令用来控制 Makefile 是否构建 test 目标，涉及工程所有目录。语法很简单，没有任何参数，ENABLE_TESTING()，一般情况这个指令放在工程的主CMakeLists.txt 中.\nADD_TEST 指令的语法是:\n    `ADD_TEST(testname Exename arg1 arg2 ...)`\ntestname 是自定义的 test 名称，Exename 可以是构建的目标文件也可以是外部脚本等等。后面连接传递给可执行文件的参数。如果没有在同一个 CMakeLists.txt 中打开\n    ENABLE_TESTING()指令，任何 ADD_TEST 都是无效的。\n比如我们前面的 Helloworld 例子，可以在工程主 CMakeLists.txt 中添加\n\nADD_TEST(mytest ${PROJECT_BINARY_DIR}/bin/main)\nENABLE_TESTING()\n生成 Makefile 后，就可以运行 make test 来执行测试了。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"AUX-SOURCE-DIRECTORY\"><a href=\"#AUX-SOURCE-DIRECTORY\" class=\"headerlink\" title=\"AUX_SOURCE_DIRECTORY\"></a>AUX_SOURCE_DIRECTORY</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">基本语法是：\nAUX_SOURCE_DIRECTORY(dir VARIABLE)\n作用是发现一个目录下所有的源代码文件并将列表存储在一个变量中，这个指令临时被用来\n自动构建源文件列表。因为目前 cmake 还不能自动发现新添加的源文件。\n比如\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nADD_EXECUTABLE(main ${SRC_LIST})\n你也可以通过后面提到的 FOREACH 指令来处理这个 LIST<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>###　CMAKE_MINIMUM_REQUIRED</p>\n<pre class=\"line-numbers language-sehll\"><code class=\"language-sehll\">其语法为 CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n比如 CMAKE_MINIMUM_REQUIRED(VERSION 2.5 FATAL_ERROR)\n如果 cmake 版本小与 2.5，则出现严重错误，整个过程中止。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"EXEC-PROGRAM\"><a href=\"#EXEC-PROGRAM\" class=\"headerlink\" title=\"EXEC_PROGRAM\"></a>EXEC_PROGRAM</h3><p>在 CMakeLists.txt 处理过程中执行命令，并不会在生成的 Makefile 中执行。具体语法为：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">EXEC_PROGRAM(Executable [directory in which to run]\n[ARGS <arguments to executable>]\n[OUTPUT_VARIABLE <var>]\n[RETURN_VALUE <var>])<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>用于在指定的目录运行某个程序，通过 ARGS 添加参数，如果要获取输出和返回值，可通过OUTPUT_VARIABLE 和 RETURN_VALUE 分别定义两个变量.<br>这个指令可以帮助你在 CMakeLists.txt 处理过程中支持任何命令，比如根据系统情况去修改代码文件等等。<br>举个简单的例子，我们要在 src 目录执行 ls 命令，并把结果和返回值存下来。<br>可以直接在 src/CMakeLists.txt 中添加：<br>EXEC_PROGRAM(ls ARGS “<em>.c” OUTPUT_VARIABLE LS_OUTPUT RETURN_VALUE LS_RVALUE)<br>IF(not LS_RVALUE)<br>    MESSAGE(STATUS “ls result: “ ${LS_OUTPUT})<br>ENDIF(not LS_RVALUE)<br>在 cmake 生成 Makefile 的过程中，就会执行 ls 命令，如果返回 0，则说明成功执行，<br>那么就输出 ls </em>.c 的结果。关于 IF 语句，后面的控制指令会提到。</p>\n<h3 id=\"FILE-指令\"><a href=\"#FILE-指令\" class=\"headerlink\" title=\"FILE 指令\"></a>FILE 指令</h3><p>文件操作指令，基本语法为:</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">FILE(WRITE filename \"message to write\"... )\nFILE(APPEND filename \"message to write\"... )\nFILE(READ filename variable)\nFILE(GLOB variable [RELATIVE path] [globbing\nexpressions]...)\nFILE(GLOB_RECURSE variable [RELATIVE path]\n[globbing expressions]...)\nFILE(REMOVE [directory]...)\nFILE(REMOVE_RECURSE [directory]...)\nFILE(MAKE_DIRECTORY [directory]...)\nFILE(RELATIVE_PATH variable directory file)\nFILE(TO_CMAKE_PATH path result)\nFILE(TO_NATIVE_PATH path result)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>这里的语法都比较简单，不在展开介绍了。</p>\n<h3 id=\"INCLUDE-指令\"><a href=\"#INCLUDE-指令\" class=\"headerlink\" title=\"INCLUDE 指令\"></a>INCLUDE 指令</h3><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">用来载入 CMakeLists.txt 文件，也用于载入预定义的 cmake 模块.\n    INCLUDE(file1 [OPTIONAL])\n    INCLUDE(module [OPTIONAL])\nOPTIONAL 参数的作用是文件不存在也不会产生错误。\n你可以指定载入一个文件，如果定义的是一个模块，那么将在 CMAKE_MODULE_PATH 中搜索这个模块并载入。\n载入的内容将在处理到 INCLUDE 语句是直接执行。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"2-控制指令：\"><a href=\"#2-控制指令：\" class=\"headerlink\" title=\"2. 控制指令：\"></a>2. 控制指令：</h2><h3 id=\"1-IF-指令\"><a href=\"#1-IF-指令\" class=\"headerlink\" title=\"1. IF 指令\"></a>1. IF 指令</h3><p>基本语法为：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">IF(expression)\n\n# THEN section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nELSE(expression)\n\n# ELSE section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDIF(expression)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>另外一个指令是 ELSEIF，总体把握一个原则，凡是出现 IF 的地方一定要有对应的<br>ENDIF.出现 ELSEIF 的地方，ENDIF 是可选的。<br>表达式的使用方法如下:<br>IF(var)，如果变量不是：空，0，N, NO, OFF, FALSE, NOTFOUND 或<br><var>_NOTFOUND 时，表达式为真。<br>IF(NOT var )，与上述条件相反。<br>IF(var1 AND var2)，当两个变量都为真是为真。<br>IF(var1 OR var2)，当两个变量其中一个为真时为真。<br>IF(COMMAND cmd)，当给定的 cmd 确实是命令并可以调用是为真。<br>IF(EXISTS dir)或者 IF(EXISTS file)，当目录名或者文件名存在时为真。<br>IF(file1 IS_NEWER_THAN file2)，当 file1 比 file2 新，或者 file1/file2 其中有一个不存在时为真，文件名请使用完整路径。<br>IF(IS_DIRECTORY dirname)，当 dirname 是目录时，为真。<br>IF(variable MATCHES regex)<br>IF(string MATCHES regex)<br>当给定的变量或者字符串能够匹配正则表达式 regex 时为真。比如：<br>IF(“hello” MATCHES “ell”)<br>MESSAGE(“true”)<br>ENDIF(“hello” MATCHES “ell”)<br>IF(variable LESS number)<br>IF(string LESS number)<br>IF(variable GREATER number)<br>IF(string GREATER number)<br>IF(variable EQUAL number)<br>IF(string EQUAL number)<br>数字比较表达式<br>IF(variable STRLESS string)<br>IF(string STRLESS string)<br>IF(variable STRGREATER string)<br>IF(string STRGREATER string)<br>IF(variable STREQUAL string)<br>IF(string STREQUAL string)<br>按照字母序的排列进行比较.<br>IF(DEFINED variable)，如果变量被定义，为真。<br>一个小例子，用来判断平台差异：<br>IF(WIN32)<br>MESSAGE(STATUS “This is windows.”)</var></p>\n<p>#作一些 Windows 相关的操作<br>ELSE(WIN32)<br>MESSAGE(STATUS “This is not windows”)</p>\n<p>#作一些非 Windows 相关的操作<br>ENDIF(WIN32)<br>上述代码用来控制在不同的平台进行不同的控制，但是，阅读起来却并不是那么舒服，<br>ELSE(WIN32)之类的语句很容易引起歧义。<br>这就用到了我们在“常用变量”一节提到的 CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS 开<br>关。<br>可以 SET(CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS ON)<br>这时候就可以写成:<br>IF(WIN32)<br>ELSE()<br>ENDIF()<br>如果配合 ELSEIF 使用，可能的写法是这样:<br>IF(WIN32)</p>\n<p>#do something related to WIN32<br>ELSEIF(UNIX)</p>\n<p>#do something related to UNIX<br>ELSEIF(APPLE)</p>\n<p>#do something related to APPLE<br>ENDIF(WIN32)</p>\n<h3 id=\"2-WHILE\"><a href=\"#2-WHILE\" class=\"headerlink\" title=\"2. WHILE\"></a>2. WHILE</h3><p>WHILE 指令的语法是：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">WHILE(condition)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDWHILE(condition)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>其真假判断条件可以参考 IF 指令。</p>\n<h3 id=\"3-FOREACH\"><a href=\"#3-FOREACH\" class=\"headerlink\" title=\"3. FOREACH\"></a>3. FOREACH</h3><p>FOREACH 指令的使用方法有三种形式：</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">1，列表\nFOREACH(loop_var arg1 arg2 ...)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDFOREACH(loop_var)\n像我们前面使用的 AUX_SOURCE_DIRECTORY 的例子\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nFOREACH(F ${SRC_LIST})\nMESSAGE(${F})\nENDFOREACH(F)\n2，范围\nFOREACH(loop_var RANGE total)\nENDFOREACH(loop_var)\n从 0 到 total 以１为步进\n举例如下：\nFOREACH(VAR RANGE 10)\nMESSAGE(${VAR})\nENDFOREACH(VAR)\n最终得到的输出是：\n0 1 2 3 4 5 6 7 8 9\n10\n３，范围和步进\nFOREACH(loop_var RANGE start stop [step])\nENDFOREACH(loop_var)\n从 start 开始到 stop 结束，以 step 为步进，\n举例如下\nFOREACH(A RANGE 5 15 3)\nMESSAGE(${A})\nENDFOREACH(A)\n最终得到的结果是：\n5 8\n11\n14\n这个指令需要注意的是，知道遇到 ENDFOREACH 指令，整个语句块才会得到真正的执行。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"十、CMakeLists配置模板\"><a href=\"#十、CMakeLists配置模板\" class=\"headerlink\" title=\"十、CMakeLists配置模板\"></a>十、<code>CMakeLists</code>配置模板</h1><h2 id=\"１-基本配置\"><a href=\"#１-基本配置\" class=\"headerlink\" title=\"１.基本配置\"></a>１.基本配置</h2><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">cmake_minimum_required(VERSION 3.14)\nproject(XXX_Project)\n\n# 设置CMAKE版本\nset(CMAKE_CXX_STANDARD 14)\n\n# 设置输出目录为 build/Debug/bin build/Debug/lib\n# 并缓存路径\nset(OUTPUT_DIRECTORY_ROOT ${CMAKE_CURRENT_SOURCE_DIR}/build/${CMAKE_BUILD_TYPE})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/bin\" CACHE PATH \"Runtime directory\" FORCE)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Library directory\" FORCE)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY \"${OUTPUT_DIRECTORY_ROOT}/lib\" CACHE PATH \"Archive directory\" FORCE)\n\n# 添加src子目录\nadd_subdirectory(src)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"２-依赖库相关配置\"><a href=\"#２-依赖库相关配置\" class=\"headerlink\" title=\"２.依赖库相关配置\"></a>２.依赖库相关配置</h2><p><strong><code>OPenCV</code>依赖库</strong></p>\n<p>将<code>OpenCV</code>依赖库下的<code>share/OpenCV</code>中，<code>OpenCVConfig.cmake</code>复制一份叫<code>FindOpenCV.cmake</code>，然后在根目录的CMakeLists.txt添加如下配置</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">#　添加make文件搜索路径\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ~/3rdparty/OpenCV-3.4.7/share/OpenCV)\n\n# 查找cmake文件，并初始化变量\nfind_package(OpenCV REQUIRED)\n# 添加头文件搜索路径\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n\n# 给执行程序添加链接库\nadd_executable(XXXXMain main.cpp)\ntarget_link_libraries(XXXXMain ${OpenCV_LIBS})<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"十一、参考\"><a href=\"#十一、参考\" class=\"headerlink\" title=\"十一、参考\"></a>十一、参考</h1><ol>\n<li>[<a href=\"http://file.ncnynl.com/ros/CMake%20Practice.pdf]\" target=\"_blank\" rel=\"noopener\">http://file.ncnynl.com/ros/CMake%20Practice.pdf]</a>(<a href=\"http://file.ncnynl.com/ros/CMake\" target=\"_blank\" rel=\"noopener\">http://file.ncnynl.com/ros/CMake</a> Practice.pdf)</li>\n<li><a href=\"https://cmake.org/cmake/help/latest/guide/tutorial/index.html\" target=\"_blank\" rel=\"noopener\">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></li>\n</ol>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><h1 id=\"一、CMake的作用\"><a href=\"#一、CMake的作用\" class=\"headerlink\" title=\"一、CMake的作用\"></a>一、<code>CMake</code>的作用</h1><p>大家都知道, 源文件的编译步骤为:</p>\n<ul>\n<li>预处理: 宏定义展开, 头文件展开, 条件编译</li>\n<li>编译: 检查语法, 生成编译文件</li>\n<li>汇编: 将汇编文件生成目标文件(二进制文件)</li>\n<li>链接: 将目标文件链接成目标程序</li>\n</ul>\n<p>但如果源文件太多，一个一个编译就会特别麻烦，为什么不批处理编译源文件呢，于是就有了make工具，它是一个自动化编译工具，你可以使用一条命令实现完全编译。还可以指定文件编译的顺序。但是使用make编译源码，需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile，所以编写makefile文件也是一个程序员所必备的技能。<br> 对于一个大工程，编写makefile实在是件复杂的事，于是人们又想，为什么不设计一个工具，读入所有源文件之后，自动生成makefile呢，于是就出现了<code>cmake</code>工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。所以在编程的世界里没有捷径可走，还是要脚踏实地的。</p>\n<p> 原文件－－camkelist —cmake —makefile —make —生成可执行文件</p>\n<h1 id=\"二、CMake基本语法规则\"><a href=\"#二、CMake基本语法规则\" class=\"headerlink\" title=\"二、CMake基本语法规则\"></a>二、<code>CMake基本语法规则</code></h1><ol>\n<li><p>变量使用${}方式取值，但是在 IF 控制语句中是直接使用变量名</p>\n</li>\n<li><p>指令(参数1  参数2  …)</p>\n<p>参数使用括弧括起，参数之间使用空格或分号分开</p>\n</li>\n<li><p>指令是大小写无关的，参数和变量是大小写相关的。推荐全部使用大写指令</p>\n</li>\n<li><p>关于双引号的疑惑</p>\n<pre><code class=\"shell\">SET(SRC_LIST main.c)也可以写成 SET(SRC_LIST “main.c”)\n是没有区别的，但是假设一个源文件的文件名是 fu nc.c(文件名中间包含了空格)。这时候就必须使用双引号，如果写成了 SET(SRC_LIST fu nc.c)，就会出现错误，提示你找不到 fu 文件和 nc.c 文件。这种情况，就必须写成:SET(SRC_LIST “fu nc.c”)</code></pre>\n</li>\n</ol>\n<h1 id=\"三、内部构建与外部构建\"><a href=\"#三、内部构建与外部构建\" class=\"headerlink\" title=\"三、内部构建与外部构建\"></a>三、内部构建与外部构建</h1><p>内部构建就是在项目跟目录直接编译</p>\n<p>引出了我们对外部编译的探讨，外部编译的过程如下：</p>\n<ol>\n<li>首先，请清除 t1 目录中除 main.c CmakeLists.txt 之外的所有中间文件，最关键的是 CMakeCache.txt。</li>\n<li>在 t1 目录中建立 build 目录，当然你也可以在任何地方建立 build 目录，不一定必须在工程目录中。</li>\n<li>进入 build 目录，运行 cmake ..(注意,..代表父目录，因为父目录存在我们需要的CMakeLists.txt，如果你在其他地方建立了 build 目录，需要运行 cmake &lt;工程的全路径&gt;)，查看一下 build 目录，就会发现了生成了编译需要的 Makefile 以及其他的中间文件.</li>\n<li>运行 make 构建工程，就会在当前目录(build 目录)中获得目标文件 hello。</li>\n<li>上述过程就是所谓的 out-of-source 外部编译，一个最大的好处是，对于原有的工程没有任何影响，所有动作全部发生在编译目录。通过这一点，也足以说服我们全部采用外部编译方式构建工程。</li>\n<li>这里需要特别注意的是：<br>通过外部编译进行工程构建，HELLO_SOURCE_DIR 仍然指代工程路径，即/backup/cmake/t1, 而 HELLO_BINARY_DIR 则指代编译路径，即/backup/cmake/t1/build</li>\n</ol>\n<p>#　四、安装库和INSTALL指令</p>\n<p>有两种安装方式，一种是从代码编译后直接 make install 安装，一种是cmake的install 指令安装。</p>\n<h2 id=\"1、make-install\"><a href=\"#1、make-install\" class=\"headerlink\" title=\"1、make install\"></a>1、<code>make install</code></h2><pre><code class=\"shell\">DESTDIR=\ninstall:\n    mkdir -p $(DESTDIR)/usr/bin\n    install -m 755 hello $(DESTDIR)/usr/bin\n你可以通过:\n    make install\n将 hello 直接安装到/usr/bin 目录，也可以通过 make install\nDESTDIR=/tmp/test 将他安装在/tmp/test/usr/bin 目录，打包时这个方式经常被使用。稍微复杂一点的是还需要定义 PREFIX，一般 autotools 工程，会运行这样的指令:\n./configure –prefix=/usr \n或者./configure --prefix=/usr/local \n来指定PREFIX\n比如上面的 Makefile 就可以改写成:\nDESTDIR=\nPREFIX=/usr\ninstall:\n    mkdir -p $(DESTDIR)/$(PREFIX)/bin\n    install -m 755 hello $(DESTDIR)/$(PREFIX)/bin</code></pre>\n<h2 id=\"2、cmake-INSTALL指令安装\"><a href=\"#2、cmake-INSTALL指令安装\" class=\"headerlink\" title=\"2、cmake INSTALL指令安装\"></a>2、<code>cmake INSTALL</code>指令安装</h2><p>这里需要引入一个新的 cmake 指令 INSTALL 和一个非常有用的变量<br>CMAKE_INSTALL_PREFIX。CMAKE_INSTALL_PREFIX 变量类似于 configure 脚本的 –prefix，常见的使用方法看起来是这个样子：<br>    <code>cmake -DCMAKE_INSTALL_PREFIX=/usr ..</code><br>INSTALL 指令用于定义安装规则，安装的内容可以包括目标二进制、动态库、静态库以及文件、目录、脚本等。</p>\n<p>INSTALL 指令包含了各种安装类型，我们需要一个个分开解释：<br>目标文件的安装：</p>\n<pre><code>INSTALL(TARGETS targets...\n    [[ARCHIVE|LIBRARY|RUNTIME]\n    [DESTINATION &lt;dir&gt;]\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [OPTIONAL]\n] [...])</code></pre><p>参数中的 TARGETS 后面跟的就是我们通过 ADD_EXECUTABLE 或者 ADD_LIBRARY 定义的<br>目标文件，可能是可执行二进制、动态库、静态库。<br>目标类型也就相对应的有三种，ARCHIVE 特指静态库，LIBRARY 特指动态库，RUNTIME<br>特指可执行目标二进制。<br>DESTINATION 定义了安装的路径，如果路径以/开头，那么指的是绝对路径，这时候<br>CMAKE_INSTALL_PREFIX 其实就无效了。如果你希望使用 CMAKE_INSTALL_PREFIX 来<br>定义安装路径，就要写成相对路径，即不要以/开头，那么安装后的路径就是<br>${CMAKE_INSTALL_PREFIX}/&lt;DESTINATION 定义的路径&gt;<br>举个简单的例子：</p>\n<pre><code class=\"shell\">INSTALL(TARGETS myrun mylib mystaticlib\n    RUNTIME DESTINATION bin\n    LIBRARY DESTINATION lib\n    ARCHIVE DESTINATION libstatic\n)</code></pre>\n<p>上面的例子会将：<br>可执行二进制 myrun 安装到${CMAKE_INSTALL_PREFIX}/bin 目录<br>动态库 libmylib 安装到${CMAKE_INSTALL_PREFIX}/lib 目录<br>静态库 libmystaticlib 安装到${CMAKE_INSTALL_PREFIX}/libstatic 目录<br>特别注意的是你不需要关心 TARGETS 具体生成的路径，只需要写上 TARGETS 名称就可以<br>了。  </p>\n<p>普通文件的安装：</p>\n<pre><code class=\"shell\">INSTALL(FILES files... DESTINATION &lt;dir&gt;\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [RENAME &lt;name&gt;] [OPTIONAL])</code></pre>\n<p>可用于安装一般文件，并可以指定访问权限，文件名是此指令所在路径下的相对路径。如果<br>默认不定义权限 PERMISSIONS，安装后的权限为：<br>OWNER_WRITE, OWNER_READ, GROUP_READ,和 WORLD_READ，即 644 权限。<br>非目标文件的可执行程序安装(比如脚本之类)：</p>\n<pre><code>INSTALL(PROGRAMS files... DESTINATION &lt;dir&gt;\n    [PERMISSIONS permissions...]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [RENAME &lt;name&gt;] [OPTIONAL])</code></pre><p>跟上面的 FILES 指令使用方法一样，唯一的不同是安装后权限为:<br>OWNER_EXECUTE, GROUP_EXECUTE, 和 WORLD_EXECUTE，即 755 权限<br>目录的安装：</p>\n<pre><code class=\"shell\">INSTALL(DIRECTORY dirs... DESTINATION &lt;dir&gt;\n    [FILE_PERMISSIONS permissions...]\n    [DIRECTORY_PERMISSIONS permissions...]\n    [USE_SOURCE_PERMISSIONS]\n    [CONFIGURATIONS [Debug|Release|...]]\n    [COMPONENT &lt;component&gt;]\n    [[PATTERN &lt;pattern&gt; | REGEX &lt;regex&gt;]\n    [EXCLUDE] [PERMISSIONS permissions...]] [...])</code></pre>\n<p>这里主要介绍其中的 DIRECTORY、PATTERN 以及 PERMISSIONS 参数。</p>\n<p>DIRECTORY 后面连接的是所在 Source 目录的相对路径，但务必注意：abc 和 abc/有很大的区别。<br>如果目录名不以/结尾，那么这个目录将被安装为目标路径下的 abc，如果目录名以/结尾，代表将这个目录中的内容安装到目标路径，但不包括这个目录本身。<br>PATTERN 用于使用正则表达式进行过滤，PERMISSIONS 用于指定 PATTERN 过滤后的文件权限。<br>我们来看一个例子:</p>\n<pre><code class=\"shell\">INSTALL(DIRECTORY icons scripts/ DESTINATION     share/myproj\nPATTERN &quot;CVS&quot; EXCLUDE\nPATTERN &quot;scripts/*&quot;\nPERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ\nGROUP_EXECUTE GROUP_READ)\n</code></pre>\n<p>这条指令的执行结果是：<br>将 icons 目录安装到 <prefix>/share/myproj，将 scripts/中的内容安装到<prefix>/share/myproj不包含目录名为 CVS 的目录，对于 scripts/*  文件指定权限为 OWNER_EXECUTE   OWNER_WRITE OWNER_READ GROUP_EXECUTE GROUP_READ.</prefix></prefix></p>\n<p>安装时 CMAKE 脚本的执行：</p>\n<pre><code>INSTALL([[SCRIPT &lt;file&gt;] [CODE &lt;code&gt;]] [...])\nSCRIPT 参数用于在安装时调用 cmake 脚本文件（也就是&lt;abc&gt;.cmake 文件）\nCODE 参数用于执行 CMAKE 指令，必须以双引号括起来。比如：\nINSTALL(CODE &quot;MESSAGE(\\&quot;Sample install message.\\&quot;)&quot;)</code></pre><h1 id=\"五、静态库和动态库构建\"><a href=\"#五、静态库和动态库构建\" class=\"headerlink\" title=\"五、静态库和动态库构建\"></a>五、静态库和动态库构建</h1><h2 id=\"1、ADD-LIBRARY指令\"><a href=\"#1、ADD-LIBRARY指令\" class=\"headerlink\" title=\"1、ADD_LIBRARY指令\"></a>1、ADD_LIBRARY指令</h2><pre><code class=\"shell\">ADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n    [EXCLUDE_FROM_ALL]\n    source1 source2 ... sourceN)\n# 不需要写全lib&lt;libname&gt;.so, 只需要填写&lt;libname&gt;,cmake系统会自动为你生成，lib&lt;libname&gt;.X\n\n# 类型有三种:\n    SHARED，动态库    .so\n    STATIC，静态库    .a\n    MODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\n\n#EXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手工构建。</code></pre>\n<h2 id=\"2、指定库的生成路径\"><a href=\"#2、指定库的生成路径\" class=\"headerlink\" title=\"2、指定库的生成路径\"></a>2、指定库的生成路径</h2><p>​    两种方法</p>\n<ol>\n<li>ADD_SUBDIRECTORY指令来指定一个编译输出位置</li>\n<li>在CMakeLists.txt中添加　SET(LIBRARY_OUTPUT_PATH &lt;路径&gt;)来指定一个新的位置</li>\n</ol>\n<h2 id=\"3、同时生成动态库和静态库\"><a href=\"#3、同时生成动态库和静态库\" class=\"headerlink\" title=\"3、同时生成动态库和静态库\"></a>3、同时生成动态库和静态库</h2><p>因为ADD_SUBDIRECTORY的TARGET(libname)是唯一的，所以生成动态库和静态库不能指定相同的名称，想要有相同的名称需要用到SET_TARGET_PROPERTIES指令。</p>\n<p>SET_TARGET_PROPERTIES，其基本语法是：</p>\n<pre><code class=\"shell\">SET_TARGET_PROPERTIES(target1 target2 ...\n    PROPERTIES prop1 value1\n    prop2 value2 ...)\n# 举例\nADD_LIBRARY(hello SHARED ${LIBHELLO_SRC})　# 动态库\nADD_LIBRARY(hello_static STATIC ${LIBHELLO_SRC}) # 静态库\nSET_TARGET_PROPERTIES(hello_static PROPERTIES OUTPUT_NAME &quot;hello&quot;)</code></pre>\n<p>这条指令可以用来设置输出的名称，对于动态库，还可以用来指定动态库版本和 API 版本。</p>\n<p>与他对应的指令是：<br>    GET_TARGET_PROPERTY(VAR target property)</p>\n<p>举例</p>\n<pre><code class=\"shell\">GET_TARGET_PROPERTY(OUTPUT_VALUE hello_static OUTPUT_NAME)\nMESSAGE(STATUS “This is the hello_static\nOUTPUT_NAME:”${OUTPUT_VALUE})\n# 如果没有这个属性定义，则返回 NOTFOUND.</code></pre>\n<h2 id=\"4、动态库版本号\"><a href=\"#4、动态库版本号\" class=\"headerlink\" title=\"4、动态库版本号\"></a>4、动态库版本号</h2><pre><code class=\"shell\">SET_TARGET_PROPERTIES(hello PROPERTIES VERSION 1.2 SOVERSION 1)\n# VERSION 指代动态库版本，SOVERSION 指代 API 版本。\n# 在 build/lib 目录会生成：\n    libhello.so.1.2\n    libhello.so.1-&gt;libhello.so.1.2\n    libhello.so -&gt; libhello.so.1</code></pre>\n<h1 id=\"六、使用共享库和头文件\"><a href=\"#六、使用共享库和头文件\" class=\"headerlink\" title=\"六、使用共享库和头文件\"></a>六、使用共享库和头文件</h1><h2 id=\"1-INCLUDE-DIRECTORIES指令\"><a href=\"#1-INCLUDE-DIRECTORIES指令\" class=\"headerlink\" title=\"1.INCLUDE_DIRECTORIES指令\"></a>1.<code>INCLUDE_DIRECTORIES</code>指令</h2><p><code>INCLUDE_DIRECTORIES([AFTER|BEFORE] [SYSTEM] dir1 dir2 ...)</code><br>这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的<br>后面，你可以通过两种方式来进行控制搜索路径添加的方式：<br>１. CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以将添加的头文件搜索路径放在已有路径的前面。<br>２. 通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。</p>\n<h2 id=\"2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES\"><a href=\"#2-LINK-DIRECTORIES和-TARGET-LINK-LIBRARIES\" class=\"headerlink\" title=\"2. LINK_DIRECTORIES和 TARGET_LINK_LIBRARIES\"></a>2. <code>LINK_DIRECTORIES</code>和 <code>TARGET_LINK_LIBRARIES</code></h2><pre><code class=\"shell\">LINK_DIRECTORIES(directory1 directory2 ...)\n# 这个指令非常简单，添加非标准的共享库搜索路径，比如，在工程内部同时存在共享库和可执行二进制，在编译时就需要指定一下这些共享库的路径。\n# TARGET_LINK_LIBRARIES 的全部语法是:\nTARGET_LINK_LIBRARIES(target library1\n    &lt;debug | optimized&gt; library2\n...)\n# 这个指令可以用来为 target 添加需要链接的共享库</code></pre>\n<h2 id=\"3-FIND系列指令\"><a href=\"#3-FIND系列指令\" class=\"headerlink\" title=\"3. FIND系列指令\"></a>3. <code>FIND</code>系列指令</h2><ol>\n<li><p>特殊的环境变量<code>CMAKE_INCLUDE_PATH</code> 和<code>CMAKE_LIBRARY_PATH</code></p>\n<p>务必注意，这两个是环境变量而不是 cmake 变量</p>\n</li>\n<li><p><code>CMAKE_INCLUDE_PATH</code>和<code>CMAKE_LIBRARY_PATH</code>是配合<code>FIND_PATH</code>和<code>FIND_LIBRARY</code>指令使用的</p>\n</li>\n<li><p>find_path指令</p>\n<pre><code class=\"shell\">find_path (&lt;VAR&gt; NAMES name)\n# &lt;VAR&gt;查找的库文件路径报存在变量VAR中\n# 默认搜索路径为`CMAKE_INCLUDE_PATH`\n\nfind_path (&lt;VAR&gt; NAMES name PATHS paths... [NO_DEFAULT_PATH])\n#　指定搜索路径\n# NO_DEFAULT_PATH　不使用默认搜索路径　\n# 举例\n为了将程序更智能一点，我们可以使用 CMAKE_INCLUDE_PATH 来进行，使用 bash 的方法\n如下：export CMAKE_INCLUDE_PATH=/usr/include/hello\n然后在头文件中将 INCLUDE_DIRECTORIES(/usr/include/hello)替换为：\nFIND_PATH(myHeader hello.h)\nIF(myHeader)\n    INCLUDE_DIRECTORIES(${myHeader})\nENDIF(myHeader)</code></pre>\n</li>\n</ol>\n<h2 id=\"4-共享库和头文件指令总结\"><a href=\"#4-共享库和头文件指令总结\" class=\"headerlink\" title=\"4. 共享库和头文件指令总结\"></a>4. 共享库和头文件指令总结</h2><ol>\n<li><strong>FIND_PATH</strong> 查找头文件所在目录</li>\n<li><strong>INCLUDE_DIRECTORIES</strong>　添加头文件目录</li>\n<li><strong>FIND_LIBRARY</strong> 查找库文件所在目录</li>\n<li><strong>LINK_DIRECTORIES</strong>   添加库文件目录</li>\n<li><strong>LINK_LIBRARIES</strong>　添加需要链接的库文件路径，注意这里是全路径</li>\n<li><em><em>TARGET_LINK_LIBRARIES </em></em>　给TARGET链接库</li>\n</ol>\n<h1 id=\"七、Find模块\"><a href=\"#七、Find模块\" class=\"headerlink\" title=\"七、Find模块\"></a>七、Find模块</h1><h2 id=\"1-Find模块使用\"><a href=\"#1-Find模块使用\" class=\"headerlink\" title=\"1.Find模块使用\"></a>1.Find模块使用</h2><pre><code class=\"shell\">FIND_PACKAGE(XXX)\nIF(XXX_FOUND)\n    INCLUDE_DIRECTORIES(${XXX_INCLUDE_DIR})\n    TARGET_LINK_LIBRARIES(xxxtest ${XXX_LIBRARY})\nELSE(XXX_FOUND)\n    MESSAGE(FATAL_ERROR ”XXX library not found”)\nENDIF(XXX_FOUND)</code></pre>\n<p>对于系统预定义的 Find<name>.cmake 模块，使用方法一般如上例所示：<br>每一个模块都会定义以下几个变量<br>    • <name>_FOUND<br>    • <name>_INCLUDE_DIR or <name>_INCLUDES<br>    • <name>_LIBRARY or <name>_LIBRARIES<br>你可以通过<name>_FOUND 来判断模块是否被找到，如果没有找到，按照工程的需要关闭某些特性、给出提醒或者中止编译</name></name></name></name></name></name></name></p>\n<h2 id=\"2-find-package指令\"><a href=\"#2-find-package指令\" class=\"headerlink\" title=\"2.find_package指令\"></a>2.find_package指令</h2><pre><code class=\"shell\">find_package(&lt;PackageName&gt; [QUIET] [REQUIRED] [[COMPONENTS] [components...]]\n             [OPTIONAL_COMPONENTS components...]\n             [NO_POLICY_SCOPE])\n\n# 查找并从外部项目加载设置，\n# &lt;PackageName&gt;_FOUND 将设置为指示是否找到该软件包, 如果查找到，该变量为true\n# [QUIET], 设置该变量，不会打印任何消息，且           &lt;PackageName&gt;_FIND_QUIETLY为true\n# [REQUIRED] 设置该变量，如果找不到软件包，该选项将停止处理并显示一条错误消息，且设置&lt;PackageName&gt;_FIND_REQUIRED为true,不过不指定该参数，即使没有找到，也能编译通过</code></pre>\n<p>find_package采用两种模式搜索库：</p>\n<ul>\n<li><strong>Module模式</strong>：搜索<strong>CMAKE_MODULE_PATH</strong>指定路径下的<strong>FindXXX.cmake</strong>文件，执行该文件从而找到XXX库。其中，具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由FindXXX.cmake模块完成。</li>\n<li><strong>Config模式</strong>：搜索<strong>XXX_DIR</strong>指定路径下的<strong>XXXConfig.cmake</strong>文件，执行该文件从而找到XXX库。其中具体查找库并给<strong>XXX_INCLUDE_DIRS</strong>和<strong>XXX_LIBRARIES</strong>两个变量赋值的操作由XXXConfig.cmake模块完成。</li>\n</ul>\n<p>两种模式看起来似乎差不多，不过cmake默认采取<strong>Module</strong>模式，如果Module模式未找到库，才会采取Config模式。如果<strong>XXX_DIR</strong>路径下找不到XXXConfig.cmake或<code>&lt;lower-case-package-name&gt;</code>config.cmake文件，则会找/usr/local/lib/cmake/XXX/中的XXXConfig.cmake文件。总之，Config模式是一个备选策略。通常，库安装时会拷贝一份XXXConfig.cmake到系统目录中，因此在没有显式指定搜索路径时也可以顺利找到。</p>\n<p>总结：CMake搜索的顺序为: 首先在<code>CMAKE_MODULE_PATH</code>中搜索名为<code>Find&lt;PackageName&gt;.cmake</code>的文件，然后在<code>&lt;PackageName&gt;_DIR</code>名为<code>PackageName&gt;Config.cmake</code>或<code>&lt;lower-case-package-name&gt;-config.cmake</code>的文件，如果还是找不到，则会去<code>/usr/local/lib/cmake</code>中查找<code>Find&lt;PackageName&gt;.cmake</code>文件。</p>\n<p>所以我们可以通过<code>CMAKE_MODULE_PATH</code>或<code>&lt;PackageName&gt;_DIR</code>变量指定cmake文件路径。</p>\n<h2 id=\"3-自定义Find模块\"><a href=\"#3-自定义Find模块\" class=\"headerlink\" title=\"3.自定义Find模块\"></a>3.自定义Find模块</h2><pre><code class=\"shell\"># 查找HELLO的头文件目录\nFIND_PATH(HELLO_INCLUDE_DIR hello.h /usr/include/hello\n/usr/local/include/hello)\n# 查找HELLO的动态库\nFIND_LIBRARY(HELLO_LIBRARY NAMES hello PATH /usr/lib\n/usr/local/lib)\nIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\n    SET(HELLO_FOUND TRUE)\nENDIF (HELLO_INCLUDE_DIR AND HELLO_LIBRARY)\nIF (HELLO_FOUND)\n    # 如果不指定QUIET参数，就打印信息\n    IF (NOT HELLO_FIND_QUIETLY)\n        MESSAGE(STATUS &quot;Found Hello: ${HELLO_LIBRARY}&quot;)\n    ENDIF (NOT HELLO_FIND_QUIETLY)\nELSE (HELLO_FOUND)\n    # 如果设置了REQUIRED参数就报错\n    IF (HELLO_FIND_REQUIRED)\n        MESSAGE(FATAL_ERROR &quot;Could not find hello library&quot;)\n    ENDIF (HELLO_FIND_REQUIRED)\nENDIF (HELLO_FOUND)</code></pre>\n<h1 id=\"八、CMake常用变量\"><a href=\"#八、CMake常用变量\" class=\"headerlink\" title=\"八、CMake常用变量\"></a>八、<code>CMake</code>常用变量</h1><h2 id=\"1-cmake-变量引用的方式：\"><a href=\"#1-cmake-变量引用的方式：\" class=\"headerlink\" title=\"1.cmake 变量引用的方式：\"></a>1.<code>cmake</code> 变量引用的方式：</h2><p>使用${}进行变量的引用。在 IF 等语句中，是直接使用变量名而不通过${}取值</p>\n<h2 id=\"2-cmake-自定义变量的方式：\"><a href=\"#2-cmake-自定义变量的方式：\" class=\"headerlink\" title=\"2.cmake 自定义变量的方式：\"></a>2.<code>cmake</code> 自定义变量的方式：</h2><p>主要有隐式定义和显式定义两种，前面举了一个隐式定义的例子，就是 PROJECT 指令，他会隐式的定义<projectname>_BINARY_DIR 和<projectname>_SOURCE_DIR 两个变量。<br>显式定义的例子我们前面也提到了，使用 SET 指令，就可以构建一个自定义变量了。比如:</projectname></projectname></p>\n<p>SET(HELLO_SRC main.SOURCE_PATHc)，就PROJECT_BINARY_DIR 可以通过${HELLO_SRC}来引用这个自定义变量了.</p>\n<h2 id=\"3-cmake-常用变量\"><a href=\"#3-cmake-常用变量\" class=\"headerlink\" title=\"3.cmake 常用变量\"></a>3.<code>cmake</code> 常用变量</h2><h3 id=\"1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR\"><a href=\"#1-CMAKE-BINARY-DIR-PROJECT-BINARY-DIR-BINARY-DIR\" class=\"headerlink\" title=\"1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/_BINARY_DIR_\"></a>1. CMAKE_BINARY_DIR/PROJECT_BINARY_DIR/<projectname>_BINARY_DIR_</projectname></h3><p>这三个变量指代的内容是一致的，如果是 in source 编译，指得就是工程顶层目录，如果是 out-of-source 编译，指的是工程编译发生的目录。PROJECT_BINARY_DIR 跟其他指令稍有区别，现在，你可以理解为他们是一致的。</p>\n<h3 id=\"2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR\"><a href=\"#2-CMAKE-SOURCE-DIR-PROJECT-SOURCE-DIR-SOURCE-DIR\" class=\"headerlink\" title=\"2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/_SOURCE_DIR\"></a>2. CMAKE_SOURCE_DIR/PROJECT_SOURCE_DIR/<projectname>_SOURCE_DIR</projectname></h3><p>这三个变量指代的内容是一致的，不论采用何种编译方式，都是工程顶层目录。</p>\n<h3 id=\"3-CMAKE-CURRENT-SOURCE-DIR\"><a href=\"#3-CMAKE-CURRENT-SOURCE-DIR\" class=\"headerlink\" title=\"3. CMAKE_CURRENT_SOURCE_DIR\"></a>3. CMAKE_CURRENT_SOURCE_DIR</h3><p>指的是<strong>当前处理的</strong> CMakeLists.txt 所在的路径</p>\n<h3 id=\"4-CMAKE-CURRRENT-BINARY-DIR\"><a href=\"#4-CMAKE-CURRRENT-BINARY-DIR\" class=\"headerlink\" title=\"4. CMAKE_CURRRENT_BINARY_DIR\"></a>4. CMAKE_CURRRENT_BINARY_DIR</h3><p>如果是 in-source 编译，它跟 CMAKE_CURRENT_SOURCE_DIR 一致，如果是 out-ofsource 编译，他指的是 target 编译目录。<br>使用我们上面提到的 ADD_SUBDIRECTORY(src bin)可以更改这个变量的值。<br>使用 SET(EXECUTABLE_OUTPUT_PATH &lt;新路径&gt;)并不会对这个变量造成影响，它仅仅修改了最终目标文件存放的路径。</p>\n<h3 id=\"５-CMAKE-CURRENT-LIST-FILE\"><a href=\"#５-CMAKE-CURRENT-LIST-FILE\" class=\"headerlink\" title=\"５. CMAKE_CURRENT_LIST_FILE\"></a>５. CMAKE_CURRENT_LIST_FILE</h3><p>​    输出调用这个变量的 CMakeLists.txt 的完整路径</p>\n<h3 id=\"6-CMAKE-CURRENT-LIST-LINE\"><a href=\"#6-CMAKE-CURRENT-LIST-LINE\" class=\"headerlink\" title=\"6. CMAKE_CURRENT_LIST_LINE\"></a>6. CMAKE_CURRENT_LIST_LINE</h3><p>​    输出这个变量所在的行</p>\n<h3 id=\"7-CMAKE-MODULE-PATH\"><a href=\"#7-CMAKE-MODULE-PATH\" class=\"headerlink\" title=\"7. CMAKE_MODULE_PATH\"></a>7. CMAKE_MODULE_PATH</h3><p>这个变量用来定义自己的 cmake 模块所在的路径。如果你的工程比较复杂，有可能会自己编写一些 cmake 模块，这些 cmake 模块是随你的工程发布的，为了让 cmake 在处理CMakeLists.txt 时找到这些模块，你需要通过 SET 指令，将自己的 cmake 模块路径设<br>置一下。比如<br>SET(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)<br>这时候你就可以通过 INCLUDE 指令来调用自己的模块了。</p>\n<h3 id=\"8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH\"><a href=\"#8-EXECUTABLE-OUTPUT-PATH-和-LIBRARY-OUTPUT-PATH\" class=\"headerlink\" title=\"8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH\"></a>8. EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH</h3><p>分别用来重新定义最终结果的存放目录，前面我们已经提到了这两个变量。</p>\n<h3 id=\"9-PROJECT-NAME\"><a href=\"#9-PROJECT-NAME\" class=\"headerlink\" title=\"9. PROJECT_NAME\"></a>9. PROJECT_NAME</h3><p>返回通过 PROJECT 指令定义的项目名称。</p>\n<h2 id=\"4-cmake-调用环境变量的方式\"><a href=\"#4-cmake-调用环境变量的方式\" class=\"headerlink\" title=\"4. cmake 调用环境变量的方式\"></a>4. cmake 调用环境变量的方式</h2><p>使用$ENV{NAME}指令就可以调用系统的环境变量了。<br>比如MESSAGE(STATUS “HOME dir: $ENV{HOME}”)<br>设置环境变量的方式是：SET(ENV{变量名} 值)</p>\n<h3 id=\"1-CMAKE-INCLUDE-CURRENT-DIR\"><a href=\"#1-CMAKE-INCLUDE-CURRENT-DIR\" class=\"headerlink\" title=\"1. CMAKE_INCLUDE_CURRENT_DIR\"></a>1. CMAKE_INCLUDE_CURRENT_DIR</h3><p>自动添加 CMAKE_CURRENT_BINARY_DIR 和 CMAKE_CURRENT_SOURCE_DIR 到当前处理<br>的 CMakeLists.txt。相当于在每个 CMakeLists.txt 加入：<br>INCLUDE_DIRECTORIES(${CMAKE_CURRENT_BINARY_DIR}<br>${CMAKE_CURRENT_SOURCE_DIR})</p>\n<h3 id=\"2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE\"><a href=\"#2-CMAKE-INCLUDE-DIRECTORIES-PROJECT-BEFORE\" class=\"headerlink\" title=\"2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE\"></a>2. CMAKE_INCLUDE_DIRECTORIES_PROJECT_BEFORE</h3><p>将工程提供的头文件目录始终至于系统头文件目录的前面，当你定义的头文件确实跟系统发生冲突时可以提供一些帮助。</p>\n<h3 id=\"3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。\"><a href=\"#3-CMAKE-INCLUDE-PATH-和-CMAKE-LIBRARY-PATH-我们在上一节已经提及。\" class=\"headerlink\" title=\"3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。\"></a>3. CMAKE_INCLUDE_PATH 和 CMAKE_LIBRARY_PATH 我们在上一节已经提及。</h3><h2 id=\"5-系统信息\"><a href=\"#5-系统信息\" class=\"headerlink\" title=\"5. 系统信息\"></a>5. 系统信息</h2><ol>\n<li><p>CMAKE_MAJOR_VERSION，CMAKE 主版本号，比如 2.4.6 中的 2</p>\n</li>\n<li><p>CMAKE_MINOR_VERSION，CMAKE 次版本号，比如 2.4.6 中的 4</p>\n</li>\n<li><p>CMAKE_PATCH_VERSION，CMAKE 补丁等级，比如 2.4.6 中的 6</p>\n</li>\n<li><p>CMAKE_SYSTEM，系统名称，比如 Linux-2.6.22</p>\n</li>\n<li><p>CMAKE_SYSTEM_NAME，不包含版本的系统名，比如 Linux</p>\n</li>\n<li><p>CMAKE_SYSTEM_VERSION，系统版本，比如 2.6.22</p>\n</li>\n<li><p>CMAKE_SYSTEM_PROCESSOR，处理器名称，比如 i686.</p>\n</li>\n<li><p>UNIX，在所有的类 UNIX 平台为 TRUE，包括 OS X 和 cygwin</p>\n</li>\n<li><p>WIN32，在所有的 win32 平台为 TRUE，包括 cygwin</p>\n</li>\n</ol>\n<h2 id=\"6-主要的开关选项：\"><a href=\"#6-主要的开关选项：\" class=\"headerlink\" title=\"6.主要的开关选项：\"></a>6.主要的开关选项：</h2><ol>\n<li><p>CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS，用来控制 IF ELSE 语句的书写方式，在<br>下一节语法部分会讲到。</p>\n</li>\n<li><p>BUILD_SHARED_LIBS<br>这个开关用来控制默认的库编译方式，如果不进行设置，使用 ADD_LIBRARY 并没有指定库类型的情况下，默认编译生成的库都是静态库。<br>如果 SET(BUILD_SHARED_LIBS ON)后，默认生成的为动态</p>\n</li>\n<li><p>CMAKE_C_FLAGS<br>设置 C 编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p>\n</li>\n<li><p>CMAKE_CXX_FLAGS<br>设置 C++编译选项，也可以通过指令 ADD_DEFINITIONS()添加。</p>\n</li>\n</ol>\n<h1 id=\"九、CMake常用指令\"><a href=\"#九、CMake常用指令\" class=\"headerlink\" title=\"九、CMake常用指令\"></a>九、<code>CMake</code>常用指令</h1><h2 id=\"1-基本指令\"><a href=\"#1-基本指令\" class=\"headerlink\" title=\"1. 基本指令\"></a>1. 基本指令</h2><h3 id=\"MESSAGE\"><a href=\"#MESSAGE\" class=\"headerlink\" title=\"MESSAGE\"></a>MESSAGE</h3><pre><code class=\"shell\">message([&lt;mode&gt;] &quot;message to display&quot; ...)\n可选&lt;mode&gt;关键字确定消息的类型:\nFATAL_ERROR    立即终止所有 cmake 过程\nSEND_ERROR 产生错误，生成过程被跳过\nWARNING\nAUTHOR_WARNING\nNOTICE\nSTATUS    输出前缀为—的信息\nVERBOSE\nDEBUG\nTRACE</code></pre>\n<h3 id=\"PROJECT\"><a href=\"#PROJECT\" class=\"headerlink\" title=\"PROJECT\"></a>PROJECT</h3><pre><code class=\"shell\">project(&lt;PROJECT-NAME&gt; [&lt;language-name&gt;...])\nproject(&lt;PROJECT-NAME&gt;\n        [VERSION &lt;major&gt;[.&lt;minor&gt;[.&lt;patch&gt;[.&lt;tweak&gt;]]]]\n        [LANGUAGES &lt;language-name&gt;...])\n\n设置项目的名称，并将其存储在变量中 PROJECT_NAME。从顶层调用时， CMakeLists.txt还将项目名称存储在变量CMAKE_PROJECT_NAME中。\n\n同时设置变量\n\nPROJECT_SOURCE_DIR， &lt;PROJECT-NAME&gt;_SOURCE_DIR\nPROJECT_BINARY_DIR， &lt;PROJECT-NAME&gt;_BINARY_DIR\n\nhttps://cmake.org/cmake/help/v3.15/command/project.html</code></pre>\n<h3 id=\"SET\"><a href=\"#SET\" class=\"headerlink\" title=\"SET\"></a>SET</h3><pre><code class=\"shell\">将普通变量，缓存变量或环境变量设置为给定值。\n指定&lt;value&gt;...占位符的此命令的签名期望零个或多个参数。多个参数将以分号分隔的列表形式加入，以形成要设置的实际变量值。零参数将导致未设置普通变量。unset() 命令显式取消设置变量。\n1、设置正常变量\nset(&lt;variable&gt; &lt;value&gt;... [PARENT_SCOPE])\n&lt;variable&gt;在当前函数或目录范围内设置给定值。\n如果PARENT_SCOPE给出了该选项，则将在当前作用域上方的作用域中设置变量。\n2、设置缓存变量\nset(&lt;variable&gt; &lt;value&gt;... CACHE &lt;type&gt; &lt;docstring&gt; [FORCE])\n3、设置环境变量\nset(ENV{&lt;variable&gt;} [&lt;value&gt;])\n</code></pre>\n<h3 id=\"add-executable\"><a href=\"#add-executable\" class=\"headerlink\" title=\"add_executable\"></a>add_executable</h3><pre><code class=\"shell\">使用指定的源文件生成可执行文件\nadd_executable(&lt;name&gt; [WIN32] [MACOSX_BUNDLE]\n               [EXCLUDE_FROM_ALL]\n               [source1] [source2 ...])\n&lt;name&gt;可执行文件名, &lt;name&gt;与逻辑目标名称相对应，并且在项目中必须是全局唯一的。构建的可执行文件的实际文件名是基于本机平台（例如&lt;name&gt;.exe或&lt;name&gt;）的约定构造的 。\n默认情况下，将在与调用命令的源树目录相对应的构建树目录中创建可执行文件。\n</code></pre>\n<h3 id=\"add-subdirectory\"><a href=\"#add-subdirectory\" class=\"headerlink\" title=\"add_subdirectory\"></a>add_subdirectory</h3><pre><code class=\"shell\">在构建中添加一个子目录。\nadd_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])\n将一个子目录添加到构建中。source_dir指定源CMakeLists.txt和代码文件所在的目录。binary_dir指定了输出文件放置的目录以及编译输出的路径。EXCLUDE_FROM_ALL 参数的含义是将这个目录从编译过程中排除，比如，工程的 example，可能就需要工程构建完成后，再进入 example 目录单独进行构建(当然，你也可以通过定义依赖来解决此类问题)。\n如果没有指定binary_dir,那么编译结果(包括中间结果)都将存放在\nbuild/source_dir 目录(这个目录跟原有的 source_dir 目录对应)，指定binary_dir 目录后，相当于在编译时将 source_dir 重命名为binary_dir，所有的中间结果和目标二进制都将存放在binary_dir 目录。</code></pre>\n<h3 id=\"subdirs\"><a href=\"#subdirs\" class=\"headerlink\" title=\"subdirs\"></a>subdirs</h3><pre><code class=\"shell\">构建多个子目录\nsubdirs(dir1 dir2 ...[EXCLUDE_FROM_ALL exclude_dir1 exclude_dir2 ...]\n        [PREORDER] )\n\n\n不论是 SUBDIRS 还是 ADD_SUBDIRECTORY 指令(不论是否指定编译输出目录)，我们都可以通过 SET 指令重新定义EXECUTABLE_OUTPUT_PATH 和 LIBRARY_OUTPUT_PATH 变量\n来指定最终的目标二进制的位置(指最终生成的 hello 或者最终的共享库，不包含编译生成的中间文件)\nSET(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)\nSET(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/lib)\n在第一节我们提到了&lt;projectname&gt;_BINARY_DIR 和 PROJECT_BINARY_DIR 变量，他们指的编译发生的当前目录，如果是内部编译，就相当于 PROJECT_SOURCE_DIR 也就是工程代码所在目录，如果是外部编译，指的是外部编译所在目录，也就是本例中的两个指令分别定义了：可执行二进制的输出路径为 build/bin 和库的输出路径为 build/lib.</code></pre>\n<h3 id=\"add-library\"><a href=\"#add-library\" class=\"headerlink\" title=\"add_library\"></a>add_library</h3><pre><code class=\"shell\">ADD_LIBRARY(libname [SHARED|STATIC|MODULE]\n[EXCLUDE_FROM_ALL]\nsource1 source2 ... sourceN)\n你不需要写全 libhello.so，只需要填写 hello 即可，cmake 系统会自动为你生成\nlibhello.X\n类型有三种:\nSHARED，动态库\nSTATIC，静态库\nMODULE，在使用 dyld 的系统有效，如果不支持 dyld，则被当作 SHARED 对待。\nEXCLUDE_FROM_ALL 参数的意思是这个库不会被默认构建，除非有其他的组件依赖或者手\n工构建。</code></pre>\n<h3 id=\"include-directories\"><a href=\"#include-directories\" class=\"headerlink\" title=\"include_directories\"></a>include_directories</h3><pre><code class=\"shell\">将include目录添加到构建中\ninclude_directories([AFTER|BEFORE] [SYSTEM] dir1 [dir2 ...])\n将给定目录添加到编译器用于搜索头文件的路径中。\n这条指令可以用来向工程添加多个特定的头文件搜索路径，路径之间用空格分割，如果路径\n中包含了空格，可以使用双引号将它括起来，默认的行为是追加到当前的头文件搜索路径的\n后面，你可以通过两种方式来进行控制搜索路径添加的方式：\n１，CMAKE_INCLUDE_DIRECTORIES_BEFORE，通过 SET 这个 cmake 变量为 on，可以\n将添加的头文件搜索路径放在已有路径的前面。\n２，通过 AFTER 或者 BEFORE 参数，也可以控制是追加还是置前。</code></pre>\n<h3 id=\"target-link-libraries-amp-link-directories\"><a href=\"#target-link-libraries-amp-link-directories\" class=\"headerlink\" title=\"target_link_libraries &amp; link_directories\"></a>target_link_libraries &amp; link_directories</h3><pre><code class=\"shell\">TARGET_LINK_LIBRARIES(target library1\n&lt;debug | optimized&gt; library2\n...)\n这个指令可以用来为 target 添加需要链接的共享库，本例中是一个可执行文件，但是同样\n可以用于为自己编写的共享库添加共享库链接。\n为了解决我们前面遇到的 HelloFunc 未定义错误，我们需要作的是向\nsrc/CMakeLists.txt 中添加如下指令：\nTARGET_LINK_LIBRARIES(main hello)\n也可以写成\nTARGET_LINK_LIBRARIES(main libhello.so)</code></pre>\n<h3 id=\"ADD-DEFINITIONS\"><a href=\"#ADD-DEFINITIONS\" class=\"headerlink\" title=\"ADD_DEFINITIONS\"></a>ADD_DEFINITIONS</h3><pre><code class=\"shell\">向 C/C++编译器添加-D 定义，比如:\nADD_DEFINITIONS(-DENABLE_DEBUG -DABC)，参数之间用空格分割。\n如果你的代码中定义了#ifdef ENABLE_DEBUG #endif，这个代码块就会生效。如果要添加其他的编译器开关，可以通过 CMAKE_C_FLAGS 变量和 CMAKE_CXX_FLAGS 变量设置。</code></pre>\n<h3 id=\"ADD-DEPENDENCIES\"><a href=\"#ADD-DEPENDENCIES\" class=\"headerlink\" title=\"ADD_DEPENDENCIES\"></a>ADD_DEPENDENCIES</h3><pre><code class=\"shell\">定义 target 依赖的其他 target，确保在编译本 target 之前，其他的 target 已经被构建。\nADD_DEPENDENCIES(target-name depend-target1\ndepend-target2 ...)</code></pre>\n<h3 id=\"ADD-TEST-与-ENABLE-TESTING-指令。\"><a href=\"#ADD-TEST-与-ENABLE-TESTING-指令。\" class=\"headerlink\" title=\"ADD_TEST 与 ENABLE_TESTING 指令。\"></a>ADD_TEST 与 ENABLE_TESTING 指令。</h3><pre><code class=\"shell\">ENABLE_TESTING 指令用来控制 Makefile 是否构建 test 目标，涉及工程所有目录。语法很简单，没有任何参数，ENABLE_TESTING()，一般情况这个指令放在工程的主CMakeLists.txt 中.\nADD_TEST 指令的语法是:\n    `ADD_TEST(testname Exename arg1 arg2 ...)`\ntestname 是自定义的 test 名称，Exename 可以是构建的目标文件也可以是外部脚本等等。后面连接传递给可执行文件的参数。如果没有在同一个 CMakeLists.txt 中打开\n    ENABLE_TESTING()指令，任何 ADD_TEST 都是无效的。\n比如我们前面的 Helloworld 例子，可以在工程主 CMakeLists.txt 中添加\n\nADD_TEST(mytest ${PROJECT_BINARY_DIR}/bin/main)\nENABLE_TESTING()\n生成 Makefile 后，就可以运行 make test 来执行测试了。</code></pre>\n<h3 id=\"AUX-SOURCE-DIRECTORY\"><a href=\"#AUX-SOURCE-DIRECTORY\" class=\"headerlink\" title=\"AUX_SOURCE_DIRECTORY\"></a>AUX_SOURCE_DIRECTORY</h3><pre><code class=\"shell\">基本语法是：\nAUX_SOURCE_DIRECTORY(dir VARIABLE)\n作用是发现一个目录下所有的源代码文件并将列表存储在一个变量中，这个指令临时被用来\n自动构建源文件列表。因为目前 cmake 还不能自动发现新添加的源文件。\n比如\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nADD_EXECUTABLE(main ${SRC_LIST})\n你也可以通过后面提到的 FOREACH 指令来处理这个 LIST</code></pre>\n<p>###　CMAKE_MINIMUM_REQUIRED</p>\n<pre><code class=\"sehll\">其语法为 CMAKE_MINIMUM_REQUIRED(VERSION versionNumber [FATAL_ERROR])\n比如 CMAKE_MINIMUM_REQUIRED(VERSION 2.5 FATAL_ERROR)\n如果 cmake 版本小与 2.5，则出现严重错误，整个过程中止。</code></pre>\n<h3 id=\"EXEC-PROGRAM\"><a href=\"#EXEC-PROGRAM\" class=\"headerlink\" title=\"EXEC_PROGRAM\"></a>EXEC_PROGRAM</h3><p>在 CMakeLists.txt 处理过程中执行命令，并不会在生成的 Makefile 中执行。具体语法为：</p>\n<pre><code class=\"shell\">EXEC_PROGRAM(Executable [directory in which to run]\n[ARGS &lt;arguments to executable&gt;]\n[OUTPUT_VARIABLE &lt;var&gt;]\n[RETURN_VALUE &lt;var&gt;])</code></pre>\n<p>用于在指定的目录运行某个程序，通过 ARGS 添加参数，如果要获取输出和返回值，可通过OUTPUT_VARIABLE 和 RETURN_VALUE 分别定义两个变量.<br>这个指令可以帮助你在 CMakeLists.txt 处理过程中支持任何命令，比如根据系统情况去修改代码文件等等。<br>举个简单的例子，我们要在 src 目录执行 ls 命令，并把结果和返回值存下来。<br>可以直接在 src/CMakeLists.txt 中添加：<br>EXEC_PROGRAM(ls ARGS “<em>.c” OUTPUT_VARIABLE LS_OUTPUT RETURN_VALUE LS_RVALUE)<br>IF(not LS_RVALUE)<br>    MESSAGE(STATUS “ls result: “ ${LS_OUTPUT})<br>ENDIF(not LS_RVALUE)<br>在 cmake 生成 Makefile 的过程中，就会执行 ls 命令，如果返回 0，则说明成功执行，<br>那么就输出 ls </em>.c 的结果。关于 IF 语句，后面的控制指令会提到。</p>\n<h3 id=\"FILE-指令\"><a href=\"#FILE-指令\" class=\"headerlink\" title=\"FILE 指令\"></a>FILE 指令</h3><p>文件操作指令，基本语法为:</p>\n<pre><code class=\"shell\">FILE(WRITE filename &quot;message to write&quot;... )\nFILE(APPEND filename &quot;message to write&quot;... )\nFILE(READ filename variable)\nFILE(GLOB variable [RELATIVE path] [globbing\nexpressions]...)\nFILE(GLOB_RECURSE variable [RELATIVE path]\n[globbing expressions]...)\nFILE(REMOVE [directory]...)\nFILE(REMOVE_RECURSE [directory]...)\nFILE(MAKE_DIRECTORY [directory]...)\nFILE(RELATIVE_PATH variable directory file)\nFILE(TO_CMAKE_PATH path result)\nFILE(TO_NATIVE_PATH path result)</code></pre>\n<p>这里的语法都比较简单，不在展开介绍了。</p>\n<h3 id=\"INCLUDE-指令\"><a href=\"#INCLUDE-指令\" class=\"headerlink\" title=\"INCLUDE 指令\"></a>INCLUDE 指令</h3><pre><code class=\"shell\">用来载入 CMakeLists.txt 文件，也用于载入预定义的 cmake 模块.\n    INCLUDE(file1 [OPTIONAL])\n    INCLUDE(module [OPTIONAL])\nOPTIONAL 参数的作用是文件不存在也不会产生错误。\n你可以指定载入一个文件，如果定义的是一个模块，那么将在 CMAKE_MODULE_PATH 中搜索这个模块并载入。\n载入的内容将在处理到 INCLUDE 语句是直接执行。</code></pre>\n<h2 id=\"2-控制指令：\"><a href=\"#2-控制指令：\" class=\"headerlink\" title=\"2. 控制指令：\"></a>2. 控制指令：</h2><h3 id=\"1-IF-指令\"><a href=\"#1-IF-指令\" class=\"headerlink\" title=\"1. IF 指令\"></a>1. IF 指令</h3><p>基本语法为：</p>\n<pre><code class=\"shell\">IF(expression)\n\n# THEN section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nELSE(expression)\n\n# ELSE section.\n\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDIF(expression)</code></pre>\n<p>另外一个指令是 ELSEIF，总体把握一个原则，凡是出现 IF 的地方一定要有对应的<br>ENDIF.出现 ELSEIF 的地方，ENDIF 是可选的。<br>表达式的使用方法如下:<br>IF(var)，如果变量不是：空，0，N, NO, OFF, FALSE, NOTFOUND 或<br><var>_NOTFOUND 时，表达式为真。<br>IF(NOT var )，与上述条件相反。<br>IF(var1 AND var2)，当两个变量都为真是为真。<br>IF(var1 OR var2)，当两个变量其中一个为真时为真。<br>IF(COMMAND cmd)，当给定的 cmd 确实是命令并可以调用是为真。<br>IF(EXISTS dir)或者 IF(EXISTS file)，当目录名或者文件名存在时为真。<br>IF(file1 IS_NEWER_THAN file2)，当 file1 比 file2 新，或者 file1/file2 其中有一个不存在时为真，文件名请使用完整路径。<br>IF(IS_DIRECTORY dirname)，当 dirname 是目录时，为真。<br>IF(variable MATCHES regex)<br>IF(string MATCHES regex)<br>当给定的变量或者字符串能够匹配正则表达式 regex 时为真。比如：<br>IF(“hello” MATCHES “ell”)<br>MESSAGE(“true”)<br>ENDIF(“hello” MATCHES “ell”)<br>IF(variable LESS number)<br>IF(string LESS number)<br>IF(variable GREATER number)<br>IF(string GREATER number)<br>IF(variable EQUAL number)<br>IF(string EQUAL number)<br>数字比较表达式<br>IF(variable STRLESS string)<br>IF(string STRLESS string)<br>IF(variable STRGREATER string)<br>IF(string STRGREATER string)<br>IF(variable STREQUAL string)<br>IF(string STREQUAL string)<br>按照字母序的排列进行比较.<br>IF(DEFINED variable)，如果变量被定义，为真。<br>一个小例子，用来判断平台差异：<br>IF(WIN32)<br>MESSAGE(STATUS “This is windows.”)</var></p>\n<p>#作一些 Windows 相关的操作<br>ELSE(WIN32)<br>MESSAGE(STATUS “This is not windows”)</p>\n<p>#作一些非 Windows 相关的操作<br>ENDIF(WIN32)<br>上述代码用来控制在不同的平台进行不同的控制，但是，阅读起来却并不是那么舒服，<br>ELSE(WIN32)之类的语句很容易引起歧义。<br>这就用到了我们在“常用变量”一节提到的 CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS 开<br>关。<br>可以 SET(CMAKE_ALLOW_LOOSE_LOOP_CONSTRUCTS ON)<br>这时候就可以写成:<br>IF(WIN32)<br>ELSE()<br>ENDIF()<br>如果配合 ELSEIF 使用，可能的写法是这样:<br>IF(WIN32)</p>\n<p>#do something related to WIN32<br>ELSEIF(UNIX)</p>\n<p>#do something related to UNIX<br>ELSEIF(APPLE)</p>\n<p>#do something related to APPLE<br>ENDIF(WIN32)</p>\n<h3 id=\"2-WHILE\"><a href=\"#2-WHILE\" class=\"headerlink\" title=\"2. WHILE\"></a>2. WHILE</h3><p>WHILE 指令的语法是：</p>\n<pre><code class=\"shell\">WHILE(condition)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDWHILE(condition)</code></pre>\n<p>其真假判断条件可以参考 IF 指令。</p>\n<h3 id=\"3-FOREACH\"><a href=\"#3-FOREACH\" class=\"headerlink\" title=\"3. FOREACH\"></a>3. FOREACH</h3><p>FOREACH 指令的使用方法有三种形式：</p>\n<pre><code class=\"shell\">1，列表\nFOREACH(loop_var arg1 arg2 ...)\nCOMMAND1(ARGS ...)\nCOMMAND2(ARGS ...)\n...\nENDFOREACH(loop_var)\n像我们前面使用的 AUX_SOURCE_DIRECTORY 的例子\nAUX_SOURCE_DIRECTORY(. SRC_LIST)\nFOREACH(F ${SRC_LIST})\nMESSAGE(${F})\nENDFOREACH(F)\n2，范围\nFOREACH(loop_var RANGE total)\nENDFOREACH(loop_var)\n从 0 到 total 以１为步进\n举例如下：\nFOREACH(VAR RANGE 10)\nMESSAGE(${VAR})\nENDFOREACH(VAR)\n最终得到的输出是：\n0 1 2 3 4 5 6 7 8 9\n10\n３，范围和步进\nFOREACH(loop_var RANGE start stop [step])\nENDFOREACH(loop_var)\n从 start 开始到 stop 结束，以 step 为步进，\n举例如下\nFOREACH(A RANGE 5 15 3)\nMESSAGE(${A})\nENDFOREACH(A)\n最终得到的结果是：\n5 8\n11\n14\n这个指令需要注意的是，知道遇到 ENDFOREACH 指令，整个语句块才会得到真正的执行。</code></pre>\n<h1 id=\"十、CMakeLists配置模板\"><a href=\"#十、CMakeLists配置模板\" class=\"headerlink\" title=\"十、CMakeLists配置模板\"></a>十、<code>CMakeLists</code>配置模板</h1><h2 id=\"１-基本配置\"><a href=\"#１-基本配置\" class=\"headerlink\" title=\"１.基本配置\"></a>１.基本配置</h2><pre><code class=\"shell\">cmake_minimum_required(VERSION 3.14)\nproject(XXX_Project)\n\n# 设置CMAKE版本\nset(CMAKE_CXX_STANDARD 14)\n\n# 设置输出目录为 build/Debug/bin build/Debug/lib\n# 并缓存路径\nset(OUTPUT_DIRECTORY_ROOT ${CMAKE_CURRENT_SOURCE_DIR}/build/${CMAKE_BUILD_TYPE})\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY &quot;${OUTPUT_DIRECTORY_ROOT}/bin&quot; CACHE PATH &quot;Runtime directory&quot; FORCE)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY &quot;${OUTPUT_DIRECTORY_ROOT}/lib&quot; CACHE PATH &quot;Library directory&quot; FORCE)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY &quot;${OUTPUT_DIRECTORY_ROOT}/lib&quot; CACHE PATH &quot;Archive directory&quot; FORCE)\n\n# 添加src子目录\nadd_subdirectory(src)</code></pre>\n<h2 id=\"２-依赖库相关配置\"><a href=\"#２-依赖库相关配置\" class=\"headerlink\" title=\"２.依赖库相关配置\"></a>２.依赖库相关配置</h2><p><strong><code>OPenCV</code>依赖库</strong></p>\n<p>将<code>OpenCV</code>依赖库下的<code>share/OpenCV</code>中，<code>OpenCVConfig.cmake</code>复制一份叫<code>FindOpenCV.cmake</code>，然后在根目录的CMakeLists.txt添加如下配置</p>\n<pre><code class=\"shell\">#　添加make文件搜索路径\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ~/3rdparty/OpenCV-3.4.7/share/OpenCV)\n\n# 查找cmake文件，并初始化变量\nfind_package(OpenCV REQUIRED)\n# 添加头文件搜索路径\ninclude_directories(${OpenCV_INCLUDE_DIRS})\n\n# 给执行程序添加链接库\nadd_executable(XXXXMain main.cpp)\ntarget_link_libraries(XXXXMain ${OpenCV_LIBS})</code></pre>\n<h1 id=\"十一、参考\"><a href=\"#十一、参考\" class=\"headerlink\" title=\"十一、参考\"></a>十一、参考</h1><ol>\n<li>[<a href=\"http://file.ncnynl.com/ros/CMake%20Practice.pdf]\" target=\"_blank\" rel=\"noopener\">http://file.ncnynl.com/ros/CMake%20Practice.pdf]</a>(<a href=\"http://file.ncnynl.com/ros/CMake\" target=\"_blank\" rel=\"noopener\">http://file.ncnynl.com/ros/CMake</a> Practice.pdf)</li>\n<li><a href=\"https://cmake.org/cmake/help/latest/guide/tutorial/index.html\" target=\"_blank\" rel=\"noopener\">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></li>\n</ol>\n"},{"title":"Pyhton高数计算库","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-25T07:57:29.000Z","password":null,"summary":null,"_content":"\n\n\n# math数学库\n\n```python\n# 导入math库\nimport math\n\n# 常用数学常量\nmath.pi\t\t# π\nmath.e\nmath.inf\t# ∞\nmath.nan\t# not a num\n\n# 指数/对数/开平方\nmath.exp(a) \t# math.e**a\nmath.log(a)\t\t# 自然底数 math.e\nmath.log(a, b)\t# 以b为底，b**x = a\nmath.sqrt(a)\t# 开平方\n\n# 近似值\nmath.ceil(4.1)\t# roud up to 5\nmath.floor(4.9) # roud up to 4\n\n# 阶乘\nmath.factorial(a) # a!\n\n# 最大公约数\nmath.gcd(35, 49)  # 7\n\n# 三角函数\nmath.sin(math.pi/2)\t\t# 1.0\nmath.cos()\nmath.tan()\nmath.asin(1)\t# 1.5707963267948966\nmath.acos()\nmath.atan()\n\n# 弧度角度转换\nmath.degrees()\t# 弧度转角度\nmath.radians()\t# 角度转弧度\n\n```\n\n\n\n\n\n# sympy代数运算库\n\n```python\n# 导入库\nfrom sympy import *\n\n# 有理数\nRational(1, 3)\t# 1/3\n\n# 特殊无理数\npi\t# math.pi\nE\t# math.e\noo\t# math.inf\n\n# jupyter pretty print\ninit_printing(pretty_print=True)  # Pretty printing mode\nN(pi) = pi.eval()\t# 3.15..默认取前15位\n# .n() and N() are equivalent to .evalf();\n\n# 代数运算 用符号代替数进行运算\nx = Symbol('x')\t# 声明一个代数符号\nx,y = symbols('x y') # 一次声明的多个代数符号\n(x+y)**2  # (𝑥+𝑦)2\n\n# 展开和分解\n# 展开多项式\nexpand((x+y)**2)\t# 𝑥2+2𝑥𝑦+𝑦2\n\n# 展开三角函数\nexpand(cos(x+y)**2, trig=True)  # sin2(𝑥)sin2(𝑦)−2sin(𝑥)sin(𝑦)cos(𝑥)cos(𝑦)+cos2(𝑥)cos2(𝑦)\n\n# 化简\nsimplify((x+x*y)/x)  # 1+y\n```\n\n## 累加运算\n\n$$\n\\sum_{x=1}^{10} {\\frac {1}{x^2 + 2x}}\n$$\n\n```python\nexpr = Sum(1/(x**2 + 2*x), (x, 1, 10))\nexpr # 上面公式\nexpr.evalf() # 求值 0.662878787878788\nexpr.doit()  # 175/264\n```\n\n\n\n## 累积运算\n\n$$\n\\prod_{x=1}^{10} {\\frac {1}{x^2 + 2x}}\n$$\n\n```python\nexpr = Product(1/(x**2 + 2*x), (x, 1, 10))\nexpr\nexpr.doit()\t# 1/869100503040000\n```\n\n\n\n## 极限\n\n$$\n\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad \n$$\n\n```python\nn = Symbol('n')\nexpr = limit(1/(n*(n+1)), n, oo)\nexpr\t# 0\n\n# 左极限和有极限\nlimit(1/x, x, 0, dir='+')\nlimit(1/x, x, 0, dir='-')\n```\n\n\n\n## 导数\n\n```python\ndiff(x**2, x)\t# 2x\ndiff(sin(2*x), x)\t# 2cos(2𝑥)\ndiff(sin(x**2+2*x),x) # diff(E**x*(x + sin(x)), x)\n\n# 高阶导数\n# 二阶导数\ndiff(sin(2*x), x, 2)  # −4sin(2𝑥)\n# 三阶导数\ndiff(sin(2*x), x, 3)\t# −8cos(2𝑥)\n```\n\n\n\n## 积分\n\n不指定区间\n$$\n\\int_{-\\infty}^\\infty {x^2} \\,{\\rm dx}\n$$\n\n```python\nintegrate(2*x, x) # 𝑥2\nintegrate(sin(x), x) # −cos(𝑥)\n```\n\n\n\n指定区间[a, b]\n$$\n\\int_a^b {x^2} \\,{\\rm dx}\n$$\n\n```python\nintegrate(2*x, (x, 0, 1)) # 1\nintegrate(cos(x), (x, -pi/2, pi/2)) # 2\n```\n\n\n\n## 解方程\n\n```python\n# 解一元方程\nsolve(x**2-3*x+2, x)\t# [1, 2]\n\n# 解二元方程\nsolve([x+5*y-2, -3*x+6*y-15], [x, y]) #{x:-3, y:1}\n```\n\n\n\n## 代数运算\n\n```python\nexpr = x**2 + 2*x + 1\n# 令x = 2\nexpr.subs(x, 2)\t# 9b\n\n# 令x=y+1\nexpr.subs(x, y+1)\t# 2𝑦+(𝑦+1)2+3\n\n# 多元函数代数\nexpr = x**3 + 4*x*y -z\nexpr.subs([(x,1), (y, 1), (z, 0)]) # 5\n\n# 使用字符串\nexpr = sympify(\"x*2 + 4*x*y\")\nexpr.subs([(x, 1), (y, 1)])\t# 6\n```\n\n\n\n## 概率论\n\n```python\nfrom sympy import stats\n\n#创建一个6个面的筛子\nX = stats.Die('X', 6)\n# 查看某个面出现的概率\nstats.density(X).dict\t# {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\n# 随机丢一次筛子\nstats.sample(X)\t# 4\n\n# \t硬币\nC = stats.Coin('C')\t\nstats.density(C).dict\t# {H: 1/2, T: 1/2}\n\n# 正态分布\nZ = stats.Normal('Z', 0, 1)\n# Z>1的概率\nstats.P(Z > 1).evalf() # 0.158655253931457\n```\n\n","source":"_posts/Pyhton高数计算库.md","raw":"---\ntitle: Pyhton高数计算库\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-25 15:57:29\npassword:\nsummary:\ntags: \n- python\n- math\n- sympy\ncategories: Python\n---\n\n\n\n# math数学库\n\n```python\n# 导入math库\nimport math\n\n# 常用数学常量\nmath.pi\t\t# π\nmath.e\nmath.inf\t# ∞\nmath.nan\t# not a num\n\n# 指数/对数/开平方\nmath.exp(a) \t# math.e**a\nmath.log(a)\t\t# 自然底数 math.e\nmath.log(a, b)\t# 以b为底，b**x = a\nmath.sqrt(a)\t# 开平方\n\n# 近似值\nmath.ceil(4.1)\t# roud up to 5\nmath.floor(4.9) # roud up to 4\n\n# 阶乘\nmath.factorial(a) # a!\n\n# 最大公约数\nmath.gcd(35, 49)  # 7\n\n# 三角函数\nmath.sin(math.pi/2)\t\t# 1.0\nmath.cos()\nmath.tan()\nmath.asin(1)\t# 1.5707963267948966\nmath.acos()\nmath.atan()\n\n# 弧度角度转换\nmath.degrees()\t# 弧度转角度\nmath.radians()\t# 角度转弧度\n\n```\n\n\n\n\n\n# sympy代数运算库\n\n```python\n# 导入库\nfrom sympy import *\n\n# 有理数\nRational(1, 3)\t# 1/3\n\n# 特殊无理数\npi\t# math.pi\nE\t# math.e\noo\t# math.inf\n\n# jupyter pretty print\ninit_printing(pretty_print=True)  # Pretty printing mode\nN(pi) = pi.eval()\t# 3.15..默认取前15位\n# .n() and N() are equivalent to .evalf();\n\n# 代数运算 用符号代替数进行运算\nx = Symbol('x')\t# 声明一个代数符号\nx,y = symbols('x y') # 一次声明的多个代数符号\n(x+y)**2  # (𝑥+𝑦)2\n\n# 展开和分解\n# 展开多项式\nexpand((x+y)**2)\t# 𝑥2+2𝑥𝑦+𝑦2\n\n# 展开三角函数\nexpand(cos(x+y)**2, trig=True)  # sin2(𝑥)sin2(𝑦)−2sin(𝑥)sin(𝑦)cos(𝑥)cos(𝑦)+cos2(𝑥)cos2(𝑦)\n\n# 化简\nsimplify((x+x*y)/x)  # 1+y\n```\n\n## 累加运算\n\n$$\n\\sum_{x=1}^{10} {\\frac {1}{x^2 + 2x}}\n$$\n\n```python\nexpr = Sum(1/(x**2 + 2*x), (x, 1, 10))\nexpr # 上面公式\nexpr.evalf() # 求值 0.662878787878788\nexpr.doit()  # 175/264\n```\n\n\n\n## 累积运算\n\n$$\n\\prod_{x=1}^{10} {\\frac {1}{x^2 + 2x}}\n$$\n\n```python\nexpr = Product(1/(x**2 + 2*x), (x, 1, 10))\nexpr\nexpr.doit()\t# 1/869100503040000\n```\n\n\n\n## 极限\n\n$$\n\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad \n$$\n\n```python\nn = Symbol('n')\nexpr = limit(1/(n*(n+1)), n, oo)\nexpr\t# 0\n\n# 左极限和有极限\nlimit(1/x, x, 0, dir='+')\nlimit(1/x, x, 0, dir='-')\n```\n\n\n\n## 导数\n\n```python\ndiff(x**2, x)\t# 2x\ndiff(sin(2*x), x)\t# 2cos(2𝑥)\ndiff(sin(x**2+2*x),x) # diff(E**x*(x + sin(x)), x)\n\n# 高阶导数\n# 二阶导数\ndiff(sin(2*x), x, 2)  # −4sin(2𝑥)\n# 三阶导数\ndiff(sin(2*x), x, 3)\t# −8cos(2𝑥)\n```\n\n\n\n## 积分\n\n不指定区间\n$$\n\\int_{-\\infty}^\\infty {x^2} \\,{\\rm dx}\n$$\n\n```python\nintegrate(2*x, x) # 𝑥2\nintegrate(sin(x), x) # −cos(𝑥)\n```\n\n\n\n指定区间[a, b]\n$$\n\\int_a^b {x^2} \\,{\\rm dx}\n$$\n\n```python\nintegrate(2*x, (x, 0, 1)) # 1\nintegrate(cos(x), (x, -pi/2, pi/2)) # 2\n```\n\n\n\n## 解方程\n\n```python\n# 解一元方程\nsolve(x**2-3*x+2, x)\t# [1, 2]\n\n# 解二元方程\nsolve([x+5*y-2, -3*x+6*y-15], [x, y]) #{x:-3, y:1}\n```\n\n\n\n## 代数运算\n\n```python\nexpr = x**2 + 2*x + 1\n# 令x = 2\nexpr.subs(x, 2)\t# 9b\n\n# 令x=y+1\nexpr.subs(x, y+1)\t# 2𝑦+(𝑦+1)2+3\n\n# 多元函数代数\nexpr = x**3 + 4*x*y -z\nexpr.subs([(x,1), (y, 1), (z, 0)]) # 5\n\n# 使用字符串\nexpr = sympify(\"x*2 + 4*x*y\")\nexpr.subs([(x, 1), (y, 1)])\t# 6\n```\n\n\n\n## 概率论\n\n```python\nfrom sympy import stats\n\n#创建一个6个面的筛子\nX = stats.Die('X', 6)\n# 查看某个面出现的概率\nstats.density(X).dict\t# {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\n# 随机丢一次筛子\nstats.sample(X)\t# 4\n\n# \t硬币\nC = stats.Coin('C')\t\nstats.density(C).dict\t# {H: 1/2, T: 1/2}\n\n# 正态分布\nZ = stats.Normal('Z', 0, 1)\n# Z>1的概率\nstats.P(Z > 1).evalf() # 0.158655253931457\n```\n\n","slug":"Pyhton高数计算库","published":1,"updated":"2019-12-02T08:53:06.517Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454trt0003zsv5pykx7evg","content":"<h1 id=\"math数学库\"><a href=\"#math数学库\" class=\"headerlink\" title=\"math数学库\"></a>math数学库</h1><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 导入math库</span>\n<span class=\"token keyword\">import</span> math\n\n<span class=\"token comment\" spellcheck=\"true\"># 常用数学常量</span>\nmath<span class=\"token punctuation\">.</span>pi        <span class=\"token comment\" spellcheck=\"true\"># π</span>\nmath<span class=\"token punctuation\">.</span>e\nmath<span class=\"token punctuation\">.</span>inf    <span class=\"token comment\" spellcheck=\"true\"># ∞</span>\nmath<span class=\"token punctuation\">.</span>nan    <span class=\"token comment\" spellcheck=\"true\"># not a num</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 指数/对数/开平方</span>\nmath<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>     <span class=\"token comment\" spellcheck=\"true\"># math.e**a</span>\nmath<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>        <span class=\"token comment\" spellcheck=\"true\"># 自然底数 math.e</span>\nmath<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> b<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 以b为底，b**x = a</span>\nmath<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 开平方</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 近似值</span>\nmath<span class=\"token punctuation\">.</span>ceil<span class=\"token punctuation\">(</span><span class=\"token number\">4.1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># roud up to 5</span>\nmath<span class=\"token punctuation\">.</span>floor<span class=\"token punctuation\">(</span><span class=\"token number\">4.9</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># roud up to 4</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 阶乘</span>\nmath<span class=\"token punctuation\">.</span>factorial<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># a!</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 最大公约数</span>\nmath<span class=\"token punctuation\">.</span>gcd<span class=\"token punctuation\">(</span><span class=\"token number\">35</span><span class=\"token punctuation\">,</span> <span class=\"token number\">49</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># 7</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 三角函数</span>\nmath<span class=\"token punctuation\">.</span>sin<span class=\"token punctuation\">(</span>math<span class=\"token punctuation\">.</span>pi<span class=\"token operator\">/</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>        <span class=\"token comment\" spellcheck=\"true\"># 1.0</span>\nmath<span class=\"token punctuation\">.</span>cos<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmath<span class=\"token punctuation\">.</span>tan<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmath<span class=\"token punctuation\">.</span>asin<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 1.5707963267948966</span>\nmath<span class=\"token punctuation\">.</span>acos<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmath<span class=\"token punctuation\">.</span>atan<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 弧度角度转换</span>\nmath<span class=\"token punctuation\">.</span>degrees<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 弧度转角度</span>\nmath<span class=\"token punctuation\">.</span>radians<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 角度转弧度</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"sympy代数运算库\"><a href=\"#sympy代数运算库\" class=\"headerlink\" title=\"sympy代数运算库\"></a>sympy代数运算库</h1><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 导入库</span>\n<span class=\"token keyword\">from</span> sympy <span class=\"token keyword\">import</span> <span class=\"token operator\">*</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 有理数</span>\nRational<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 1/3</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 特殊无理数</span>\npi    <span class=\"token comment\" spellcheck=\"true\"># math.pi</span>\nE    <span class=\"token comment\" spellcheck=\"true\"># math.e</span>\noo    <span class=\"token comment\" spellcheck=\"true\"># math.inf</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># jupyter pretty print</span>\ninit_printing<span class=\"token punctuation\">(</span>pretty_print<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># Pretty printing mode</span>\nN<span class=\"token punctuation\">(</span>pi<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> pi<span class=\"token punctuation\">.</span>eval<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 3.15..默认取前15位</span>\n<span class=\"token comment\" spellcheck=\"true\"># .n() and N() are equivalent to .evalf();</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 代数运算 用符号代替数进行运算</span>\nx <span class=\"token operator\">=</span> Symbol<span class=\"token punctuation\">(</span><span class=\"token string\">'x'</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 声明一个代数符号</span>\nx<span class=\"token punctuation\">,</span>y <span class=\"token operator\">=</span> symbols<span class=\"token punctuation\">(</span><span class=\"token string\">'x y'</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 一次声明的多个代数符号</span>\n<span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span><span class=\"token operator\">**</span><span class=\"token number\">2</span>  <span class=\"token comment\" spellcheck=\"true\"># (𝑥+𝑦)2</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 展开和分解</span>\n<span class=\"token comment\" spellcheck=\"true\"># 展开多项式</span>\nexpand<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span><span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 𝑥2+2𝑥𝑦+𝑦2</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 展开三角函数</span>\nexpand<span class=\"token punctuation\">(</span>cos<span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span><span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> trig<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># sin2(𝑥)sin2(𝑦)−2sin(𝑥)sin(𝑦)cos(𝑥)cos(𝑦)+cos2(𝑥)cos2(𝑦)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 化简</span>\nsimplify<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">+</span>x<span class=\"token operator\">*</span>y<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span>x<span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># 1+y</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"累加运算\"><a href=\"#累加运算\" class=\"headerlink\" title=\"累加运算\"></a>累加运算</h2><p>$$<br>\\sum_{x=1}^{10} {\\frac {1}{x^2 + 2x}}<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">expr <span class=\"token operator\">=</span> Sum<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nexpr <span class=\"token comment\" spellcheck=\"true\"># 上面公式</span>\nexpr<span class=\"token punctuation\">.</span>evalf<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 求值 0.662878787878788</span>\nexpr<span class=\"token punctuation\">.</span>doit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># 175/264</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"累积运算\"><a href=\"#累积运算\" class=\"headerlink\" title=\"累积运算\"></a>累积运算</h2><p>$$<br>\\prod_{x=1}^{10} {\\frac {1}{x^2 + 2x}}<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">expr <span class=\"token operator\">=</span> Product<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nexpr\nexpr<span class=\"token punctuation\">.</span>doit<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 1/869100503040000</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"极限\"><a href=\"#极限\" class=\"headerlink\" title=\"极限\"></a>极限</h2><p>$$<br>\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">n <span class=\"token operator\">=</span> Symbol<span class=\"token punctuation\">(</span><span class=\"token string\">'n'</span><span class=\"token punctuation\">)</span>\nexpr <span class=\"token operator\">=</span> limit<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>n<span class=\"token operator\">*</span><span class=\"token punctuation\">(</span>n<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> n<span class=\"token punctuation\">,</span> oo<span class=\"token punctuation\">)</span>\nexpr    <span class=\"token comment\" spellcheck=\"true\"># 0</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 左极限和有极限</span>\nlimit<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span>x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> dir<span class=\"token operator\">=</span><span class=\"token string\">'+'</span><span class=\"token punctuation\">)</span>\nlimit<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token operator\">/</span>x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> dir<span class=\"token operator\">=</span><span class=\"token string\">'-'</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"导数\"><a href=\"#导数\" class=\"headerlink\" title=\"导数\"></a>导数</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\">diff<span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 2x</span>\ndiff<span class=\"token punctuation\">(</span>sin<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 2cos(2𝑥)</span>\ndiff<span class=\"token punctuation\">(</span>sin<span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token operator\">+</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>x<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># diff(E**x*(x + sin(x)), x)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 高阶导数</span>\n<span class=\"token comment\" spellcheck=\"true\"># 二阶导数</span>\ndiff<span class=\"token punctuation\">(</span>sin<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># −4sin(2𝑥)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 三阶导数</span>\ndiff<span class=\"token punctuation\">(</span>sin<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># −8cos(2𝑥)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"积分\"><a href=\"#积分\" class=\"headerlink\" title=\"积分\"></a>积分</h2><p>不指定区间<br>$$<br>\\int_{-\\infty}^\\infty {x^2} \\,{\\rm dx}<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">integrate<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 𝑥2</span>\nintegrate<span class=\"token punctuation\">(</span>sin<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># −cos(𝑥)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p>指定区间[a, b]<br>$$<br>\\int_a^b {x^2} \\,{\\rm dx}<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">integrate<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>x<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 1</span>\nintegrate<span class=\"token punctuation\">(</span>cos<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span>pi<span class=\"token operator\">/</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> pi<span class=\"token operator\">/</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 2</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<h2 id=\"解方程\"><a href=\"#解方程\" class=\"headerlink\" title=\"解方程\"></a>解方程</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 解一元方程</span>\nsolve<span class=\"token punctuation\">(</span>x<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token operator\">*</span>x<span class=\"token operator\">+</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># [1, 2]</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 解二元方程</span>\nsolve<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x<span class=\"token operator\">+</span><span class=\"token number\">5</span><span class=\"token operator\">*</span>y<span class=\"token number\">-2</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token operator\">*</span>x<span class=\"token operator\">+</span><span class=\"token number\">6</span><span class=\"token operator\">*</span>y<span class=\"token number\">-15</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#{x:-3, y:1}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"代数运算\"><a href=\"#代数运算\" class=\"headerlink\" title=\"代数运算\"></a>代数运算</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\">expr <span class=\"token operator\">=</span> x<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> <span class=\"token number\">2</span><span class=\"token operator\">*</span>x <span class=\"token operator\">+</span> <span class=\"token number\">1</span>\n<span class=\"token comment\" spellcheck=\"true\"># 令x = 2</span>\nexpr<span class=\"token punctuation\">.</span>subs<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 9b</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 令x=y+1</span>\nexpr<span class=\"token punctuation\">.</span>subs<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 2𝑦+(𝑦+1)2+3</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 多元函数代数</span>\nexpr <span class=\"token operator\">=</span> x<span class=\"token operator\">**</span><span class=\"token number\">3</span> <span class=\"token operator\">+</span> <span class=\"token number\">4</span><span class=\"token operator\">*</span>x<span class=\"token operator\">*</span>y <span class=\"token operator\">-</span>z\nexpr<span class=\"token punctuation\">.</span>subs<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 5</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 使用字符串</span>\nexpr <span class=\"token operator\">=</span> sympify<span class=\"token punctuation\">(</span><span class=\"token string\">\"x*2 + 4*x*y\"</span><span class=\"token punctuation\">)</span>\nexpr<span class=\"token punctuation\">.</span>subs<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 6</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"概率论\"><a href=\"#概率论\" class=\"headerlink\" title=\"概率论\"></a>概率论</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sympy <span class=\"token keyword\">import</span> stats\n\n<span class=\"token comment\" spellcheck=\"true\">#创建一个6个面的筛子</span>\nX <span class=\"token operator\">=</span> stats<span class=\"token punctuation\">.</span>Die<span class=\"token punctuation\">(</span><span class=\"token string\">'X'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 查看某个面出现的概率</span>\nstats<span class=\"token punctuation\">.</span>density<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>dict    <span class=\"token comment\" spellcheck=\"true\"># {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 随机丢一次筛子</span>\nstats<span class=\"token punctuation\">.</span>sample<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 4</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#     硬币</span>\nC <span class=\"token operator\">=</span> stats<span class=\"token punctuation\">.</span>Coin<span class=\"token punctuation\">(</span><span class=\"token string\">'C'</span><span class=\"token punctuation\">)</span>    \nstats<span class=\"token punctuation\">.</span>density<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>dict    <span class=\"token comment\" spellcheck=\"true\"># {H: 1/2, T: 1/2}</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 正态分布</span>\nZ <span class=\"token operator\">=</span> stats<span class=\"token punctuation\">.</span>Normal<span class=\"token punctuation\">(</span><span class=\"token string\">'Z'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># Z>1的概率</span>\nstats<span class=\"token punctuation\">.</span>P<span class=\"token punctuation\">(</span>Z <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>evalf<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># 0.158655253931457</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"math数学库\"><a href=\"#math数学库\" class=\"headerlink\" title=\"math数学库\"></a>math数学库</h1><pre><code class=\"python\"># 导入math库\nimport math\n\n# 常用数学常量\nmath.pi        # π\nmath.e\nmath.inf    # ∞\nmath.nan    # not a num\n\n# 指数/对数/开平方\nmath.exp(a)     # math.e**a\nmath.log(a)        # 自然底数 math.e\nmath.log(a, b)    # 以b为底，b**x = a\nmath.sqrt(a)    # 开平方\n\n# 近似值\nmath.ceil(4.1)    # roud up to 5\nmath.floor(4.9) # roud up to 4\n\n# 阶乘\nmath.factorial(a) # a!\n\n# 最大公约数\nmath.gcd(35, 49)  # 7\n\n# 三角函数\nmath.sin(math.pi/2)        # 1.0\nmath.cos()\nmath.tan()\nmath.asin(1)    # 1.5707963267948966\nmath.acos()\nmath.atan()\n\n# 弧度角度转换\nmath.degrees()    # 弧度转角度\nmath.radians()    # 角度转弧度\n</code></pre>\n<h1 id=\"sympy代数运算库\"><a href=\"#sympy代数运算库\" class=\"headerlink\" title=\"sympy代数运算库\"></a>sympy代数运算库</h1><pre><code class=\"python\"># 导入库\nfrom sympy import *\n\n# 有理数\nRational(1, 3)    # 1/3\n\n# 特殊无理数\npi    # math.pi\nE    # math.e\noo    # math.inf\n\n# jupyter pretty print\ninit_printing(pretty_print=True)  # Pretty printing mode\nN(pi) = pi.eval()    # 3.15..默认取前15位\n# .n() and N() are equivalent to .evalf();\n\n# 代数运算 用符号代替数进行运算\nx = Symbol(&#39;x&#39;)    # 声明一个代数符号\nx,y = symbols(&#39;x y&#39;) # 一次声明的多个代数符号\n(x+y)**2  # (𝑥+𝑦)2\n\n# 展开和分解\n# 展开多项式\nexpand((x+y)**2)    # 𝑥2+2𝑥𝑦+𝑦2\n\n# 展开三角函数\nexpand(cos(x+y)**2, trig=True)  # sin2(𝑥)sin2(𝑦)−2sin(𝑥)sin(𝑦)cos(𝑥)cos(𝑦)+cos2(𝑥)cos2(𝑦)\n\n# 化简\nsimplify((x+x*y)/x)  # 1+y</code></pre>\n<h2 id=\"累加运算\"><a href=\"#累加运算\" class=\"headerlink\" title=\"累加运算\"></a>累加运算</h2><p>$$<br>\\sum_{x=1}^{10} {\\frac {1}{x^2 + 2x}}<br>$$</p>\n<pre><code class=\"python\">expr = Sum(1/(x**2 + 2*x), (x, 1, 10))\nexpr # 上面公式\nexpr.evalf() # 求值 0.662878787878788\nexpr.doit()  # 175/264</code></pre>\n<h2 id=\"累积运算\"><a href=\"#累积运算\" class=\"headerlink\" title=\"累积运算\"></a>累积运算</h2><p>$$<br>\\prod_{x=1}^{10} {\\frac {1}{x^2 + 2x}}<br>$$</p>\n<pre><code class=\"python\">expr = Product(1/(x**2 + 2*x), (x, 1, 10))\nexpr\nexpr.doit()    # 1/869100503040000</code></pre>\n<h2 id=\"极限\"><a href=\"#极限\" class=\"headerlink\" title=\"极限\"></a>极限</h2><p>$$<br>\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad<br>$$</p>\n<pre><code class=\"python\">n = Symbol(&#39;n&#39;)\nexpr = limit(1/(n*(n+1)), n, oo)\nexpr    # 0\n\n# 左极限和有极限\nlimit(1/x, x, 0, dir=&#39;+&#39;)\nlimit(1/x, x, 0, dir=&#39;-&#39;)</code></pre>\n<h2 id=\"导数\"><a href=\"#导数\" class=\"headerlink\" title=\"导数\"></a>导数</h2><pre><code class=\"python\">diff(x**2, x)    # 2x\ndiff(sin(2*x), x)    # 2cos(2𝑥)\ndiff(sin(x**2+2*x),x) # diff(E**x*(x + sin(x)), x)\n\n# 高阶导数\n# 二阶导数\ndiff(sin(2*x), x, 2)  # −4sin(2𝑥)\n# 三阶导数\ndiff(sin(2*x), x, 3)    # −8cos(2𝑥)</code></pre>\n<h2 id=\"积分\"><a href=\"#积分\" class=\"headerlink\" title=\"积分\"></a>积分</h2><p>不指定区间<br>$$<br>\\int_{-\\infty}^\\infty {x^2} \\,{\\rm dx}<br>$$</p>\n<pre><code class=\"python\">integrate(2*x, x) # 𝑥2\nintegrate(sin(x), x) # −cos(𝑥)</code></pre>\n<p>指定区间[a, b]<br>$$<br>\\int_a^b {x^2} \\,{\\rm dx}<br>$$</p>\n<pre><code class=\"python\">integrate(2*x, (x, 0, 1)) # 1\nintegrate(cos(x), (x, -pi/2, pi/2)) # 2</code></pre>\n<h2 id=\"解方程\"><a href=\"#解方程\" class=\"headerlink\" title=\"解方程\"></a>解方程</h2><pre><code class=\"python\"># 解一元方程\nsolve(x**2-3*x+2, x)    # [1, 2]\n\n# 解二元方程\nsolve([x+5*y-2, -3*x+6*y-15], [x, y]) #{x:-3, y:1}</code></pre>\n<h2 id=\"代数运算\"><a href=\"#代数运算\" class=\"headerlink\" title=\"代数运算\"></a>代数运算</h2><pre><code class=\"python\">expr = x**2 + 2*x + 1\n# 令x = 2\nexpr.subs(x, 2)    # 9b\n\n# 令x=y+1\nexpr.subs(x, y+1)    # 2𝑦+(𝑦+1)2+3\n\n# 多元函数代数\nexpr = x**3 + 4*x*y -z\nexpr.subs([(x,1), (y, 1), (z, 0)]) # 5\n\n# 使用字符串\nexpr = sympify(&quot;x*2 + 4*x*y&quot;)\nexpr.subs([(x, 1), (y, 1)])    # 6</code></pre>\n<h2 id=\"概率论\"><a href=\"#概率论\" class=\"headerlink\" title=\"概率论\"></a>概率论</h2><pre><code class=\"python\">from sympy import stats\n\n#创建一个6个面的筛子\nX = stats.Die(&#39;X&#39;, 6)\n# 查看某个面出现的概率\nstats.density(X).dict    # {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\n# 随机丢一次筛子\nstats.sample(X)    # 4\n\n#     硬币\nC = stats.Coin(&#39;C&#39;)    \nstats.density(C).dict    # {H: 1/2, T: 1/2}\n\n# 正态分布\nZ = stats.Normal(&#39;Z&#39;, 0, 1)\n# Z&gt;1的概率\nstats.P(Z &gt; 1).evalf() # 0.158655253931457</code></pre>\n"},{"title":"Hexo+Github博客搭建","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-14T19:02:58.000Z","password":null,"summary":null,"_content":"\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=4913023&auto=1&height=66\"></iframe>\n\n# 前言\n\n​\t**准备工作**\n\n+ Github账号\n+ node.js、hexo、npm安装\n\n\n\n# 一、安装node.js\n\n1. 下载windows版node.js\n\n   下载地址: https://nodejs.org/en/download/\n\n   选择Windows Installer(.msi) 64-bit\n\n2. 双击node-v12.13.0-x64.msi, 一直next安装完成\n\n3. 测试是否安装成功\n\n   win+R键，输入cmd,然后回车，打开cmd窗口\n\n   输入node -v \t显示node.js版本\n\n   输入npm -v \t显示npm版本\n\n   安装完成\n\n   \n\n# 二、安装hexo\n\n1. 先创建hexo的安装目录, 例如:  F:\\LearnSpace\\Blog\n\n2. cd Blob  进入Blob目录\n\n3. npm install hexo-cli -g    安装hexo\n\n4. hexo -v  验证是否安装成功\n\n5. npm init blog    初始化blog文件夹，存放博客\n\n6. npm install 安装必备组件\n\n7. cd blog\n\n8. hexo g    生成静态网页\n\n9. hexo s     打开本地服务器\n\n10. http://localhost:4000/    打开网页\n\n11. ctrl + c   关闭本地服务器\n\n    \n\n# 三、连接Github与本地\n\n1. 新建一个名为`你的github用户名.github.io`的仓库，比如说，如果你的`Github`用户名是test，那么你就新建`test.github.io`的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是` http://test.github.io` 了。\n\n   点击`Settings`，向下拉到最后有个`GitHub Pages`，点击`Choose a theme`选择一个主题。然后等一会儿，再回到`GitHub Pages`, 就会像下面一样\n\n   ![](2.png)\n\n2. 修改配置文件\n\n   编辑blog根目录下的`_config.yml`, 修改最后一行的配置\n\n```\ndeploy:\n  type: git\n  repository: https://github.com/981935539/981935539.github.io.git\n  branch: master\n```\n\n3. 安装Git部署插件: `npm install hexo-deployer-git --save`\n\n# 四、编辑第一篇博客\n\n``` \nhexo new post \"first-article\"  # 创建第一篇博客\nhexo g  # 生成静态网页\nhexo s  # 本地预览效果\nhexo d  # 上传github\n```\n\n此时可以在github.io主页就能看到发布的文章啦。\n\n# 五、绑定域名\n\n1. 以阿里云为例，如下图所示，添加两条解析记录:\n\n​\t![](1.png)\n\n2. 然后打开你的Github博客项目，点击`settings`，拉到下面`Custom domain`处，填上你自己的域名，保存\n\n3. 这时候你的`F:\\LearnSpace\\Blog\\blob\\source` 会出现一个CNAME的文件\n\n4. 如果没有CNAME文件\n\n   打开你本地博客`/source`目录，我的是`F:\\LearnSpace\\Blog\\blob\\source`，新建`CNAME`文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行`hexo g`、`hexo d`上传到Github。\n\n   \n\n# 六、hexo常用命令\n\n```\nnpm install hexo-cli -g  \t# 安装hexo\nnpm uninstall hexo-cli -g  \t# 卸载hexo\n\nhexo generate #生成静态页面至public目录\nhexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）\nhexo deploy #部署到GitHub\nhexo help  # 查看帮助\nhexo version  #查看Hexo的版本\n\n# 缩写\nhexo n == hexo new\nhexo g == hexo generate\nhexo s == hexo server\nhexo d == hexo deploy\n\n# 组合\nhexo s -g #生成并本地预览\nhexo d -g #生成并上传\n```\n\n\n\n# 七、写博客的规范\n\n1. _config.yml\n\n   冒号后面必须有一个空格，否则会出问题\n\n2. 图片\n\n   引用图片需要把图片放在对应的文件夹中，只需要写文件名就可以了\n\n3. 文章头设置\n\n   模板在/scaffolds/post.md\n\n   ```\n   --- \n   title: {{ title }} # 文章名称\n   date: {{ date }} # 文章生成时间\n   top: false \n   cover: false \n   password: \n   toc: true \n   mathjax: true \n   summary: \n   tags:\n   -- [tag1]\n   -- [tag2]\n   -- [tag3]\n   categories: \n   -- [cat1]\n   ---\n   ```\n\n   \n\n# 八、备份博客源文件\n\n​\t博客已经搭建完成，但是博客仓库只是保存生成的静态网页文件，是没有博客源文件的，如果电脑出现了问题，那就麻烦了，所以源文件也需要备份一下。\n\n1. 在`Github`上创建一个与本地仓库同名的仓库, 我的是`hexo-matery`\n\n2. 初始化本地仓库\n\n   ```shell\n   git init       \n   添加.gitignore文件\n   .gitignore\n       .DS_Store\n       Thumbs.db\n       *.log\n       public/\n       .deploy*/\n       .vscode/\n   ```\n\n   \n\n3. 连接到远程`Github`,\n\n   ```shell\n   git remote add github git@github.com:981935539/hexo-matery.git\n   git fetch\n   git merge --allow-unrelated-histories github/master\n   ```\n\n4. 推送本地源文件到`Github`\n\n   ```shell\n   git add .\n   git commit -m \"第一次备份本地仓库\"\n   git push --set-upstream github master\n   ```\n\n   \n\n5. 现在在任何一台电脑上, 执行`git clonegit@github.com:981935539/hexo-matery.git`\n\n   就可以把博客源文件复制到本地。\n\n   \n\n# 九、Ubuntu安装node.js和hexo\n\n```shell\ntar -xvf node-v12.13.0-linux-x64.tar.xz\nsudo mv node-v12.13.0-linux-x64 /usr/local\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/node /usr/local/bin/node\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/npm /usr/local/bin/npm\n\nsudo npm install -g hexo\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/hexo /usr/local/bin/hexo\n```\n\n\n\n# 十、参考\n\n​\thttps://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9\n\n​\thttps://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html","source":"_posts/Hexo-Github博客搭建.md","raw":"---\ntitle: Hexo+Github博客搭建\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-15 03:02:58\npassword:\nsummary:\ntags: \n- Github\n- Hexo\n- node.js\ncategories: 随笔\n---\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=4913023&auto=1&height=66\"></iframe>\n\n# 前言\n\n​\t**准备工作**\n\n+ Github账号\n+ node.js、hexo、npm安装\n\n\n\n# 一、安装node.js\n\n1. 下载windows版node.js\n\n   下载地址: https://nodejs.org/en/download/\n\n   选择Windows Installer(.msi) 64-bit\n\n2. 双击node-v12.13.0-x64.msi, 一直next安装完成\n\n3. 测试是否安装成功\n\n   win+R键，输入cmd,然后回车，打开cmd窗口\n\n   输入node -v \t显示node.js版本\n\n   输入npm -v \t显示npm版本\n\n   安装完成\n\n   \n\n# 二、安装hexo\n\n1. 先创建hexo的安装目录, 例如:  F:\\LearnSpace\\Blog\n\n2. cd Blob  进入Blob目录\n\n3. npm install hexo-cli -g    安装hexo\n\n4. hexo -v  验证是否安装成功\n\n5. npm init blog    初始化blog文件夹，存放博客\n\n6. npm install 安装必备组件\n\n7. cd blog\n\n8. hexo g    生成静态网页\n\n9. hexo s     打开本地服务器\n\n10. http://localhost:4000/    打开网页\n\n11. ctrl + c   关闭本地服务器\n\n    \n\n# 三、连接Github与本地\n\n1. 新建一个名为`你的github用户名.github.io`的仓库，比如说，如果你的`Github`用户名是test，那么你就新建`test.github.io`的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是` http://test.github.io` 了。\n\n   点击`Settings`，向下拉到最后有个`GitHub Pages`，点击`Choose a theme`选择一个主题。然后等一会儿，再回到`GitHub Pages`, 就会像下面一样\n\n   ![](2.png)\n\n2. 修改配置文件\n\n   编辑blog根目录下的`_config.yml`, 修改最后一行的配置\n\n```\ndeploy:\n  type: git\n  repository: https://github.com/981935539/981935539.github.io.git\n  branch: master\n```\n\n3. 安装Git部署插件: `npm install hexo-deployer-git --save`\n\n# 四、编辑第一篇博客\n\n``` \nhexo new post \"first-article\"  # 创建第一篇博客\nhexo g  # 生成静态网页\nhexo s  # 本地预览效果\nhexo d  # 上传github\n```\n\n此时可以在github.io主页就能看到发布的文章啦。\n\n# 五、绑定域名\n\n1. 以阿里云为例，如下图所示，添加两条解析记录:\n\n​\t![](1.png)\n\n2. 然后打开你的Github博客项目，点击`settings`，拉到下面`Custom domain`处，填上你自己的域名，保存\n\n3. 这时候你的`F:\\LearnSpace\\Blog\\blob\\source` 会出现一个CNAME的文件\n\n4. 如果没有CNAME文件\n\n   打开你本地博客`/source`目录，我的是`F:\\LearnSpace\\Blog\\blob\\source`，新建`CNAME`文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行`hexo g`、`hexo d`上传到Github。\n\n   \n\n# 六、hexo常用命令\n\n```\nnpm install hexo-cli -g  \t# 安装hexo\nnpm uninstall hexo-cli -g  \t# 卸载hexo\n\nhexo generate #生成静态页面至public目录\nhexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）\nhexo deploy #部署到GitHub\nhexo help  # 查看帮助\nhexo version  #查看Hexo的版本\n\n# 缩写\nhexo n == hexo new\nhexo g == hexo generate\nhexo s == hexo server\nhexo d == hexo deploy\n\n# 组合\nhexo s -g #生成并本地预览\nhexo d -g #生成并上传\n```\n\n\n\n# 七、写博客的规范\n\n1. _config.yml\n\n   冒号后面必须有一个空格，否则会出问题\n\n2. 图片\n\n   引用图片需要把图片放在对应的文件夹中，只需要写文件名就可以了\n\n3. 文章头设置\n\n   模板在/scaffolds/post.md\n\n   ```\n   --- \n   title: {{ title }} # 文章名称\n   date: {{ date }} # 文章生成时间\n   top: false \n   cover: false \n   password: \n   toc: true \n   mathjax: true \n   summary: \n   tags:\n   -- [tag1]\n   -- [tag2]\n   -- [tag3]\n   categories: \n   -- [cat1]\n   ---\n   ```\n\n   \n\n# 八、备份博客源文件\n\n​\t博客已经搭建完成，但是博客仓库只是保存生成的静态网页文件，是没有博客源文件的，如果电脑出现了问题，那就麻烦了，所以源文件也需要备份一下。\n\n1. 在`Github`上创建一个与本地仓库同名的仓库, 我的是`hexo-matery`\n\n2. 初始化本地仓库\n\n   ```shell\n   git init       \n   添加.gitignore文件\n   .gitignore\n       .DS_Store\n       Thumbs.db\n       *.log\n       public/\n       .deploy*/\n       .vscode/\n   ```\n\n   \n\n3. 连接到远程`Github`,\n\n   ```shell\n   git remote add github git@github.com:981935539/hexo-matery.git\n   git fetch\n   git merge --allow-unrelated-histories github/master\n   ```\n\n4. 推送本地源文件到`Github`\n\n   ```shell\n   git add .\n   git commit -m \"第一次备份本地仓库\"\n   git push --set-upstream github master\n   ```\n\n   \n\n5. 现在在任何一台电脑上, 执行`git clonegit@github.com:981935539/hexo-matery.git`\n\n   就可以把博客源文件复制到本地。\n\n   \n\n# 九、Ubuntu安装node.js和hexo\n\n```shell\ntar -xvf node-v12.13.0-linux-x64.tar.xz\nsudo mv node-v12.13.0-linux-x64 /usr/local\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/node /usr/local/bin/node\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/npm /usr/local/bin/npm\n\nsudo npm install -g hexo\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/hexo /usr/local/bin/hexo\n```\n\n\n\n# 十、参考\n\n​\thttps://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9\n\n​\thttps://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html","slug":"Hexo-Github博客搭建","published":1,"updated":"2019-11-18T09:12:25.933Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454trx0007zsv5yp03gkt9","content":"<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=4913023&auto=1&height=66\"></iframe>\n\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>​    <strong>准备工作</strong></p>\n<ul>\n<li>Github账号</li>\n<li>node.js、hexo、npm安装</li>\n</ul>\n<h1 id=\"一、安装node-js\"><a href=\"#一、安装node-js\" class=\"headerlink\" title=\"一、安装node.js\"></a>一、安装node.js</h1><ol>\n<li><p>下载windows版node.js</p>\n<p>下载地址: <a href=\"https://nodejs.org/en/download/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/download/</a></p>\n<p>选择Windows Installer(.msi) 64-bit</p>\n</li>\n<li><p>双击node-v12.13.0-x64.msi, 一直next安装完成</p>\n</li>\n<li><p>测试是否安装成功</p>\n<p>win+R键，输入cmd,然后回车，打开cmd窗口</p>\n<p>输入node -v     显示node.js版本</p>\n<p>输入npm -v     显示npm版本</p>\n<p>安装完成</p>\n</li>\n</ol>\n<h1 id=\"二、安装hexo\"><a href=\"#二、安装hexo\" class=\"headerlink\" title=\"二、安装hexo\"></a>二、安装hexo</h1><ol>\n<li><p>先创建hexo的安装目录, 例如:  F:\\LearnSpace\\Blog</p>\n</li>\n<li><p>cd Blob  进入Blob目录</p>\n</li>\n<li><p>npm install hexo-cli -g    安装hexo</p>\n</li>\n<li><p>hexo -v  验证是否安装成功</p>\n</li>\n<li><p>npm init blog    初始化blog文件夹，存放博客</p>\n</li>\n<li><p>npm install 安装必备组件</p>\n</li>\n<li><p>cd blog</p>\n</li>\n<li><p>hexo g    生成静态网页</p>\n</li>\n<li><p>hexo s     打开本地服务器</p>\n</li>\n<li><p><a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a>    打开网页</p>\n</li>\n<li><p>ctrl + c   关闭本地服务器</p>\n</li>\n</ol>\n<h1 id=\"三、连接Github与本地\"><a href=\"#三、连接Github与本地\" class=\"headerlink\" title=\"三、连接Github与本地\"></a>三、连接Github与本地</h1><ol>\n<li><p>新建一个名为<code>你的github用户名.github.io</code>的仓库，比如说，如果你的<code>Github</code>用户名是test，那么你就新建<code>test.github.io</code>的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是<code>http://test.github.io</code> 了。</p>\n<p>点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>, 就会像下面一样</p>\n<p><img src=\"2.png\" alt></p>\n</li>\n<li><p>修改配置文件</p>\n<p>编辑blog根目录下的<code>_config.yml</code>, 修改最后一行的配置</p>\n</li>\n</ol>\n<pre><code>deploy:\n  type: git\n  repository: https://github.com/981935539/981935539.github.io.git\n  branch: master</code></pre><ol start=\"3\">\n<li>安装Git部署插件: <code>npm install hexo-deployer-git --save</code></li>\n</ol>\n<h1 id=\"四、编辑第一篇博客\"><a href=\"#四、编辑第一篇博客\" class=\"headerlink\" title=\"四、编辑第一篇博客\"></a>四、编辑第一篇博客</h1><pre><code>hexo new post &quot;first-article&quot;  # 创建第一篇博客\nhexo g  # 生成静态网页\nhexo s  # 本地预览效果\nhexo d  # 上传github</code></pre><p>此时可以在github.io主页就能看到发布的文章啦。</p>\n<h1 id=\"五、绑定域名\"><a href=\"#五、绑定域名\" class=\"headerlink\" title=\"五、绑定域名\"></a>五、绑定域名</h1><ol>\n<li>以阿里云为例，如下图所示，添加两条解析记录:</li>\n</ol>\n<p>​    <img src=\"1.png\" alt></p>\n<ol start=\"2\">\n<li><p>然后打开你的Github博客项目，点击<code>settings</code>，拉到下面<code>Custom domain</code>处，填上你自己的域名，保存</p>\n</li>\n<li><p>这时候你的<code>F:\\LearnSpace\\Blog\\blob\\source</code> 会出现一个CNAME的文件</p>\n</li>\n<li><p>如果没有CNAME文件</p>\n<p>打开你本地博客<code>/source</code>目录，我的是<code>F:\\LearnSpace\\Blog\\blob\\source</code>，新建<code>CNAME</code>文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行<code>hexo g</code>、<code>hexo d</code>上传到Github。</p>\n</li>\n</ol>\n<h1 id=\"六、hexo常用命令\"><a href=\"#六、hexo常用命令\" class=\"headerlink\" title=\"六、hexo常用命令\"></a>六、hexo常用命令</h1><pre><code>npm install hexo-cli -g      # 安装hexo\nnpm uninstall hexo-cli -g      # 卸载hexo\n\nhexo generate #生成静态页面至public目录\nhexo server #开启预览访问端口（默认端口4000，&#39;ctrl + c&#39;关闭server）\nhexo deploy #部署到GitHub\nhexo help  # 查看帮助\nhexo version  #查看Hexo的版本\n\n# 缩写\nhexo n == hexo new\nhexo g == hexo generate\nhexo s == hexo server\nhexo d == hexo deploy\n\n# 组合\nhexo s -g #生成并本地预览\nhexo d -g #生成并上传</code></pre><h1 id=\"七、写博客的规范\"><a href=\"#七、写博客的规范\" class=\"headerlink\" title=\"七、写博客的规范\"></a>七、写博客的规范</h1><ol>\n<li><p>_config.yml</p>\n<p>冒号后面必须有一个空格，否则会出问题</p>\n</li>\n<li><p>图片</p>\n<p>引用图片需要把图片放在对应的文件夹中，只需要写文件名就可以了</p>\n</li>\n<li><p>文章头设置</p>\n<p>模板在/scaffolds/post.md</p>\n<pre><code>--- \ntitle: {{ title }} # 文章名称\ndate: {{ date }} # 文章生成时间\ntop: false \ncover: false \npassword: \ntoc: true \nmathjax: true \nsummary: \ntags:\n-- [tag1]\n-- [tag2]\n-- [tag3]\ncategories: \n-- [cat1]\n---</code></pre></li>\n</ol>\n<h1 id=\"八、备份博客源文件\"><a href=\"#八、备份博客源文件\" class=\"headerlink\" title=\"八、备份博客源文件\"></a>八、备份博客源文件</h1><p>​    博客已经搭建完成，但是博客仓库只是保存生成的静态网页文件，是没有博客源文件的，如果电脑出现了问题，那就麻烦了，所以源文件也需要备份一下。</p>\n<ol>\n<li><p>在<code>Github</code>上创建一个与本地仓库同名的仓库, 我的是<code>hexo-matery</code></p>\n</li>\n<li><p>初始化本地仓库</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">git init       \n添加.gitignore文件\n.gitignore\n    .DS_Store\n    Thumbs.db\n    *.log\n    public/\n    .deploy*/\n    .vscode/<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>连接到远程<code>Github</code>,</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">git remote add github git@github.com:981935539/hexo-matery.git\ngit fetch\ngit merge --allow-unrelated-histories github/master<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n</li>\n<li><p>推送本地源文件到<code>Github</code></p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">git add .\ngit commit -m \"第一次备份本地仓库\"\ngit push --set-upstream github master<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<ol start=\"5\">\n<li><p>现在在任何一台电脑上, 执行<code>git clonegit@github.com:981935539/hexo-matery.git</code></p>\n<p>就可以把博客源文件复制到本地。</p>\n</li>\n</ol>\n<h1 id=\"九、Ubuntu安装node-js和hexo\"><a href=\"#九、Ubuntu安装node-js和hexo\" class=\"headerlink\" title=\"九、Ubuntu安装node.js和hexo\"></a>九、Ubuntu安装node.js和hexo</h1><pre class=\"line-numbers language-shell\"><code class=\"language-shell\">tar -xvf node-v12.13.0-linux-x64.tar.xz\nsudo mv node-v12.13.0-linux-x64 /usr/local\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/node /usr/local/bin/node\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/npm /usr/local/bin/npm\n\nsudo npm install -g hexo\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/hexo /usr/local/bin/hexo<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"十、参考\"><a href=\"#十、参考\" class=\"headerlink\" title=\"十、参考\"></a>十、参考</h1><p>​    <a href=\"https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9\" target=\"_blank\" rel=\"noopener\">https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9</a></p>\n<p>​    <a href=\"https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=4913023&auto=1&height=66\"></iframe>\n\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>​    <strong>准备工作</strong></p>\n<ul>\n<li>Github账号</li>\n<li>node.js、hexo、npm安装</li>\n</ul>\n<h1 id=\"一、安装node-js\"><a href=\"#一、安装node-js\" class=\"headerlink\" title=\"一、安装node.js\"></a>一、安装node.js</h1><ol>\n<li><p>下载windows版node.js</p>\n<p>下载地址: <a href=\"https://nodejs.org/en/download/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/download/</a></p>\n<p>选择Windows Installer(.msi) 64-bit</p>\n</li>\n<li><p>双击node-v12.13.0-x64.msi, 一直next安装完成</p>\n</li>\n<li><p>测试是否安装成功</p>\n<p>win+R键，输入cmd,然后回车，打开cmd窗口</p>\n<p>输入node -v     显示node.js版本</p>\n<p>输入npm -v     显示npm版本</p>\n<p>安装完成</p>\n</li>\n</ol>\n<h1 id=\"二、安装hexo\"><a href=\"#二、安装hexo\" class=\"headerlink\" title=\"二、安装hexo\"></a>二、安装hexo</h1><ol>\n<li><p>先创建hexo的安装目录, 例如:  F:\\LearnSpace\\Blog</p>\n</li>\n<li><p>cd Blob  进入Blob目录</p>\n</li>\n<li><p>npm install hexo-cli -g    安装hexo</p>\n</li>\n<li><p>hexo -v  验证是否安装成功</p>\n</li>\n<li><p>npm init blog    初始化blog文件夹，存放博客</p>\n</li>\n<li><p>npm install 安装必备组件</p>\n</li>\n<li><p>cd blog</p>\n</li>\n<li><p>hexo g    生成静态网页</p>\n</li>\n<li><p>hexo s     打开本地服务器</p>\n</li>\n<li><p><a href=\"http://localhost:4000/\" target=\"_blank\" rel=\"noopener\">http://localhost:4000/</a>    打开网页</p>\n</li>\n<li><p>ctrl + c   关闭本地服务器</p>\n</li>\n</ol>\n<h1 id=\"三、连接Github与本地\"><a href=\"#三、连接Github与本地\" class=\"headerlink\" title=\"三、连接Github与本地\"></a>三、连接Github与本地</h1><ol>\n<li><p>新建一个名为<code>你的github用户名.github.io</code>的仓库，比如说，如果你的<code>Github</code>用户名是test，那么你就新建<code>test.github.io</code>的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是<code>http://test.github.io</code> 了。</p>\n<p>点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>, 就会像下面一样</p>\n<p><img src=\"2.png\" alt></p>\n</li>\n<li><p>修改配置文件</p>\n<p>编辑blog根目录下的<code>_config.yml</code>, 修改最后一行的配置</p>\n</li>\n</ol>\n<pre><code>deploy:\n  type: git\n  repository: https://github.com/981935539/981935539.github.io.git\n  branch: master</code></pre><ol start=\"3\">\n<li>安装Git部署插件: <code>npm install hexo-deployer-git --save</code></li>\n</ol>\n<h1 id=\"四、编辑第一篇博客\"><a href=\"#四、编辑第一篇博客\" class=\"headerlink\" title=\"四、编辑第一篇博客\"></a>四、编辑第一篇博客</h1><pre><code>hexo new post &quot;first-article&quot;  # 创建第一篇博客\nhexo g  # 生成静态网页\nhexo s  # 本地预览效果\nhexo d  # 上传github</code></pre><p>此时可以在github.io主页就能看到发布的文章啦。</p>\n<h1 id=\"五、绑定域名\"><a href=\"#五、绑定域名\" class=\"headerlink\" title=\"五、绑定域名\"></a>五、绑定域名</h1><ol>\n<li>以阿里云为例，如下图所示，添加两条解析记录:</li>\n</ol>\n<p>​    <img src=\"1.png\" alt></p>\n<ol start=\"2\">\n<li><p>然后打开你的Github博客项目，点击<code>settings</code>，拉到下面<code>Custom domain</code>处，填上你自己的域名，保存</p>\n</li>\n<li><p>这时候你的<code>F:\\LearnSpace\\Blog\\blob\\source</code> 会出现一个CNAME的文件</p>\n</li>\n<li><p>如果没有CNAME文件</p>\n<p>打开你本地博客<code>/source</code>目录，我的是<code>F:\\LearnSpace\\Blog\\blob\\source</code>，新建<code>CNAME</code>文件，注意没有后缀。然后在里面写上你的域名，保存。最后运行<code>hexo g</code>、<code>hexo d</code>上传到Github。</p>\n</li>\n</ol>\n<h1 id=\"六、hexo常用命令\"><a href=\"#六、hexo常用命令\" class=\"headerlink\" title=\"六、hexo常用命令\"></a>六、hexo常用命令</h1><pre><code>npm install hexo-cli -g      # 安装hexo\nnpm uninstall hexo-cli -g      # 卸载hexo\n\nhexo generate #生成静态页面至public目录\nhexo server #开启预览访问端口（默认端口4000，&#39;ctrl + c&#39;关闭server）\nhexo deploy #部署到GitHub\nhexo help  # 查看帮助\nhexo version  #查看Hexo的版本\n\n# 缩写\nhexo n == hexo new\nhexo g == hexo generate\nhexo s == hexo server\nhexo d == hexo deploy\n\n# 组合\nhexo s -g #生成并本地预览\nhexo d -g #生成并上传</code></pre><h1 id=\"七、写博客的规范\"><a href=\"#七、写博客的规范\" class=\"headerlink\" title=\"七、写博客的规范\"></a>七、写博客的规范</h1><ol>\n<li><p>_config.yml</p>\n<p>冒号后面必须有一个空格，否则会出问题</p>\n</li>\n<li><p>图片</p>\n<p>引用图片需要把图片放在对应的文件夹中，只需要写文件名就可以了</p>\n</li>\n<li><p>文章头设置</p>\n<p>模板在/scaffolds/post.md</p>\n<pre><code>--- \ntitle: {{ title }} # 文章名称\ndate: {{ date }} # 文章生成时间\ntop: false \ncover: false \npassword: \ntoc: true \nmathjax: true \nsummary: \ntags:\n-- [tag1]\n-- [tag2]\n-- [tag3]\ncategories: \n-- [cat1]\n---</code></pre></li>\n</ol>\n<h1 id=\"八、备份博客源文件\"><a href=\"#八、备份博客源文件\" class=\"headerlink\" title=\"八、备份博客源文件\"></a>八、备份博客源文件</h1><p>​    博客已经搭建完成，但是博客仓库只是保存生成的静态网页文件，是没有博客源文件的，如果电脑出现了问题，那就麻烦了，所以源文件也需要备份一下。</p>\n<ol>\n<li><p>在<code>Github</code>上创建一个与本地仓库同名的仓库, 我的是<code>hexo-matery</code></p>\n</li>\n<li><p>初始化本地仓库</p>\n<pre><code class=\"shell\">git init       \n添加.gitignore文件\n.gitignore\n    .DS_Store\n    Thumbs.db\n    *.log\n    public/\n    .deploy*/\n    .vscode/</code></pre>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>连接到远程<code>Github</code>,</p>\n<pre><code class=\"shell\">git remote add github git@github.com:981935539/hexo-matery.git\ngit fetch\ngit merge --allow-unrelated-histories github/master</code></pre>\n</li>\n<li><p>推送本地源文件到<code>Github</code></p>\n<pre><code class=\"shell\">git add .\ngit commit -m &quot;第一次备份本地仓库&quot;\ngit push --set-upstream github master</code></pre>\n</li>\n</ol>\n<ol start=\"5\">\n<li><p>现在在任何一台电脑上, 执行<code>git clonegit@github.com:981935539/hexo-matery.git</code></p>\n<p>就可以把博客源文件复制到本地。</p>\n</li>\n</ol>\n<h1 id=\"九、Ubuntu安装node-js和hexo\"><a href=\"#九、Ubuntu安装node-js和hexo\" class=\"headerlink\" title=\"九、Ubuntu安装node.js和hexo\"></a>九、Ubuntu安装node.js和hexo</h1><pre><code class=\"shell\">tar -xvf node-v12.13.0-linux-x64.tar.xz\nsudo mv node-v12.13.0-linux-x64 /usr/local\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/node /usr/local/bin/node\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/npm /usr/local/bin/npm\n\nsudo npm install -g hexo\nsudo ln -s /usr/local/node-v12.13.0-linux-x64/bin/hexo /usr/local/bin/hexo</code></pre>\n<h1 id=\"十、参考\"><a href=\"#十、参考\" class=\"headerlink\" title=\"十、参考\"></a>十、参考</h1><p>​    <a href=\"https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9\" target=\"_blank\" rel=\"noopener\">https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-9</a></p>\n<p>​    <a href=\"https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p>\n"},{"title":"Python中with的用法","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-27T16:14:20.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、什么是with语句\n\n​\t对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。\n\n```python\n# 操作文件的方式\n\n# 普通方式操作文件\nf = open('test.txt', 'w')\nf.write('test')\nf.close()\n# 普通方式操作文件的问题\n# 1、忘记关闭文件\n# 2、程序执行的过程中发生了异常导致关闭文件的代码没有被执行\n\n# 可以使用try...finally的方式解决上述问题\ntry:\n    f = open('test.txt', 'w')\n\tf.write('test')\nfinally:\n\tf.close()\n    \n# Python提供了一种更简单的解决方案:with语句\n# 不论执行过程中是否发生异常，文件都会关闭\nwith open('test.txt', 'w') as f:\n    f.write('test')\n\n```\n\n**with 语句的语法格式**:\n\n```shell\nwith 表达式 [as目标]:\t\n\t代码块\t\t\n```\n\n​\t\t\t\t\t\t\t\t\n\n#  二、with语句的原理\n\n1. 上下文管理器\n\n   上下文管理器是一个实现了上下文协议的对象，即在对象中定义了`__enter__`和`__exit__`方法。上下文管理器定义运行时需要建立的上下文，处理程序的进入和退出。\n\n   ```python\n   # open函数返回的就是一个上下文管理器对象\n   f.__enter__   # <function TextIOWrapper.__enter__>\n   f.__exit__    # <function TextIOWrapper.__exit__>\n   ```\n\n   `__enter__(self)`:该方法只接收一个self参数。当对象返回时该方法立即执行，并返回当前对象或者与运行时上下文相关的其他对象。如果有as变量（as子句是可选项），返回值将被赋给as后面的变量上。\n\n   `__exit__(self, exception_type, exception_value, traceback)`:退出运行时上下文，并返回一个布尔值标示是否有需要处理的异常。如果在执行with语句体时发生异常，那退出时参数会包括异常类型、异常值、异常追踪信息，否则，3个参数都是None。返回True异常被处理，返回其他任何值，异常都会抛出。\n\n   \n\n2. 自定义上下文管理器\n\n   任何实现了上下文管理器协议的对象都可以称为一个上下文管理器。\n\n   ```python\n   # 实现一个简单的open()上下文管理\n   class myopen:\n       def __init__(self, name, mode='r'):\n           self.file = open(name, mode)          \n       def __enter__(self):\n           return self.file\n       def __exit__(self, exception_type, exception_value, traceback):\n           self.file.close()\n           return False\n               \n   with myopen('test.txt') as f:\n        print(f.read())  # test\n   f.closed  # True\n   \n   # 同时打开多个文件\n   with myopen('test1.txt', 'w') as f1, myopen('test2.txt', 'w') as f2, myopen('test3.txt', 'w') as f3:\n       f1.write('test1')\n       f2.write('test2')\n       f3.write('test3')\n   ```\n\n\n\n# 三、contextlib\n\n​\t为了更好的辅助上下文管理，python提供了`contextlib`模块，该模块通过`Generator`实现，其中的`contextmanager`作为装饰器来提供了一种针对函数级别的上下文管理机制，可以直接使用与函数/对象而不用关心`__enter__`和`__exit__`方法的具体实现。\n\n1. `contextmanager`\n\n   ```python\n   from contextlib import contextmanager\n   \n   @contextmanager\n   def myopen(name, model='r'):\n       try:\n           f = open(name, model)\n           yield f\n       finally:\n           f.close()\n           \n   with myopen('test.txt') as f:\n        print(f.read())  # test\n   f.closed  # True\n   ```\n\n\n\n2. `closing`\n\n   文件类是支持上下文管理协议的，可以直接用with语句，还有一些对象并不支持该协议，但使用的时候又要确保正常退出，这时就可以使用closing创建上下文管理器。\n\n```python\nfrom urllib import request\nfrom contextlib import closing, contextmanager\nwith closing(request.urlopen('https://www.baidu.com/')) as f:\n\tdata = f.read()\n\tprint('Status:', f.status, f.reason)\n\tprint('Data:', data)\n\n# 等价于:\n@contextmanager\ndef closing(f):\n\ttry:\n\t\tyield f\n\tfinally:\n\t\tf.close()\n```\n\n   ","source":"_posts/Python中with的用法.md","raw":"---\ntitle: Python中with的用法\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-28 00:14:20\npassword:\nsummary:\ntags: \n- python\ncategories: Python\n---\n\n\n\n# 一、什么是with语句\n\n​\t对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。\n\n```python\n# 操作文件的方式\n\n# 普通方式操作文件\nf = open('test.txt', 'w')\nf.write('test')\nf.close()\n# 普通方式操作文件的问题\n# 1、忘记关闭文件\n# 2、程序执行的过程中发生了异常导致关闭文件的代码没有被执行\n\n# 可以使用try...finally的方式解决上述问题\ntry:\n    f = open('test.txt', 'w')\n\tf.write('test')\nfinally:\n\tf.close()\n    \n# Python提供了一种更简单的解决方案:with语句\n# 不论执行过程中是否发生异常，文件都会关闭\nwith open('test.txt', 'w') as f:\n    f.write('test')\n\n```\n\n**with 语句的语法格式**:\n\n```shell\nwith 表达式 [as目标]:\t\n\t代码块\t\t\n```\n\n​\t\t\t\t\t\t\t\t\n\n#  二、with语句的原理\n\n1. 上下文管理器\n\n   上下文管理器是一个实现了上下文协议的对象，即在对象中定义了`__enter__`和`__exit__`方法。上下文管理器定义运行时需要建立的上下文，处理程序的进入和退出。\n\n   ```python\n   # open函数返回的就是一个上下文管理器对象\n   f.__enter__   # <function TextIOWrapper.__enter__>\n   f.__exit__    # <function TextIOWrapper.__exit__>\n   ```\n\n   `__enter__(self)`:该方法只接收一个self参数。当对象返回时该方法立即执行，并返回当前对象或者与运行时上下文相关的其他对象。如果有as变量（as子句是可选项），返回值将被赋给as后面的变量上。\n\n   `__exit__(self, exception_type, exception_value, traceback)`:退出运行时上下文，并返回一个布尔值标示是否有需要处理的异常。如果在执行with语句体时发生异常，那退出时参数会包括异常类型、异常值、异常追踪信息，否则，3个参数都是None。返回True异常被处理，返回其他任何值，异常都会抛出。\n\n   \n\n2. 自定义上下文管理器\n\n   任何实现了上下文管理器协议的对象都可以称为一个上下文管理器。\n\n   ```python\n   # 实现一个简单的open()上下文管理\n   class myopen:\n       def __init__(self, name, mode='r'):\n           self.file = open(name, mode)          \n       def __enter__(self):\n           return self.file\n       def __exit__(self, exception_type, exception_value, traceback):\n           self.file.close()\n           return False\n               \n   with myopen('test.txt') as f:\n        print(f.read())  # test\n   f.closed  # True\n   \n   # 同时打开多个文件\n   with myopen('test1.txt', 'w') as f1, myopen('test2.txt', 'w') as f2, myopen('test3.txt', 'w') as f3:\n       f1.write('test1')\n       f2.write('test2')\n       f3.write('test3')\n   ```\n\n\n\n# 三、contextlib\n\n​\t为了更好的辅助上下文管理，python提供了`contextlib`模块，该模块通过`Generator`实现，其中的`contextmanager`作为装饰器来提供了一种针对函数级别的上下文管理机制，可以直接使用与函数/对象而不用关心`__enter__`和`__exit__`方法的具体实现。\n\n1. `contextmanager`\n\n   ```python\n   from contextlib import contextmanager\n   \n   @contextmanager\n   def myopen(name, model='r'):\n       try:\n           f = open(name, model)\n           yield f\n       finally:\n           f.close()\n           \n   with myopen('test.txt') as f:\n        print(f.read())  # test\n   f.closed  # True\n   ```\n\n\n\n2. `closing`\n\n   文件类是支持上下文管理协议的，可以直接用with语句，还有一些对象并不支持该协议，但使用的时候又要确保正常退出，这时就可以使用closing创建上下文管理器。\n\n```python\nfrom urllib import request\nfrom contextlib import closing, contextmanager\nwith closing(request.urlopen('https://www.baidu.com/')) as f:\n\tdata = f.read()\n\tprint('Status:', f.status, f.reason)\n\tprint('Data:', data)\n\n# 等价于:\n@contextmanager\ndef closing(f):\n\ttry:\n\t\tyield f\n\tfinally:\n\t\tf.close()\n```\n\n   ","slug":"Python中with的用法","published":1,"updated":"2019-12-02T08:53:06.518Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454ts20009zsv5criei4mw","content":"<h1 id=\"一、什么是with语句\"><a href=\"#一、什么是with语句\" class=\"headerlink\" title=\"一、什么是with语句\"></a>一、什么是with语句</h1><p>​    对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 操作文件的方式</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 普通方式操作文件</span>\nf <span class=\"token operator\">=</span> open<span class=\"token punctuation\">(</span><span class=\"token string\">'test.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span>\nf<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">)</span>\nf<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># 普通方式操作文件的问题</span>\n<span class=\"token comment\" spellcheck=\"true\"># 1、忘记关闭文件</span>\n<span class=\"token comment\" spellcheck=\"true\"># 2、程序执行的过程中发生了异常导致关闭文件的代码没有被执行</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 可以使用try...finally的方式解决上述问题</span>\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    f <span class=\"token operator\">=</span> open<span class=\"token punctuation\">(</span><span class=\"token string\">'test.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span>\n    f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">finally</span><span class=\"token punctuation\">:</span>\n    f<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># Python提供了一种更简单的解决方案:with语句</span>\n<span class=\"token comment\" spellcheck=\"true\"># 不论执行过程中是否发生异常，文件都会关闭</span>\n<span class=\"token keyword\">with</span> open<span class=\"token punctuation\">(</span><span class=\"token string\">'test.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    f<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">)</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>with 语句的语法格式</strong>:</p>\n<pre class=\"line-numbers language-shell\"><code class=\"language-shell\">with 表达式 [as目标]:    \n    代码块        <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p>​                                </p>\n<h1 id=\"二、with语句的原理\"><a href=\"#二、with语句的原理\" class=\"headerlink\" title=\"二、with语句的原理\"></a>二、with语句的原理</h1><ol>\n<li><p>上下文管理器</p>\n<p>上下文管理器是一个实现了上下文协议的对象，即在对象中定义了<code>__enter__</code>和<code>__exit__</code>方法。上下文管理器定义运行时需要建立的上下文，处理程序的进入和退出。</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># open函数返回的就是一个上下文管理器对象</span>\nf<span class=\"token punctuation\">.</span>__enter__   <span class=\"token comment\" spellcheck=\"true\"># &lt;function TextIOWrapper.__enter__></span>\nf<span class=\"token punctuation\">.</span>__exit__    <span class=\"token comment\" spellcheck=\"true\"># &lt;function TextIOWrapper.__exit__></span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p><code>__enter__(self)</code>:该方法只接收一个self参数。当对象返回时该方法立即执行，并返回当前对象或者与运行时上下文相关的其他对象。如果有as变量（as子句是可选项），返回值将被赋给as后面的变量上。</p>\n<p><code>__exit__(self, exception_type, exception_value, traceback)</code>:退出运行时上下文，并返回一个布尔值标示是否有需要处理的异常。如果在执行with语句体时发生异常，那退出时参数会包括异常类型、异常值、异常追踪信息，否则，3个参数都是None。返回True异常被处理，返回其他任何值，异常都会抛出。</p>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>自定义上下文管理器</p>\n<p>任何实现了上下文管理器协议的对象都可以称为一个上下文管理器。</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># 实现一个简单的open()上下文管理</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">myopen</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">,</span> mode<span class=\"token operator\">=</span><span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>file <span class=\"token operator\">=</span> open<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> mode<span class=\"token punctuation\">)</span>          \n    <span class=\"token keyword\">def</span> <span class=\"token function\">__enter__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>file\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__exit__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> exception_type<span class=\"token punctuation\">,</span> exception_value<span class=\"token punctuation\">,</span> traceback<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>file<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> <span class=\"token boolean\">False</span>\n\n<span class=\"token keyword\">with</span> myopen<span class=\"token punctuation\">(</span><span class=\"token string\">'test.txt'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n     <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># test</span>\nf<span class=\"token punctuation\">.</span>closed  <span class=\"token comment\" spellcheck=\"true\"># True</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 同时打开多个文件</span>\n<span class=\"token keyword\">with</span> myopen<span class=\"token punctuation\">(</span><span class=\"token string\">'test1.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f1<span class=\"token punctuation\">,</span> myopen<span class=\"token punctuation\">(</span><span class=\"token string\">'test2.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f2<span class=\"token punctuation\">,</span> myopen<span class=\"token punctuation\">(</span><span class=\"token string\">'test3.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f3<span class=\"token punctuation\">:</span>\n    f1<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test1'</span><span class=\"token punctuation\">)</span>\n    f2<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test2'</span><span class=\"token punctuation\">)</span>\n    f3<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'test3'</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<h1 id=\"三、contextlib\"><a href=\"#三、contextlib\" class=\"headerlink\" title=\"三、contextlib\"></a>三、contextlib</h1><p>​    为了更好的辅助上下文管理，python提供了<code>contextlib</code>模块，该模块通过<code>Generator</code>实现，其中的<code>contextmanager</code>作为装饰器来提供了一种针对函数级别的上下文管理机制，可以直接使用与函数/对象而不用关心<code>__enter__</code>和<code>__exit__</code>方法的具体实现。</p>\n<ol>\n<li><p><code>contextmanager</code></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> contextlib <span class=\"token keyword\">import</span> contextmanager\n\n@contextmanager\n<span class=\"token keyword\">def</span> <span class=\"token function\">myopen</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> model<span class=\"token operator\">=</span><span class=\"token string\">'r'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n        f <span class=\"token operator\">=</span> open<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">yield</span> f\n    <span class=\"token keyword\">finally</span><span class=\"token punctuation\">:</span>\n        f<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> myopen<span class=\"token punctuation\">(</span><span class=\"token string\">'test.txt'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n     <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># test</span>\nf<span class=\"token punctuation\">.</span>closed  <span class=\"token comment\" spellcheck=\"true\"># True</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p><code>closing</code></p>\n<p>文件类是支持上下文管理协议的，可以直接用with语句，还有一些对象并不支持该协议，但使用的时候又要确保正常退出，这时就可以使用closing创建上下文管理器。</p>\n</li>\n</ol>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> urllib <span class=\"token keyword\">import</span> request\n<span class=\"token keyword\">from</span> contextlib <span class=\"token keyword\">import</span> closing<span class=\"token punctuation\">,</span> contextmanager\n<span class=\"token keyword\">with</span> closing<span class=\"token punctuation\">(</span>request<span class=\"token punctuation\">.</span>urlopen<span class=\"token punctuation\">(</span><span class=\"token string\">'https://www.baidu.com/'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    data <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Status:'</span><span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">.</span>status<span class=\"token punctuation\">,</span> f<span class=\"token punctuation\">.</span>reason<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Data:'</span><span class=\"token punctuation\">,</span> data<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 等价于:</span>\n@contextmanager\n<span class=\"token keyword\">def</span> <span class=\"token function\">closing</span><span class=\"token punctuation\">(</span>f<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">yield</span> f\n    <span class=\"token keyword\">finally</span><span class=\"token punctuation\">:</span>\n        f<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、什么是with语句\"><a href=\"#一、什么是with语句\" class=\"headerlink\" title=\"一、什么是with语句\"></a>一、什么是with语句</h1><p>​    对于系统资源如文件、数据库连接、socket 而言，应用程序打开这些资源并执行完业务逻辑之后，必须做的一件事就是要关闭（断开）该资源。</p>\n<pre><code class=\"python\"># 操作文件的方式\n\n# 普通方式操作文件\nf = open(&#39;test.txt&#39;, &#39;w&#39;)\nf.write(&#39;test&#39;)\nf.close()\n# 普通方式操作文件的问题\n# 1、忘记关闭文件\n# 2、程序执行的过程中发生了异常导致关闭文件的代码没有被执行\n\n# 可以使用try...finally的方式解决上述问题\ntry:\n    f = open(&#39;test.txt&#39;, &#39;w&#39;)\n    f.write(&#39;test&#39;)\nfinally:\n    f.close()\n\n# Python提供了一种更简单的解决方案:with语句\n# 不论执行过程中是否发生异常，文件都会关闭\nwith open(&#39;test.txt&#39;, &#39;w&#39;) as f:\n    f.write(&#39;test&#39;)\n</code></pre>\n<p><strong>with 语句的语法格式</strong>:</p>\n<pre><code class=\"shell\">with 表达式 [as目标]:    \n    代码块        </code></pre>\n<p>​                                </p>\n<h1 id=\"二、with语句的原理\"><a href=\"#二、with语句的原理\" class=\"headerlink\" title=\"二、with语句的原理\"></a>二、with语句的原理</h1><ol>\n<li><p>上下文管理器</p>\n<p>上下文管理器是一个实现了上下文协议的对象，即在对象中定义了<code>__enter__</code>和<code>__exit__</code>方法。上下文管理器定义运行时需要建立的上下文，处理程序的进入和退出。</p>\n<pre><code class=\"python\"># open函数返回的就是一个上下文管理器对象\nf.__enter__   # &lt;function TextIOWrapper.__enter__&gt;\nf.__exit__    # &lt;function TextIOWrapper.__exit__&gt;</code></pre>\n<p><code>__enter__(self)</code>:该方法只接收一个self参数。当对象返回时该方法立即执行，并返回当前对象或者与运行时上下文相关的其他对象。如果有as变量（as子句是可选项），返回值将被赋给as后面的变量上。</p>\n<p><code>__exit__(self, exception_type, exception_value, traceback)</code>:退出运行时上下文，并返回一个布尔值标示是否有需要处理的异常。如果在执行with语句体时发生异常，那退出时参数会包括异常类型、异常值、异常追踪信息，否则，3个参数都是None。返回True异常被处理，返回其他任何值，异常都会抛出。</p>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>自定义上下文管理器</p>\n<p>任何实现了上下文管理器协议的对象都可以称为一个上下文管理器。</p>\n<pre><code class=\"python\"># 实现一个简单的open()上下文管理\nclass myopen:\n    def __init__(self, name, mode=&#39;r&#39;):\n        self.file = open(name, mode)          \n    def __enter__(self):\n        return self.file\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.file.close()\n        return False\n\nwith myopen(&#39;test.txt&#39;) as f:\n     print(f.read())  # test\nf.closed  # True\n\n# 同时打开多个文件\nwith myopen(&#39;test1.txt&#39;, &#39;w&#39;) as f1, myopen(&#39;test2.txt&#39;, &#39;w&#39;) as f2, myopen(&#39;test3.txt&#39;, &#39;w&#39;) as f3:\n    f1.write(&#39;test1&#39;)\n    f2.write(&#39;test2&#39;)\n    f3.write(&#39;test3&#39;)</code></pre>\n</li>\n</ol>\n<h1 id=\"三、contextlib\"><a href=\"#三、contextlib\" class=\"headerlink\" title=\"三、contextlib\"></a>三、contextlib</h1><p>​    为了更好的辅助上下文管理，python提供了<code>contextlib</code>模块，该模块通过<code>Generator</code>实现，其中的<code>contextmanager</code>作为装饰器来提供了一种针对函数级别的上下文管理机制，可以直接使用与函数/对象而不用关心<code>__enter__</code>和<code>__exit__</code>方法的具体实现。</p>\n<ol>\n<li><p><code>contextmanager</code></p>\n<pre><code class=\"python\">from contextlib import contextmanager\n\n@contextmanager\ndef myopen(name, model=&#39;r&#39;):\n    try:\n        f = open(name, model)\n        yield f\n    finally:\n        f.close()\n\nwith myopen(&#39;test.txt&#39;) as f:\n     print(f.read())  # test\nf.closed  # True</code></pre>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p><code>closing</code></p>\n<p>文件类是支持上下文管理协议的，可以直接用with语句，还有一些对象并不支持该协议，但使用的时候又要确保正常退出，这时就可以使用closing创建上下文管理器。</p>\n</li>\n</ol>\n<pre><code class=\"python\">from urllib import request\nfrom contextlib import closing, contextmanager\nwith closing(request.urlopen(&#39;https://www.baidu.com/&#39;)) as f:\n    data = f.read()\n    print(&#39;Status:&#39;, f.status, f.reason)\n    print(&#39;Data:&#39;, data)\n\n# 等价于:\n@contextmanager\ndef closing(f):\n    try:\n        yield f\n    finally:\n        f.close()</code></pre>\n"},{"title":"Python包管理","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-27T16:08:24.000Z","password":null,"summary":null,"_content":"\n\n\n\n# 一、什么是包？\n\n简单来说包即是目录，但和普通目录不同，它除了包含python文件以外，还包含一个`__init__.py`文件，同时它允许嵌套。\n\n包结构如下：\n\n```python\npackage/__init__.py\n\t\tmodule1.py\n    \t\tclass C1:pass\n    \tmodule2.py\n        \tclass C2:pass\n        subpackage/__init__.py\n        \t\t   module1.py        \t\t\t\n               \t   module2.py              \t\n                   module3.py\n\nmain.py\n                    \nimport package\nimport package.module1\nimport package.subpackage\nimport package.subpackage.module1\n\nfrom package import module1\nfrom package import subpackage\nfrom package.subpackage import module1\n# from package import module3\n```\n\n\n\n# 二、`__init__.py`的作用\n\n1. 区别包和普通目录\n\n2. 可以使模块中的对象变成包可见\n\n   例如：要导入package包下module1中的类Test, 当`__init__.py`文件为空的时候需要使用完整的导入路径:`from package.module import Test`, 但如果在`__init__.py`中添加`from module1 import Test`语句，就可以直接使用`from package import Test`来导入类Test。\n\n3. 通过在该文件中定义`__all__`变量，控制需要导入的子包或模块。\n\n   \n\n\n# 三、`__all__`的作用\n\n​\t`__all__`只控制`from xxx import *`的行为, 不影响`import` 和 `from xxx import xxxx`的行为\n\n1. 在`__init__.py`文件中添加：\n\n   ​\t\t\t\t\t`__all__ = ['module1', 'subpackage']`\n\n   `__init__.py`不使用`__all__`属性，不能通过`from package import * `导入\n\n   `__init__.py`使用`__all__`属性，`from package import *`只能导入`__all__`列表中的成员，但可以通过\n\n   `import package.module2`和`from package import module2`导入\n\n2. 在普通`*.py`文件中添加：`__all__`\n\n   ```python\n   # from xxx import * 这种方式只能导入公有的属性，方法或类【无法导入以单下划线开头（protected）或以双下划线开头(private)的属性，方法或类】\n   \n   # from xxx import aa, bb 可以导入public,protected,private\n   # import xxx   xxx.__func  可以访问public,protected,private\n   ```\n\n   模块中不使用`__all__`属性，可以导入模块内的所有公有属性，函数和类 。\n\n   模块中使用`__all__`属性，只能导入`__all__`中定义的属性，函数和类(包括私有属性和保护属性)。\n\n\n\n# 四、`from ... import ...`的问题\n\n1. 命名空间的冲突\n\n   ```python\n   # module1.py\n   def add()\n   \tprint(\"add in module1\")\n   # module1.py\n   def add()\n   \tprint(\"add in module2\")\n       \n   # main.py\n   from package.module1 import add\n   from package.module2 import add\n   \n   # 最近导入的add,会覆盖先导入的add\n   ```\n\n   \n\n2. 循环嵌套导入的问题\n\n   ```python\n   # module1.py\n   from module2 import g\n   def x()\n   \tpass\n   # module2.py\n   from module1 import x\n   def g()\n   \tpass\n   \n   # 会抛出一个ImportError: cannot import name 'g'异常，解决方法直接使用import 语句\n   ```\n\n\n\n","source":"_posts/Python包管理.md","raw":"---\ntitle: Python包管理\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-28 00:08:24\npassword:\nsummary:\ntags: \n- python\ncategories: Python\n---\n\n\n\n\n# 一、什么是包？\n\n简单来说包即是目录，但和普通目录不同，它除了包含python文件以外，还包含一个`__init__.py`文件，同时它允许嵌套。\n\n包结构如下：\n\n```python\npackage/__init__.py\n\t\tmodule1.py\n    \t\tclass C1:pass\n    \tmodule2.py\n        \tclass C2:pass\n        subpackage/__init__.py\n        \t\t   module1.py        \t\t\t\n               \t   module2.py              \t\n                   module3.py\n\nmain.py\n                    \nimport package\nimport package.module1\nimport package.subpackage\nimport package.subpackage.module1\n\nfrom package import module1\nfrom package import subpackage\nfrom package.subpackage import module1\n# from package import module3\n```\n\n\n\n# 二、`__init__.py`的作用\n\n1. 区别包和普通目录\n\n2. 可以使模块中的对象变成包可见\n\n   例如：要导入package包下module1中的类Test, 当`__init__.py`文件为空的时候需要使用完整的导入路径:`from package.module import Test`, 但如果在`__init__.py`中添加`from module1 import Test`语句，就可以直接使用`from package import Test`来导入类Test。\n\n3. 通过在该文件中定义`__all__`变量，控制需要导入的子包或模块。\n\n   \n\n\n# 三、`__all__`的作用\n\n​\t`__all__`只控制`from xxx import *`的行为, 不影响`import` 和 `from xxx import xxxx`的行为\n\n1. 在`__init__.py`文件中添加：\n\n   ​\t\t\t\t\t`__all__ = ['module1', 'subpackage']`\n\n   `__init__.py`不使用`__all__`属性，不能通过`from package import * `导入\n\n   `__init__.py`使用`__all__`属性，`from package import *`只能导入`__all__`列表中的成员，但可以通过\n\n   `import package.module2`和`from package import module2`导入\n\n2. 在普通`*.py`文件中添加：`__all__`\n\n   ```python\n   # from xxx import * 这种方式只能导入公有的属性，方法或类【无法导入以单下划线开头（protected）或以双下划线开头(private)的属性，方法或类】\n   \n   # from xxx import aa, bb 可以导入public,protected,private\n   # import xxx   xxx.__func  可以访问public,protected,private\n   ```\n\n   模块中不使用`__all__`属性，可以导入模块内的所有公有属性，函数和类 。\n\n   模块中使用`__all__`属性，只能导入`__all__`中定义的属性，函数和类(包括私有属性和保护属性)。\n\n\n\n# 四、`from ... import ...`的问题\n\n1. 命名空间的冲突\n\n   ```python\n   # module1.py\n   def add()\n   \tprint(\"add in module1\")\n   # module1.py\n   def add()\n   \tprint(\"add in module2\")\n       \n   # main.py\n   from package.module1 import add\n   from package.module2 import add\n   \n   # 最近导入的add,会覆盖先导入的add\n   ```\n\n   \n\n2. 循环嵌套导入的问题\n\n   ```python\n   # module1.py\n   from module2 import g\n   def x()\n   \tpass\n   # module2.py\n   from module1 import x\n   def g()\n   \tpass\n   \n   # 会抛出一个ImportError: cannot import name 'g'异常，解决方法直接使用import 语句\n   ```\n\n\n\n","slug":"Python包管理","published":1,"updated":"2019-12-02T08:53:06.518Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454ts5000bzsv54pepxk8o","content":"<h1 id=\"一、什么是包？\"><a href=\"#一、什么是包？\" class=\"headerlink\" title=\"一、什么是包？\"></a>一、什么是包？</h1><p>简单来说包即是目录，但和普通目录不同，它除了包含python文件以外，还包含一个<code>__init__.py</code>文件，同时它允许嵌套。</p>\n<p>包结构如下：</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">package<span class=\"token operator\">/</span>__init__<span class=\"token punctuation\">.</span>py\n        module1<span class=\"token punctuation\">.</span>py\n            <span class=\"token keyword\">class</span> <span class=\"token class-name\">C1</span><span class=\"token punctuation\">:</span><span class=\"token keyword\">pass</span>\n        module2<span class=\"token punctuation\">.</span>py\n            <span class=\"token keyword\">class</span> <span class=\"token class-name\">C2</span><span class=\"token punctuation\">:</span><span class=\"token keyword\">pass</span>\n        subpackage<span class=\"token operator\">/</span>__init__<span class=\"token punctuation\">.</span>py\n                   module1<span class=\"token punctuation\">.</span>py                    \n                      module2<span class=\"token punctuation\">.</span>py                  \n                   module3<span class=\"token punctuation\">.</span>py\n\nmain<span class=\"token punctuation\">.</span>py\n\n<span class=\"token keyword\">import</span> package\n<span class=\"token keyword\">import</span> package<span class=\"token punctuation\">.</span>module1\n<span class=\"token keyword\">import</span> package<span class=\"token punctuation\">.</span>subpackage\n<span class=\"token keyword\">import</span> package<span class=\"token punctuation\">.</span>subpackage<span class=\"token punctuation\">.</span>module1\n\n<span class=\"token keyword\">from</span> package <span class=\"token keyword\">import</span> module1\n<span class=\"token keyword\">from</span> package <span class=\"token keyword\">import</span> subpackage\n<span class=\"token keyword\">from</span> package<span class=\"token punctuation\">.</span>subpackage <span class=\"token keyword\">import</span> module1\n<span class=\"token comment\" spellcheck=\"true\"># from package import module3</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"二、-init-py的作用\"><a href=\"#二、-init-py的作用\" class=\"headerlink\" title=\"二、__init__.py的作用\"></a>二、<code>__init__.py</code>的作用</h1><ol>\n<li><p>区别包和普通目录</p>\n</li>\n<li><p>可以使模块中的对象变成包可见</p>\n<p>例如：要导入package包下module1中的类Test, 当<code>__init__.py</code>文件为空的时候需要使用完整的导入路径:<code>from package.module import Test</code>, 但如果在<code>__init__.py</code>中添加<code>from module1 import Test</code>语句，就可以直接使用<code>from package import Test</code>来导入类Test。</p>\n</li>\n<li><p>通过在该文件中定义<code>__all__</code>变量，控制需要导入的子包或模块。</p>\n</li>\n</ol>\n<h1 id=\"三、-all-的作用\"><a href=\"#三、-all-的作用\" class=\"headerlink\" title=\"三、__all__的作用\"></a>三、<code>__all__</code>的作用</h1><p>​    <code>__all__</code>只控制<code>from xxx import *</code>的行为, 不影响<code>import</code> 和 <code>from xxx import xxxx</code>的行为</p>\n<ol>\n<li><p>在<code>__init__.py</code>文件中添加：</p>\n<p>​                    <code>__all__ = [&#39;module1&#39;, &#39;subpackage&#39;]</code></p>\n<p><code>__init__.py</code>不使用<code>__all__</code>属性，不能通过<code>from package import *</code>导入</p>\n<p><code>__init__.py</code>使用<code>__all__</code>属性，<code>from package import *</code>只能导入<code>__all__</code>列表中的成员，但可以通过</p>\n<p><code>import package.module2</code>和<code>from package import module2</code>导入</p>\n</li>\n<li><p>在普通<code>*.py</code>文件中添加：<code>__all__</code></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># from xxx import * 这种方式只能导入公有的属性，方法或类【无法导入以单下划线开头（protected）或以双下划线开头(private)的属性，方法或类】</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># from xxx import aa, bb 可以导入public,protected,private</span>\n<span class=\"token comment\" spellcheck=\"true\"># import xxx   xxx.__func  可以访问public,protected,private</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>模块中不使用<code>__all__</code>属性，可以导入模块内的所有公有属性，函数和类 。</p>\n<p>模块中使用<code>__all__</code>属性，只能导入<code>__all__</code>中定义的属性，函数和类(包括私有属性和保护属性)。</p>\n</li>\n</ol>\n<h1 id=\"四、from-import-的问题\"><a href=\"#四、from-import-的问题\" class=\"headerlink\" title=\"四、from ... import ...的问题\"></a>四、<code>from ... import ...</code>的问题</h1><ol>\n<li><p>命名空间的冲突</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># module1.py</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">add</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"add in module1\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\"># module1.py</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">add</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"add in module2\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># main.py</span>\n<span class=\"token keyword\">from</span> package<span class=\"token punctuation\">.</span>module1 <span class=\"token keyword\">import</span> add\n<span class=\"token keyword\">from</span> package<span class=\"token punctuation\">.</span>module2 <span class=\"token keyword\">import</span> add\n\n<span class=\"token comment\" spellcheck=\"true\"># 最近导入的add,会覆盖先导入的add</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>循环嵌套导入的问题</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token comment\" spellcheck=\"true\"># module1.py</span>\n<span class=\"token keyword\">from</span> module2 <span class=\"token keyword\">import</span> g\n<span class=\"token keyword\">def</span> <span class=\"token function\">x</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">pass</span>\n<span class=\"token comment\" spellcheck=\"true\"># module2.py</span>\n<span class=\"token keyword\">from</span> module1 <span class=\"token keyword\">import</span> x\n<span class=\"token keyword\">def</span> <span class=\"token function\">g</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">pass</span>\n\n<span class=\"token comment\" spellcheck=\"true\"># 会抛出一个ImportError: cannot import name 'g'异常，解决方法直接使用import 语句</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、什么是包？\"><a href=\"#一、什么是包？\" class=\"headerlink\" title=\"一、什么是包？\"></a>一、什么是包？</h1><p>简单来说包即是目录，但和普通目录不同，它除了包含python文件以外，还包含一个<code>__init__.py</code>文件，同时它允许嵌套。</p>\n<p>包结构如下：</p>\n<pre><code class=\"python\">package/__init__.py\n        module1.py\n            class C1:pass\n        module2.py\n            class C2:pass\n        subpackage/__init__.py\n                   module1.py                    \n                      module2.py                  \n                   module3.py\n\nmain.py\n\nimport package\nimport package.module1\nimport package.subpackage\nimport package.subpackage.module1\n\nfrom package import module1\nfrom package import subpackage\nfrom package.subpackage import module1\n# from package import module3</code></pre>\n<h1 id=\"二、-init-py的作用\"><a href=\"#二、-init-py的作用\" class=\"headerlink\" title=\"二、__init__.py的作用\"></a>二、<code>__init__.py</code>的作用</h1><ol>\n<li><p>区别包和普通目录</p>\n</li>\n<li><p>可以使模块中的对象变成包可见</p>\n<p>例如：要导入package包下module1中的类Test, 当<code>__init__.py</code>文件为空的时候需要使用完整的导入路径:<code>from package.module import Test</code>, 但如果在<code>__init__.py</code>中添加<code>from module1 import Test</code>语句，就可以直接使用<code>from package import Test</code>来导入类Test。</p>\n</li>\n<li><p>通过在该文件中定义<code>__all__</code>变量，控制需要导入的子包或模块。</p>\n</li>\n</ol>\n<h1 id=\"三、-all-的作用\"><a href=\"#三、-all-的作用\" class=\"headerlink\" title=\"三、__all__的作用\"></a>三、<code>__all__</code>的作用</h1><p>​    <code>__all__</code>只控制<code>from xxx import *</code>的行为, 不影响<code>import</code> 和 <code>from xxx import xxxx</code>的行为</p>\n<ol>\n<li><p>在<code>__init__.py</code>文件中添加：</p>\n<p>​                    <code>__all__ = [&#39;module1&#39;, &#39;subpackage&#39;]</code></p>\n<p><code>__init__.py</code>不使用<code>__all__</code>属性，不能通过<code>from package import *</code>导入</p>\n<p><code>__init__.py</code>使用<code>__all__</code>属性，<code>from package import *</code>只能导入<code>__all__</code>列表中的成员，但可以通过</p>\n<p><code>import package.module2</code>和<code>from package import module2</code>导入</p>\n</li>\n<li><p>在普通<code>*.py</code>文件中添加：<code>__all__</code></p>\n<pre><code class=\"python\"># from xxx import * 这种方式只能导入公有的属性，方法或类【无法导入以单下划线开头（protected）或以双下划线开头(private)的属性，方法或类】\n\n# from xxx import aa, bb 可以导入public,protected,private\n# import xxx   xxx.__func  可以访问public,protected,private</code></pre>\n<p>模块中不使用<code>__all__</code>属性，可以导入模块内的所有公有属性，函数和类 。</p>\n<p>模块中使用<code>__all__</code>属性，只能导入<code>__all__</code>中定义的属性，函数和类(包括私有属性和保护属性)。</p>\n</li>\n</ol>\n<h1 id=\"四、from-import-的问题\"><a href=\"#四、from-import-的问题\" class=\"headerlink\" title=\"四、from ... import ...的问题\"></a>四、<code>from ... import ...</code>的问题</h1><ol>\n<li><p>命名空间的冲突</p>\n<pre><code class=\"python\"># module1.py\ndef add()\n    print(&quot;add in module1&quot;)\n# module1.py\ndef add()\n    print(&quot;add in module2&quot;)\n\n# main.py\nfrom package.module1 import add\nfrom package.module2 import add\n\n# 最近导入的add,会覆盖先导入的add</code></pre>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>循环嵌套导入的问题</p>\n<pre><code class=\"python\"># module1.py\nfrom module2 import g\ndef x()\n    pass\n# module2.py\nfrom module1 import x\ndef g()\n    pass\n\n# 会抛出一个ImportError: cannot import name &#39;g&#39;异常，解决方法直接使用import 语句</code></pre>\n</li>\n</ol>\n"},{"title":"数据结构与算法之树","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-23T08:11:20.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、树\n\n## 树的定义\n\n树(Tree)是n(n>=0)个结点的有限集。当n=0时成为空树，在任意一颗非空树中：\n\n- 有且仅有一个特定的称为根(Root)的结点;\n- 当n>1时，其余节点可分为m(m>0)个**互不相交**的有限集T1、T2、。。。、Tm, 其中每一个集合本身又是一棵树，并且称为根的子树(SubTree)。\n\n## 结点的分类\n\n结点拥有的子树称为结点的度(Degree), 树的度取树内各结点的度的最大值。\n\n- 度为0的结点称为叶结点(Leaf)或终端结点；\n- 度不为0的点称为分支结点或非终端结点，除根结点外，分支结点也称为内部结点。\n\n## 结点间的关系\n\n- 结点的子树的根称为结点的孩子(Child), 相应的，该结点称为孩子的双亲(Parent), 同一双亲的孩子之间互称为兄弟(Sibling)。\n- 结点的祖先是从根到该结点所经过分支上的所有结点。\n\n## 结点的层次\n\n- 结点的层次(Level)从根开始，根为第一层，根的孩子为第二层。\n- 其双亲在同一层的结点互为堂兄弟。\n- 树中结点最大层称为树的深度(Depth)或者高度。\n\n## 有序树和森林\n\n- 如果将树中结点的各个子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。\n- 森林(Forest)是m(m>=0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。\n\n## 树的存储结构\n\n### 1.双亲表示法\n\n- 双亲表示法，言外之意就是以双亲作为索引的关键词的一种存储方式。\n\n- 我们假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示双亲结点在数组中位置的元素。\n\n- 也就是说，每个结点除了知道自己是谁，还知道它的双亲在哪里。\n\n- 那么我们可以做如下定义:\n\n  ```c++\n  // 树的双亲表示法结构定义\n  #define MAX_TREE_SIZE 100\n  typedef int ElemType;\n  typedef struct PTNode\n  {\n  \tElemType data; // 结点数据\n  \tint parent; \t// 双亲位置\n  } PTNode;\n  \n  typedef struct{\n      PTNode nodes[MAX_TREE_SIZE];\n      int r;\t\t// 根的位置\n      int n;\t\t// 结点数目\n  } PTree;\n  ```\n\n  ![](parents.png)\n  \n- 这样的存储结构，我们可以根据某结点的parent指针找到它的双亲结点，所用的时间复杂度是O(1), 索引到parent的值为-1时，表示找到了树结点的根。\n\n- 可是，如果我们要知道某结点的孩子是什么？那么不好意思，请遍历整个树结构。\n\n- 改进一些也很简单，只需要在每个结点中添加孩子的索引\n\n### 2.孩子表示法\n\n- 方案一：根据树的度，声明足够空间存放子树的结点。缺点十分明显，就是造成了浪费！\n\n  ![](childs01.png)\n\n- 方案二：根据每个结点的度申请空间存放子树结点。\n\n  ![](childs02.png)\n\n- 方案三:  数组和链表结合\n\n  ![](childs03.png)\n\n### 3.双亲孩子表示法\n\n​\t前两种方案结合\n\n​\t![](parchild.png)\n\n```c++\n#define MAX_TREE_SIZE 100\n// 孩子节点\ntypedef struct CTNode{\n    int child;  // 孩子结点下标\n    struct CTNode *next; // 指向下一个孩子的指针\n} *ChildPtr;\n\n// 表头结构\ntypedef struct{\n    ElemType data;  // 存放在树中的结点的数据\n    int parent;\t\t// 存放双亲的下标\n    ChildPtr firstchild; // 指向第一个孩子的指针\n} CTBox;\n\n// 树结构\ntypedef struct{\n    CTBox nodes[MAX_TREE_SIZE]; // 结点数组\n    int r;\t\t// 根的位置\n    int n;\t\t// 结点数目\n}\n```\n\n\n\n# 二、二叉树\n\n## 二叉树的定义\n\n- 二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）\n- 左子树和右子树是有顺序的，次序不能颠倒。\n- 即是树中某结点只有一颗子树，也要区分它是左子树还是右子树。\n\n## 二叉树的五种基本形态\n\n- 空二叉树\n\n- 只有一个根结点\n\n- 根结点只有左子树\n\n- 根结点只有右子树\n\n- 根节点即有左子树又有右子树\n\n  ![](binarytree.png)\n\n## 特殊二叉树\n\n- 斜树\n\n- 满二叉树\n\n  - 叶子只能出现在最下一层\n\n  - 非叶子结点的度都是2\n\n  - 在同样深度的二叉树中，满二叉树的结点个数一定是最多的，同时叶子也是最多的。\n\n    ![](满二叉树.png)\n\n- 完全二叉树\n\n  若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。\n\n  - 叶子结点只能出现在最下两层\n  - 最下层的叶子一定集中在左部连续位置。\n  - 倒数第二层，若有叶子结点，一定都在右部连续位置。\n  - 如果结点度为1，则该结点只有左孩子\n  - 同样结点树的二叉树，完全二叉树的深度是最小的。\n\n  ![](完全二叉树.png)\n\n## 二叉树的性质\n\n**性质1:** 在二叉树的第i层上至多有2^(i-1)个结点（i>0）\n**性质2:** 深度为k的二叉树至多有2^k - 1个结点（k>0）\n**性质3:** 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;\n**性质4:**具有n个结点的完全二叉树的深度必为 log2(n+1)\n**性质5:**对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）\n\n# 三、动态查找树\n\n## 一）二叉查找树\n\n\n\n## 二）平衡二叉树(AVL树)\n\n## 三）红黑树\n\n\n\n# 四、多路查找树\n\n## 一）B树\n\n## 二）B+树\n\n## 三）B*树\n\n## 四）R树\n\n\n\n# 五、决策树\n\n\n\n# 六 、`LeetCode`关于树的题目","source":"_posts/数据结构与算法之树.md","raw":"---\ntitle: 数据结构与算法之树\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-23 16:11:20\npassword:\nsummary:\ntags: \n- Tree\n- C/C++\ncategories: 数据结构与算法\n---\n\n\n\n# 一、树\n\n## 树的定义\n\n树(Tree)是n(n>=0)个结点的有限集。当n=0时成为空树，在任意一颗非空树中：\n\n- 有且仅有一个特定的称为根(Root)的结点;\n- 当n>1时，其余节点可分为m(m>0)个**互不相交**的有限集T1、T2、。。。、Tm, 其中每一个集合本身又是一棵树，并且称为根的子树(SubTree)。\n\n## 结点的分类\n\n结点拥有的子树称为结点的度(Degree), 树的度取树内各结点的度的最大值。\n\n- 度为0的结点称为叶结点(Leaf)或终端结点；\n- 度不为0的点称为分支结点或非终端结点，除根结点外，分支结点也称为内部结点。\n\n## 结点间的关系\n\n- 结点的子树的根称为结点的孩子(Child), 相应的，该结点称为孩子的双亲(Parent), 同一双亲的孩子之间互称为兄弟(Sibling)。\n- 结点的祖先是从根到该结点所经过分支上的所有结点。\n\n## 结点的层次\n\n- 结点的层次(Level)从根开始，根为第一层，根的孩子为第二层。\n- 其双亲在同一层的结点互为堂兄弟。\n- 树中结点最大层称为树的深度(Depth)或者高度。\n\n## 有序树和森林\n\n- 如果将树中结点的各个子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。\n- 森林(Forest)是m(m>=0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。\n\n## 树的存储结构\n\n### 1.双亲表示法\n\n- 双亲表示法，言外之意就是以双亲作为索引的关键词的一种存储方式。\n\n- 我们假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示双亲结点在数组中位置的元素。\n\n- 也就是说，每个结点除了知道自己是谁，还知道它的双亲在哪里。\n\n- 那么我们可以做如下定义:\n\n  ```c++\n  // 树的双亲表示法结构定义\n  #define MAX_TREE_SIZE 100\n  typedef int ElemType;\n  typedef struct PTNode\n  {\n  \tElemType data; // 结点数据\n  \tint parent; \t// 双亲位置\n  } PTNode;\n  \n  typedef struct{\n      PTNode nodes[MAX_TREE_SIZE];\n      int r;\t\t// 根的位置\n      int n;\t\t// 结点数目\n  } PTree;\n  ```\n\n  ![](parents.png)\n  \n- 这样的存储结构，我们可以根据某结点的parent指针找到它的双亲结点，所用的时间复杂度是O(1), 索引到parent的值为-1时，表示找到了树结点的根。\n\n- 可是，如果我们要知道某结点的孩子是什么？那么不好意思，请遍历整个树结构。\n\n- 改进一些也很简单，只需要在每个结点中添加孩子的索引\n\n### 2.孩子表示法\n\n- 方案一：根据树的度，声明足够空间存放子树的结点。缺点十分明显，就是造成了浪费！\n\n  ![](childs01.png)\n\n- 方案二：根据每个结点的度申请空间存放子树结点。\n\n  ![](childs02.png)\n\n- 方案三:  数组和链表结合\n\n  ![](childs03.png)\n\n### 3.双亲孩子表示法\n\n​\t前两种方案结合\n\n​\t![](parchild.png)\n\n```c++\n#define MAX_TREE_SIZE 100\n// 孩子节点\ntypedef struct CTNode{\n    int child;  // 孩子结点下标\n    struct CTNode *next; // 指向下一个孩子的指针\n} *ChildPtr;\n\n// 表头结构\ntypedef struct{\n    ElemType data;  // 存放在树中的结点的数据\n    int parent;\t\t// 存放双亲的下标\n    ChildPtr firstchild; // 指向第一个孩子的指针\n} CTBox;\n\n// 树结构\ntypedef struct{\n    CTBox nodes[MAX_TREE_SIZE]; // 结点数组\n    int r;\t\t// 根的位置\n    int n;\t\t// 结点数目\n}\n```\n\n\n\n# 二、二叉树\n\n## 二叉树的定义\n\n- 二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）\n- 左子树和右子树是有顺序的，次序不能颠倒。\n- 即是树中某结点只有一颗子树，也要区分它是左子树还是右子树。\n\n## 二叉树的五种基本形态\n\n- 空二叉树\n\n- 只有一个根结点\n\n- 根结点只有左子树\n\n- 根结点只有右子树\n\n- 根节点即有左子树又有右子树\n\n  ![](binarytree.png)\n\n## 特殊二叉树\n\n- 斜树\n\n- 满二叉树\n\n  - 叶子只能出现在最下一层\n\n  - 非叶子结点的度都是2\n\n  - 在同样深度的二叉树中，满二叉树的结点个数一定是最多的，同时叶子也是最多的。\n\n    ![](满二叉树.png)\n\n- 完全二叉树\n\n  若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。\n\n  - 叶子结点只能出现在最下两层\n  - 最下层的叶子一定集中在左部连续位置。\n  - 倒数第二层，若有叶子结点，一定都在右部连续位置。\n  - 如果结点度为1，则该结点只有左孩子\n  - 同样结点树的二叉树，完全二叉树的深度是最小的。\n\n  ![](完全二叉树.png)\n\n## 二叉树的性质\n\n**性质1:** 在二叉树的第i层上至多有2^(i-1)个结点（i>0）\n**性质2:** 深度为k的二叉树至多有2^k - 1个结点（k>0）\n**性质3:** 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;\n**性质4:**具有n个结点的完全二叉树的深度必为 log2(n+1)\n**性质5:**对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）\n\n# 三、动态查找树\n\n## 一）二叉查找树\n\n\n\n## 二）平衡二叉树(AVL树)\n\n## 三）红黑树\n\n\n\n# 四、多路查找树\n\n## 一）B树\n\n## 二）B+树\n\n## 三）B*树\n\n## 四）R树\n\n\n\n# 五、决策树\n\n\n\n# 六 、`LeetCode`关于树的题目","slug":"数据结构与算法之树","published":1,"updated":"2019-11-23T18:06:46.889Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454ts8000fzsv5130vsj44","content":"<h1 id=\"一、树\"><a href=\"#一、树\" class=\"headerlink\" title=\"一、树\"></a>一、树</h1><h2 id=\"树的定义\"><a href=\"#树的定义\" class=\"headerlink\" title=\"树的定义\"></a>树的定义</h2><p>树(Tree)是n(n&gt;=0)个结点的有限集。当n=0时成为空树，在任意一颗非空树中：</p>\n<ul>\n<li>有且仅有一个特定的称为根(Root)的结点;</li>\n<li>当n&gt;1时，其余节点可分为m(m&gt;0)个<strong>互不相交</strong>的有限集T1、T2、。。。、Tm, 其中每一个集合本身又是一棵树，并且称为根的子树(SubTree)。</li>\n</ul>\n<h2 id=\"结点的分类\"><a href=\"#结点的分类\" class=\"headerlink\" title=\"结点的分类\"></a>结点的分类</h2><p>结点拥有的子树称为结点的度(Degree), 树的度取树内各结点的度的最大值。</p>\n<ul>\n<li>度为0的结点称为叶结点(Leaf)或终端结点；</li>\n<li>度不为0的点称为分支结点或非终端结点，除根结点外，分支结点也称为内部结点。</li>\n</ul>\n<h2 id=\"结点间的关系\"><a href=\"#结点间的关系\" class=\"headerlink\" title=\"结点间的关系\"></a>结点间的关系</h2><ul>\n<li>结点的子树的根称为结点的孩子(Child), 相应的，该结点称为孩子的双亲(Parent), 同一双亲的孩子之间互称为兄弟(Sibling)。</li>\n<li>结点的祖先是从根到该结点所经过分支上的所有结点。</li>\n</ul>\n<h2 id=\"结点的层次\"><a href=\"#结点的层次\" class=\"headerlink\" title=\"结点的层次\"></a>结点的层次</h2><ul>\n<li>结点的层次(Level)从根开始，根为第一层，根的孩子为第二层。</li>\n<li>其双亲在同一层的结点互为堂兄弟。</li>\n<li>树中结点最大层称为树的深度(Depth)或者高度。</li>\n</ul>\n<h2 id=\"有序树和森林\"><a href=\"#有序树和森林\" class=\"headerlink\" title=\"有序树和森林\"></a>有序树和森林</h2><ul>\n<li>如果将树中结点的各个子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。</li>\n<li>森林(Forest)是m(m&gt;=0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。</li>\n</ul>\n<h2 id=\"树的存储结构\"><a href=\"#树的存储结构\" class=\"headerlink\" title=\"树的存储结构\"></a>树的存储结构</h2><h3 id=\"1-双亲表示法\"><a href=\"#1-双亲表示法\" class=\"headerlink\" title=\"1.双亲表示法\"></a>1.双亲表示法</h3><ul>\n<li><p>双亲表示法，言外之意就是以双亲作为索引的关键词的一种存储方式。</p>\n</li>\n<li><p>我们假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示双亲结点在数组中位置的元素。</p>\n</li>\n<li><p>也就是说，每个结点除了知道自己是谁，还知道它的双亲在哪里。</p>\n</li>\n<li><p>那么我们可以做如下定义:</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">// 树的双亲表示法结构定义\n#define MAX_TREE_SIZE 100\ntypedef int ElemType;\ntypedef struct PTNode\n{\n    ElemType data; // 结点数据\n    int parent;     // 双亲位置\n} PTNode;\n\ntypedef struct{\n    PTNode nodes[MAX_TREE_SIZE];\n    int r;        // 根的位置\n    int n;        // 结点数目\n} PTree;<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><img src=\"parents.png\" alt></p>\n</li>\n<li><p>这样的存储结构，我们可以根据某结点的parent指针找到它的双亲结点，所用的时间复杂度是O(1), 索引到parent的值为-1时，表示找到了树结点的根。</p>\n</li>\n<li><p>可是，如果我们要知道某结点的孩子是什么？那么不好意思，请遍历整个树结构。</p>\n</li>\n<li><p>改进一些也很简单，只需要在每个结点中添加孩子的索引</p>\n</li>\n</ul>\n<h3 id=\"2-孩子表示法\"><a href=\"#2-孩子表示法\" class=\"headerlink\" title=\"2.孩子表示法\"></a>2.孩子表示法</h3><ul>\n<li><p>方案一：根据树的度，声明足够空间存放子树的结点。缺点十分明显，就是造成了浪费！</p>\n<p><img src=\"childs01.png\" alt></p>\n</li>\n<li><p>方案二：根据每个结点的度申请空间存放子树结点。</p>\n<p><img src=\"childs02.png\" alt></p>\n</li>\n<li><p>方案三:  数组和链表结合</p>\n<p><img src=\"childs03.png\" alt></p>\n</li>\n</ul>\n<h3 id=\"3-双亲孩子表示法\"><a href=\"#3-双亲孩子表示法\" class=\"headerlink\" title=\"3.双亲孩子表示法\"></a>3.双亲孩子表示法</h3><p>​    前两种方案结合</p>\n<p>​    <img src=\"parchild.png\" alt></p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">#define MAX_TREE_SIZE 100\n// 孩子节点\ntypedef struct CTNode{\n    int child;  // 孩子结点下标\n    struct CTNode *next; // 指向下一个孩子的指针\n} *ChildPtr;\n\n// 表头结构\ntypedef struct{\n    ElemType data;  // 存放在树中的结点的数据\n    int parent;        // 存放双亲的下标\n    ChildPtr firstchild; // 指向第一个孩子的指针\n} CTBox;\n\n// 树结构\ntypedef struct{\n    CTBox nodes[MAX_TREE_SIZE]; // 结点数组\n    int r;        // 根的位置\n    int n;        // 结点数目\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"二、二叉树\"><a href=\"#二、二叉树\" class=\"headerlink\" title=\"二、二叉树\"></a>二、二叉树</h1><h2 id=\"二叉树的定义\"><a href=\"#二叉树的定义\" class=\"headerlink\" title=\"二叉树的定义\"></a>二叉树的定义</h2><ul>\n<li>二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）</li>\n<li>左子树和右子树是有顺序的，次序不能颠倒。</li>\n<li>即是树中某结点只有一颗子树，也要区分它是左子树还是右子树。</li>\n</ul>\n<h2 id=\"二叉树的五种基本形态\"><a href=\"#二叉树的五种基本形态\" class=\"headerlink\" title=\"二叉树的五种基本形态\"></a>二叉树的五种基本形态</h2><ul>\n<li><p>空二叉树</p>\n</li>\n<li><p>只有一个根结点</p>\n</li>\n<li><p>根结点只有左子树</p>\n</li>\n<li><p>根结点只有右子树</p>\n</li>\n<li><p>根节点即有左子树又有右子树</p>\n<p><img src=\"binarytree.png\" alt></p>\n</li>\n</ul>\n<h2 id=\"特殊二叉树\"><a href=\"#特殊二叉树\" class=\"headerlink\" title=\"特殊二叉树\"></a>特殊二叉树</h2><ul>\n<li><p>斜树</p>\n</li>\n<li><p>满二叉树</p>\n<ul>\n<li><p>叶子只能出现在最下一层</p>\n</li>\n<li><p>非叶子结点的度都是2</p>\n</li>\n<li><p>在同样深度的二叉树中，满二叉树的结点个数一定是最多的，同时叶子也是最多的。</p>\n<p><img src=\"%E6%BB%A1%E4%BA%8C%E5%8F%89%E6%A0%91.png\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>完全二叉树</p>\n<p>若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。</p>\n<ul>\n<li>叶子结点只能出现在最下两层</li>\n<li>最下层的叶子一定集中在左部连续位置。</li>\n<li>倒数第二层，若有叶子结点，一定都在右部连续位置。</li>\n<li>如果结点度为1，则该结点只有左孩子</li>\n<li>同样结点树的二叉树，完全二叉树的深度是最小的。</li>\n</ul>\n<p><img src=\"%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91.png\" alt></p>\n</li>\n</ul>\n<h2 id=\"二叉树的性质\"><a href=\"#二叉树的性质\" class=\"headerlink\" title=\"二叉树的性质\"></a>二叉树的性质</h2><p><strong>性质1:</strong> 在二叉树的第i层上至多有2^(i-1)个结点（i&gt;0）<br><strong>性质2:</strong> 深度为k的二叉树至多有2^k - 1个结点（k&gt;0）<br><strong>性质3:</strong> 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;<br><strong>性质4:</strong>具有n个结点的完全二叉树的深度必为 log2(n+1)<br><strong>性质5:</strong>对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）</p>\n<h1 id=\"三、动态查找树\"><a href=\"#三、动态查找树\" class=\"headerlink\" title=\"三、动态查找树\"></a>三、动态查找树</h1><h2 id=\"一）二叉查找树\"><a href=\"#一）二叉查找树\" class=\"headerlink\" title=\"一）二叉查找树\"></a>一）二叉查找树</h2><h2 id=\"二）平衡二叉树-AVL树\"><a href=\"#二）平衡二叉树-AVL树\" class=\"headerlink\" title=\"二）平衡二叉树(AVL树)\"></a>二）平衡二叉树(AVL树)</h2><h2 id=\"三）红黑树\"><a href=\"#三）红黑树\" class=\"headerlink\" title=\"三）红黑树\"></a>三）红黑树</h2><h1 id=\"四、多路查找树\"><a href=\"#四、多路查找树\" class=\"headerlink\" title=\"四、多路查找树\"></a>四、多路查找树</h1><h2 id=\"一）B树\"><a href=\"#一）B树\" class=\"headerlink\" title=\"一）B树\"></a>一）B树</h2><h2 id=\"二）B-树\"><a href=\"#二）B-树\" class=\"headerlink\" title=\"二）B+树\"></a>二）B+树</h2><h2 id=\"三）B-树\"><a href=\"#三）B-树\" class=\"headerlink\" title=\"三）B*树\"></a>三）B*树</h2><h2 id=\"四）R树\"><a href=\"#四）R树\" class=\"headerlink\" title=\"四）R树\"></a>四）R树</h2><h1 id=\"五、决策树\"><a href=\"#五、决策树\" class=\"headerlink\" title=\"五、决策树\"></a>五、决策树</h1><h1 id=\"六-、LeetCode关于树的题目\"><a href=\"#六-、LeetCode关于树的题目\" class=\"headerlink\" title=\"六 、LeetCode关于树的题目\"></a>六 、<code>LeetCode</code>关于树的题目</h1>","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、树\"><a href=\"#一、树\" class=\"headerlink\" title=\"一、树\"></a>一、树</h1><h2 id=\"树的定义\"><a href=\"#树的定义\" class=\"headerlink\" title=\"树的定义\"></a>树的定义</h2><p>树(Tree)是n(n&gt;=0)个结点的有限集。当n=0时成为空树，在任意一颗非空树中：</p>\n<ul>\n<li>有且仅有一个特定的称为根(Root)的结点;</li>\n<li>当n&gt;1时，其余节点可分为m(m&gt;0)个<strong>互不相交</strong>的有限集T1、T2、。。。、Tm, 其中每一个集合本身又是一棵树，并且称为根的子树(SubTree)。</li>\n</ul>\n<h2 id=\"结点的分类\"><a href=\"#结点的分类\" class=\"headerlink\" title=\"结点的分类\"></a>结点的分类</h2><p>结点拥有的子树称为结点的度(Degree), 树的度取树内各结点的度的最大值。</p>\n<ul>\n<li>度为0的结点称为叶结点(Leaf)或终端结点；</li>\n<li>度不为0的点称为分支结点或非终端结点，除根结点外，分支结点也称为内部结点。</li>\n</ul>\n<h2 id=\"结点间的关系\"><a href=\"#结点间的关系\" class=\"headerlink\" title=\"结点间的关系\"></a>结点间的关系</h2><ul>\n<li>结点的子树的根称为结点的孩子(Child), 相应的，该结点称为孩子的双亲(Parent), 同一双亲的孩子之间互称为兄弟(Sibling)。</li>\n<li>结点的祖先是从根到该结点所经过分支上的所有结点。</li>\n</ul>\n<h2 id=\"结点的层次\"><a href=\"#结点的层次\" class=\"headerlink\" title=\"结点的层次\"></a>结点的层次</h2><ul>\n<li>结点的层次(Level)从根开始，根为第一层，根的孩子为第二层。</li>\n<li>其双亲在同一层的结点互为堂兄弟。</li>\n<li>树中结点最大层称为树的深度(Depth)或者高度。</li>\n</ul>\n<h2 id=\"有序树和森林\"><a href=\"#有序树和森林\" class=\"headerlink\" title=\"有序树和森林\"></a>有序树和森林</h2><ul>\n<li>如果将树中结点的各个子树看成从左至右是有次序的，不能互换的，则称该树为有序树，否则称为无序树。</li>\n<li>森林(Forest)是m(m&gt;=0)棵互不相交的树的集合。对树中每个结点而言，其子树的集合即为森林。</li>\n</ul>\n<h2 id=\"树的存储结构\"><a href=\"#树的存储结构\" class=\"headerlink\" title=\"树的存储结构\"></a>树的存储结构</h2><h3 id=\"1-双亲表示法\"><a href=\"#1-双亲表示法\" class=\"headerlink\" title=\"1.双亲表示法\"></a>1.双亲表示法</h3><ul>\n<li><p>双亲表示法，言外之意就是以双亲作为索引的关键词的一种存储方式。</p>\n</li>\n<li><p>我们假设以一组连续空间存储树的结点，同时在每个结点中，附设一个指示双亲结点在数组中位置的元素。</p>\n</li>\n<li><p>也就是说，每个结点除了知道自己是谁，还知道它的双亲在哪里。</p>\n</li>\n<li><p>那么我们可以做如下定义:</p>\n<pre><code class=\"c++\">// 树的双亲表示法结构定义\n#define MAX_TREE_SIZE 100\ntypedef int ElemType;\ntypedef struct PTNode\n{\n    ElemType data; // 结点数据\n    int parent;     // 双亲位置\n} PTNode;\n\ntypedef struct{\n    PTNode nodes[MAX_TREE_SIZE];\n    int r;        // 根的位置\n    int n;        // 结点数目\n} PTree;</code></pre>\n<p><img src=\"parents.png\" alt></p>\n</li>\n<li><p>这样的存储结构，我们可以根据某结点的parent指针找到它的双亲结点，所用的时间复杂度是O(1), 索引到parent的值为-1时，表示找到了树结点的根。</p>\n</li>\n<li><p>可是，如果我们要知道某结点的孩子是什么？那么不好意思，请遍历整个树结构。</p>\n</li>\n<li><p>改进一些也很简单，只需要在每个结点中添加孩子的索引</p>\n</li>\n</ul>\n<h3 id=\"2-孩子表示法\"><a href=\"#2-孩子表示法\" class=\"headerlink\" title=\"2.孩子表示法\"></a>2.孩子表示法</h3><ul>\n<li><p>方案一：根据树的度，声明足够空间存放子树的结点。缺点十分明显，就是造成了浪费！</p>\n<p><img src=\"childs01.png\" alt></p>\n</li>\n<li><p>方案二：根据每个结点的度申请空间存放子树结点。</p>\n<p><img src=\"childs02.png\" alt></p>\n</li>\n<li><p>方案三:  数组和链表结合</p>\n<p><img src=\"childs03.png\" alt></p>\n</li>\n</ul>\n<h3 id=\"3-双亲孩子表示法\"><a href=\"#3-双亲孩子表示法\" class=\"headerlink\" title=\"3.双亲孩子表示法\"></a>3.双亲孩子表示法</h3><p>​    前两种方案结合</p>\n<p>​    <img src=\"parchild.png\" alt></p>\n<pre><code class=\"c++\">#define MAX_TREE_SIZE 100\n// 孩子节点\ntypedef struct CTNode{\n    int child;  // 孩子结点下标\n    struct CTNode *next; // 指向下一个孩子的指针\n} *ChildPtr;\n\n// 表头结构\ntypedef struct{\n    ElemType data;  // 存放在树中的结点的数据\n    int parent;        // 存放双亲的下标\n    ChildPtr firstchild; // 指向第一个孩子的指针\n} CTBox;\n\n// 树结构\ntypedef struct{\n    CTBox nodes[MAX_TREE_SIZE]; // 结点数组\n    int r;        // 根的位置\n    int n;        // 结点数目\n}</code></pre>\n<h1 id=\"二、二叉树\"><a href=\"#二、二叉树\" class=\"headerlink\" title=\"二、二叉树\"></a>二、二叉树</h1><h2 id=\"二叉树的定义\"><a href=\"#二叉树的定义\" class=\"headerlink\" title=\"二叉树的定义\"></a>二叉树的定义</h2><ul>\n<li>二叉树是每个节点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）</li>\n<li>左子树和右子树是有顺序的，次序不能颠倒。</li>\n<li>即是树中某结点只有一颗子树，也要区分它是左子树还是右子树。</li>\n</ul>\n<h2 id=\"二叉树的五种基本形态\"><a href=\"#二叉树的五种基本形态\" class=\"headerlink\" title=\"二叉树的五种基本形态\"></a>二叉树的五种基本形态</h2><ul>\n<li><p>空二叉树</p>\n</li>\n<li><p>只有一个根结点</p>\n</li>\n<li><p>根结点只有左子树</p>\n</li>\n<li><p>根结点只有右子树</p>\n</li>\n<li><p>根节点即有左子树又有右子树</p>\n<p><img src=\"binarytree.png\" alt></p>\n</li>\n</ul>\n<h2 id=\"特殊二叉树\"><a href=\"#特殊二叉树\" class=\"headerlink\" title=\"特殊二叉树\"></a>特殊二叉树</h2><ul>\n<li><p>斜树</p>\n</li>\n<li><p>满二叉树</p>\n<ul>\n<li><p>叶子只能出现在最下一层</p>\n</li>\n<li><p>非叶子结点的度都是2</p>\n</li>\n<li><p>在同样深度的二叉树中，满二叉树的结点个数一定是最多的，同时叶子也是最多的。</p>\n<p><img src=\"%E6%BB%A1%E4%BA%8C%E5%8F%89%E6%A0%91.png\" alt></p>\n</li>\n</ul>\n</li>\n<li><p>完全二叉树</p>\n<p>若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。</p>\n<ul>\n<li>叶子结点只能出现在最下两层</li>\n<li>最下层的叶子一定集中在左部连续位置。</li>\n<li>倒数第二层，若有叶子结点，一定都在右部连续位置。</li>\n<li>如果结点度为1，则该结点只有左孩子</li>\n<li>同样结点树的二叉树，完全二叉树的深度是最小的。</li>\n</ul>\n<p><img src=\"%E5%AE%8C%E5%85%A8%E4%BA%8C%E5%8F%89%E6%A0%91.png\" alt></p>\n</li>\n</ul>\n<h2 id=\"二叉树的性质\"><a href=\"#二叉树的性质\" class=\"headerlink\" title=\"二叉树的性质\"></a>二叉树的性质</h2><p><strong>性质1:</strong> 在二叉树的第i层上至多有2^(i-1)个结点（i&gt;0）<br><strong>性质2:</strong> 深度为k的二叉树至多有2^k - 1个结点（k&gt;0）<br><strong>性质3:</strong> 对于任意一棵二叉树，如果其叶结点数为N0，而度数为2的结点总数为N2，则N0=N2+1;<br><strong>性质4:</strong>具有n个结点的完全二叉树的深度必为 log2(n+1)<br><strong>性质5:</strong>对完全二叉树，若从上至下、从左至右编号，则编号为i 的结点，其左孩子编号必为2i，其右孩子编号必为2i＋1；其双亲的编号必为i/2（i＝1 时为根,除外）</p>\n<h1 id=\"三、动态查找树\"><a href=\"#三、动态查找树\" class=\"headerlink\" title=\"三、动态查找树\"></a>三、动态查找树</h1><h2 id=\"一）二叉查找树\"><a href=\"#一）二叉查找树\" class=\"headerlink\" title=\"一）二叉查找树\"></a>一）二叉查找树</h2><h2 id=\"二）平衡二叉树-AVL树\"><a href=\"#二）平衡二叉树-AVL树\" class=\"headerlink\" title=\"二）平衡二叉树(AVL树)\"></a>二）平衡二叉树(AVL树)</h2><h2 id=\"三）红黑树\"><a href=\"#三）红黑树\" class=\"headerlink\" title=\"三）红黑树\"></a>三）红黑树</h2><h1 id=\"四、多路查找树\"><a href=\"#四、多路查找树\" class=\"headerlink\" title=\"四、多路查找树\"></a>四、多路查找树</h1><h2 id=\"一）B树\"><a href=\"#一）B树\" class=\"headerlink\" title=\"一）B树\"></a>一）B树</h2><h2 id=\"二）B-树\"><a href=\"#二）B-树\" class=\"headerlink\" title=\"二）B+树\"></a>二）B+树</h2><h2 id=\"三）B-树\"><a href=\"#三）B-树\" class=\"headerlink\" title=\"三）B*树\"></a>三）B*树</h2><h2 id=\"四）R树\"><a href=\"#四）R树\" class=\"headerlink\" title=\"四）R树\"></a>四）R树</h2><h1 id=\"五、决策树\"><a href=\"#五、决策树\" class=\"headerlink\" title=\"五、决策树\"></a>五、决策树</h1><h1 id=\"六-、LeetCode关于树的题目\"><a href=\"#六-、LeetCode关于树的题目\" class=\"headerlink\" title=\"六 、LeetCode关于树的题目\"></a>六 、<code>LeetCode</code>关于树的题目</h1>"},{"title":"机器学习概述","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-24T10:33:32.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、人工智能\n\n1. 什么是人工智能\n\n   ​\t\t![3](3.jpg)\n\n   ​\t\t人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n   ​\t\t人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n\n2. 强人工智能和弱人工智能\n\n   ​\t\t早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。\n   ​\t\t人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n   ​\t\t我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类；或者人脸识别。\n\n3. 人工智能，机器学习和深度学习的关系\n\n   ![](1.png)\n\n   \n\n   机器学习是人工智能的一种实现方式，也是最重要的实现方式。目前机器学习的方法被大量的应用解决人工智能的问题。\n\n   深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。\n\n   总的来说，深度学习是机器学习的一个子集，机器学习是人工智能的一个子集。\n\n\n\n# 二、机器学习\n\n## 机器学习的概念\n\n1. 什么是机器学习\n\n   机器学习就是机器像人类一样学习，人能从过去的经验中学习，对于机器来说过去的经验就是记录的数据。机器理解大量的数据然后归纳出模型来对数据进行预测和分析。\n\n   ![](11.png)\n\n   \n\n2. 机器学习的对象\n\n   机器学习的对象是数据(data), 它从数据出发，提取数据的特征，抽取数据的模型，发现数据的知识，又回到对数据的分析与预测中去。\n\n   作为机器学习的对象，数据包括各种数字、文字、图像、音频、视频数据以及它们的组合。\n\n   \n\n3. 机器学习的目的\n\n   机器学习用于对数据进行预测和分析，特别是对未知新数据进行预测和分析。对数据的预测可以让计算机更加智能化；对数据的分析可以让人们获取新的知识。\n\n   对数据的预测和分析是通过构建模型实现的。机器学习总的目标就是考虑学习什么样的模型和如何学习模型，已使模型对数据进行准确的预测和分析，同时也要尽可能地提升学习的效率。\n\n   \n\n4. 机器学习的分类\n\n   机器学习由监督学习、非监督学习、半监督学习和强化学习等组成。\n\n   ![](6.jpg)\n\n   监督学习，非监督学习，半监督学习的区别是训练数据是否有标记。\n\n   \n\n5. 机器学习的应用场景\n\n   ![](5.jpg)\n\n   \n\n## 机器学习三要素\n\n​\t\t机器学习的方法由模型，策略，算法构成。\n\n1. 模型\n\n   假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，这个函数的集合就称为假设空间。\n\n   模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的概率分布或决策函数。\n\n   例如：线性回归算法，它的模型就是一个线性函数，即\n\n   $$\n   f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n   $$\n   一般用向量形式写成   $f(x) = w^Tx +b$, 其中$w = (w_1, w_2, ..., w_3)$.\n\n   $w$和$d$确定之后，模型就确定了。\n\n   根据$w$和$b$的所有取值所组成的集合就是线性回归算法的假设空间。\n\n2. 策略\n\n   应用某个评估指标, 从假设空间中选择一个最优的模型。\n\n   对于给定的输入$X$, 模型$f(X)$给出相应的输出$Y$, 这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。记作$L(Y, f(X))$\n\n   机器学习中常用的损失函数有以下几种:\n\n   (1) 0-1损失函数(0-1 loss function)\n   $$\n   L(Y,f(X)) =\n   \\begin{cases}\n   0, & \\text{Y = f(X)}  \\\\\n   1, & \\text{Y $\\neq$ f(X)}\n   \\end{cases}\n   $$\n\n   (2) 平方损失函数(quadratic loss function)\n   $$\n   L(Y, f(X)) = (Y - f(X))^2\n   $$\n   (3) 绝对损失函数\n   $$\n   L(Y, f(X)) = |Y - f(X)|\n   $$\n   (4) Huber损失----平滑绝对误差\n   $$\n   L_\\delta(Y,f(X)) =\n   \\begin{cases}\n   {\\frac 12}(y-f(x))^2, & for|y-f(x)|\\le \\delta  \\\\\n   \\delta|y-f(x)| - {\\frac 12}{\\delta}^2, & \\text{otherwise}\n   \\end{cases}\n   $$\n\n   (5) 对数损失函数(logarithmic loss function)\n   $$\n   L(Y, P(Y|X)) = -logP(Y|X)\n   $$\n   损失函数值越小，模型就越好。**损失函数值最小的模型就是最优模型**。\n\n   \n\n   举例: 线性回归, $$f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$ , 均方误差为\n   $$\n   E(w, b) = {\\frac 1n}\\sum_{i=1}^n (f(x_i) - Y)^2\n   $$\n   均方误差的几何意义就是欧几里得距离。\n\n   ![](12.png)\n\n   ![](14.png)\n\n3. 算法\n\n   算法是指学习模型的具体计算方法。\n\n   机器学习常用优化算法:\n\n   (1) 梯度下降\n\n   ​\t随机梯度下降(`Stochastic Gradient Descent, SGD`)\n\n   ​\t批量梯度下降(`Batch Gradient Descent, BGD`)\n\n   ​\t小批量梯度下降(`Mini-batch Gradient Descent, MBGD`)\n\n   (2) 梯度下降的变体\n\n   ​\t`Momentum`、`Adagrad`、`Adadelta`、`RMSprop`、`Adam`\n\n   (3) 牛顿法和拟牛顿法\n\n   \n\n​\t\t举例：线性回归的优化算法可以使用梯度下降或最小二乘法\n\n​\t\t在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之后最小\n\n​\t\t求解$w$和$b$使$E(w, b) = \\sum_{i=1}^n(y_i - wx_i -b)$最小化，可以将$E(w, b)$分别对$w$和$b$求导，等于0，可以得到$w$和$b$的值。\n\n\n\n## 机器学习的训练步骤\n\n### 1. 明确问题和目标\n\n​\t需要解决什么问题，达到什么目标\n\n### 2.确定输入，收集数据\n\n​\t通过多种途径得到一个有限的训练数据的集合\n\n\n​\t[Kaggle数据集](https://www.kaggle.com/datasets)\n\n​\t[亚马逊数据集](https://registry.opendata.aws)\n\n​\t[UCI机器学习库](https://archive.ics.uci.edu/ml/datasets.html)\n\n​\t[谷歌的数据集搜素引擎](https://toolbox.google.com/datasetsearch)\n\n​\t[微软数据集](https://msropendata.com/)\n\n​\t[Awesome公共数据集](https://github.com/awesomedata/awesome-public-datasets)\n\n​\t[计算机视觉数据集](https://www.visualdata.io/)\n\n​\t[ImageNet](http://image-net.org/)\n\n​\t[MS COCO](http://cocodataset.org/)\n\n ### 3.确定输出，选择算法\n\n​\t根据输入输出数据的类型决定使用的算法类型，分类还是回归？\n\n​\t输入变量与输出变量均为连续变量的预测问题称为回归问题；\n\n​\t输出变量为有限个离散变量的预测问题称为分类问题；\n\n​\t在对应的算法类型中选择一个或多个算法。\n\n### 4.特征工程\n\n- 数据预处理\n  - 缺失数据---> 删除 和 填充 (平均数，众数)\n  - 处理特征数据---> 正规化 (归一化，正则化，白化)\n  - 处理类别数据--->独热编码\n  - 数据集划分--->训练、验证、测试\n\n- 数据降维\n\n  - 特征选择\n    - 使用L1正则化进行数据稀疏化\n    - 序列特征选择算法  SBS\n    - 通过随机森林判定特征的重要性\n\n  - 特征提取 (将特征压缩到一个低维空间，而不是像特征选择那样完全剔除不相关的特征)\n    - PCA  主成分分析\n    - 线性判别\t\n\n### 5. 建立模型\n\n- 模型空间\n\n- 损失函数\n\n- 优化算法\n\n- 评估标准\n\n  \n\n1. 模型空间\n\n   确定了算法也就确定了模型空间，模型空间包含了算法的所有可能\n\n2. 损失函数\n\n   根据具体的算法和输出决定损坏函数\n\n3. 优化算法\n\n   选择优化算法\n\n4. 评估指标:\n\n![](7.png)\n\n- 分类算法\n  - 准确率\n  - 精确率和召回率(查准率和查全率)\n  - ROC和AUC\n  - $F_1$和$F_{\\beta}$\n\n- 回归算法\n\n  - 平均绝对误差\n\n  - 均方误差\n  - R2分数\n\n### 6.确定最优模型\n\n​\t不断重复**训练模型/评估模型/选择模型**的步骤指导选择最优的模型\n\n- 模型选择数据集划分:\n\n  ​\t\t训练集训练模型\n\n  ​\t\t验证集评估模型\n\n  ​\t\t测试集测试模型\n\n  ​\t\t![](13.png)\n\n- 使用k折交叉验证评估模型性能\n\n  - holdout方法\n  - k折交叉验证\n\n  通常情况下，我们将k折交叉验证用于模型的调优，也就是找到使得模型泛化性能最优的超参值。一旦找到了满意的超参值，我们就可\n  以在全部的训练数据上重新训练模型，并使用独立的测试数据集对模型性能做出最终评价。 \n\n- 通过学习及验证曲线来调试算法\n\n  - 使用学习曲线判定偏差和方差问题\n  - 使用验证曲线判定过拟合与欠拟合\n\n-  使用网格搜索调优机器学习模型\n  - 使用网格搜索调优超参数\n  - 通过嵌套交叉验证选择模型\n  - 网格搜索（grid search），它通过寻找最优的超参值的组合以进一步提高模型的性能。 \n\n\n\n### 7.应用实际问题\n\n​\t利用学习的最优模型对新数据进行预测或分析\n\n\n\n# 三、参考\n\n​\t《统计学习方法》李航\n\n​\t 《Python机器学习》[美] [塞巴斯蒂安·拉施卡]著  高明 徐莹 陶虎成译","source":"_posts/机器学习概述.md","raw":"---\ntitle: 机器学习概述\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-24 18:33:32\npassword:\nsummary:\ntags: \n- AI\n- ML\n- DL\ncategories: 机器学习\n---\n\n\n\n# 一、人工智能\n\n1. 什么是人工智能\n\n   ​\t\t![3](3.jpg)\n\n   ​\t\t人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n   ​\t\t人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n\n2. 强人工智能和弱人工智能\n\n   ​\t\t早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。\n   ​\t\t人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。\n   ​\t\t我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类；或者人脸识别。\n\n3. 人工智能，机器学习和深度学习的关系\n\n   ![](1.png)\n\n   \n\n   机器学习是人工智能的一种实现方式，也是最重要的实现方式。目前机器学习的方法被大量的应用解决人工智能的问题。\n\n   深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。\n\n   总的来说，深度学习是机器学习的一个子集，机器学习是人工智能的一个子集。\n\n\n\n# 二、机器学习\n\n## 机器学习的概念\n\n1. 什么是机器学习\n\n   机器学习就是机器像人类一样学习，人能从过去的经验中学习，对于机器来说过去的经验就是记录的数据。机器理解大量的数据然后归纳出模型来对数据进行预测和分析。\n\n   ![](11.png)\n\n   \n\n2. 机器学习的对象\n\n   机器学习的对象是数据(data), 它从数据出发，提取数据的特征，抽取数据的模型，发现数据的知识，又回到对数据的分析与预测中去。\n\n   作为机器学习的对象，数据包括各种数字、文字、图像、音频、视频数据以及它们的组合。\n\n   \n\n3. 机器学习的目的\n\n   机器学习用于对数据进行预测和分析，特别是对未知新数据进行预测和分析。对数据的预测可以让计算机更加智能化；对数据的分析可以让人们获取新的知识。\n\n   对数据的预测和分析是通过构建模型实现的。机器学习总的目标就是考虑学习什么样的模型和如何学习模型，已使模型对数据进行准确的预测和分析，同时也要尽可能地提升学习的效率。\n\n   \n\n4. 机器学习的分类\n\n   机器学习由监督学习、非监督学习、半监督学习和强化学习等组成。\n\n   ![](6.jpg)\n\n   监督学习，非监督学习，半监督学习的区别是训练数据是否有标记。\n\n   \n\n5. 机器学习的应用场景\n\n   ![](5.jpg)\n\n   \n\n## 机器学习三要素\n\n​\t\t机器学习的方法由模型，策略，算法构成。\n\n1. 模型\n\n   假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，这个函数的集合就称为假设空间。\n\n   模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的概率分布或决策函数。\n\n   例如：线性回归算法，它的模型就是一个线性函数，即\n\n   $$\n   f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n   $$\n   一般用向量形式写成   $f(x) = w^Tx +b$, 其中$w = (w_1, w_2, ..., w_3)$.\n\n   $w$和$d$确定之后，模型就确定了。\n\n   根据$w$和$b$的所有取值所组成的集合就是线性回归算法的假设空间。\n\n2. 策略\n\n   应用某个评估指标, 从假设空间中选择一个最优的模型。\n\n   对于给定的输入$X$, 模型$f(X)$给出相应的输出$Y$, 这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。记作$L(Y, f(X))$\n\n   机器学习中常用的损失函数有以下几种:\n\n   (1) 0-1损失函数(0-1 loss function)\n   $$\n   L(Y,f(X)) =\n   \\begin{cases}\n   0, & \\text{Y = f(X)}  \\\\\n   1, & \\text{Y $\\neq$ f(X)}\n   \\end{cases}\n   $$\n\n   (2) 平方损失函数(quadratic loss function)\n   $$\n   L(Y, f(X)) = (Y - f(X))^2\n   $$\n   (3) 绝对损失函数\n   $$\n   L(Y, f(X)) = |Y - f(X)|\n   $$\n   (4) Huber损失----平滑绝对误差\n   $$\n   L_\\delta(Y,f(X)) =\n   \\begin{cases}\n   {\\frac 12}(y-f(x))^2, & for|y-f(x)|\\le \\delta  \\\\\n   \\delta|y-f(x)| - {\\frac 12}{\\delta}^2, & \\text{otherwise}\n   \\end{cases}\n   $$\n\n   (5) 对数损失函数(logarithmic loss function)\n   $$\n   L(Y, P(Y|X)) = -logP(Y|X)\n   $$\n   损失函数值越小，模型就越好。**损失函数值最小的模型就是最优模型**。\n\n   \n\n   举例: 线性回归, $$f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$ , 均方误差为\n   $$\n   E(w, b) = {\\frac 1n}\\sum_{i=1}^n (f(x_i) - Y)^2\n   $$\n   均方误差的几何意义就是欧几里得距离。\n\n   ![](12.png)\n\n   ![](14.png)\n\n3. 算法\n\n   算法是指学习模型的具体计算方法。\n\n   机器学习常用优化算法:\n\n   (1) 梯度下降\n\n   ​\t随机梯度下降(`Stochastic Gradient Descent, SGD`)\n\n   ​\t批量梯度下降(`Batch Gradient Descent, BGD`)\n\n   ​\t小批量梯度下降(`Mini-batch Gradient Descent, MBGD`)\n\n   (2) 梯度下降的变体\n\n   ​\t`Momentum`、`Adagrad`、`Adadelta`、`RMSprop`、`Adam`\n\n   (3) 牛顿法和拟牛顿法\n\n   \n\n​\t\t举例：线性回归的优化算法可以使用梯度下降或最小二乘法\n\n​\t\t在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之后最小\n\n​\t\t求解$w$和$b$使$E(w, b) = \\sum_{i=1}^n(y_i - wx_i -b)$最小化，可以将$E(w, b)$分别对$w$和$b$求导，等于0，可以得到$w$和$b$的值。\n\n\n\n## 机器学习的训练步骤\n\n### 1. 明确问题和目标\n\n​\t需要解决什么问题，达到什么目标\n\n### 2.确定输入，收集数据\n\n​\t通过多种途径得到一个有限的训练数据的集合\n\n\n​\t[Kaggle数据集](https://www.kaggle.com/datasets)\n\n​\t[亚马逊数据集](https://registry.opendata.aws)\n\n​\t[UCI机器学习库](https://archive.ics.uci.edu/ml/datasets.html)\n\n​\t[谷歌的数据集搜素引擎](https://toolbox.google.com/datasetsearch)\n\n​\t[微软数据集](https://msropendata.com/)\n\n​\t[Awesome公共数据集](https://github.com/awesomedata/awesome-public-datasets)\n\n​\t[计算机视觉数据集](https://www.visualdata.io/)\n\n​\t[ImageNet](http://image-net.org/)\n\n​\t[MS COCO](http://cocodataset.org/)\n\n ### 3.确定输出，选择算法\n\n​\t根据输入输出数据的类型决定使用的算法类型，分类还是回归？\n\n​\t输入变量与输出变量均为连续变量的预测问题称为回归问题；\n\n​\t输出变量为有限个离散变量的预测问题称为分类问题；\n\n​\t在对应的算法类型中选择一个或多个算法。\n\n### 4.特征工程\n\n- 数据预处理\n  - 缺失数据---> 删除 和 填充 (平均数，众数)\n  - 处理特征数据---> 正规化 (归一化，正则化，白化)\n  - 处理类别数据--->独热编码\n  - 数据集划分--->训练、验证、测试\n\n- 数据降维\n\n  - 特征选择\n    - 使用L1正则化进行数据稀疏化\n    - 序列特征选择算法  SBS\n    - 通过随机森林判定特征的重要性\n\n  - 特征提取 (将特征压缩到一个低维空间，而不是像特征选择那样完全剔除不相关的特征)\n    - PCA  主成分分析\n    - 线性判别\t\n\n### 5. 建立模型\n\n- 模型空间\n\n- 损失函数\n\n- 优化算法\n\n- 评估标准\n\n  \n\n1. 模型空间\n\n   确定了算法也就确定了模型空间，模型空间包含了算法的所有可能\n\n2. 损失函数\n\n   根据具体的算法和输出决定损坏函数\n\n3. 优化算法\n\n   选择优化算法\n\n4. 评估指标:\n\n![](7.png)\n\n- 分类算法\n  - 准确率\n  - 精确率和召回率(查准率和查全率)\n  - ROC和AUC\n  - $F_1$和$F_{\\beta}$\n\n- 回归算法\n\n  - 平均绝对误差\n\n  - 均方误差\n  - R2分数\n\n### 6.确定最优模型\n\n​\t不断重复**训练模型/评估模型/选择模型**的步骤指导选择最优的模型\n\n- 模型选择数据集划分:\n\n  ​\t\t训练集训练模型\n\n  ​\t\t验证集评估模型\n\n  ​\t\t测试集测试模型\n\n  ​\t\t![](13.png)\n\n- 使用k折交叉验证评估模型性能\n\n  - holdout方法\n  - k折交叉验证\n\n  通常情况下，我们将k折交叉验证用于模型的调优，也就是找到使得模型泛化性能最优的超参值。一旦找到了满意的超参值，我们就可\n  以在全部的训练数据上重新训练模型，并使用独立的测试数据集对模型性能做出最终评价。 \n\n- 通过学习及验证曲线来调试算法\n\n  - 使用学习曲线判定偏差和方差问题\n  - 使用验证曲线判定过拟合与欠拟合\n\n-  使用网格搜索调优机器学习模型\n  - 使用网格搜索调优超参数\n  - 通过嵌套交叉验证选择模型\n  - 网格搜索（grid search），它通过寻找最优的超参值的组合以进一步提高模型的性能。 \n\n\n\n### 7.应用实际问题\n\n​\t利用学习的最优模型对新数据进行预测或分析\n\n\n\n# 三、参考\n\n​\t《统计学习方法》李航\n\n​\t 《Python机器学习》[美] [塞巴斯蒂安·拉施卡]著  高明 徐莹 陶虎成译","slug":"机器学习概述","published":1,"updated":"2019-12-02T08:53:06.519Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tsb000hzsv5hgxv5hke","content":"<h1 id=\"一、人工智能\"><a href=\"#一、人工智能\" class=\"headerlink\" title=\"一、人工智能\"></a>一、人工智能</h1><ol>\n<li><p>什么是人工智能</p>\n<p>​        <img src=\"3.jpg\" alt=\"3\"></p>\n<p>​        人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。<br>​        人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。</p>\n</li>\n<li><p>强人工智能和弱人工智能</p>\n<p>​        早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。<br>​        人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。<br>​        我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类；或者人脸识别。</p>\n</li>\n<li><p>人工智能，机器学习和深度学习的关系</p>\n<p><img src=\"1.png\" alt></p>\n</li>\n</ol>\n<p>   机器学习是人工智能的一种实现方式，也是最重要的实现方式。目前机器学习的方法被大量的应用解决人工智能的问题。</p>\n<p>   深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。</p>\n<p>   总的来说，深度学习是机器学习的一个子集，机器学习是人工智能的一个子集。</p>\n<h1 id=\"二、机器学习\"><a href=\"#二、机器学习\" class=\"headerlink\" title=\"二、机器学习\"></a>二、机器学习</h1><h2 id=\"机器学习的概念\"><a href=\"#机器学习的概念\" class=\"headerlink\" title=\"机器学习的概念\"></a>机器学习的概念</h2><ol>\n<li><p>什么是机器学习</p>\n<p>机器学习就是机器像人类一样学习，人能从过去的经验中学习，对于机器来说过去的经验就是记录的数据。机器理解大量的数据然后归纳出模型来对数据进行预测和分析。</p>\n<p><img src=\"11.png\" alt></p>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>机器学习的对象</p>\n<p>机器学习的对象是数据(data), 它从数据出发，提取数据的特征，抽取数据的模型，发现数据的知识，又回到对数据的分析与预测中去。</p>\n<p>作为机器学习的对象，数据包括各种数字、文字、图像、音频、视频数据以及它们的组合。</p>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>机器学习的目的</p>\n<p>机器学习用于对数据进行预测和分析，特别是对未知新数据进行预测和分析。对数据的预测可以让计算机更加智能化；对数据的分析可以让人们获取新的知识。</p>\n<p>对数据的预测和分析是通过构建模型实现的。机器学习总的目标就是考虑学习什么样的模型和如何学习模型，已使模型对数据进行准确的预测和分析，同时也要尽可能地提升学习的效率。</p>\n</li>\n</ol>\n<ol start=\"4\">\n<li><p>机器学习的分类</p>\n<p>机器学习由监督学习、非监督学习、半监督学习和强化学习等组成。</p>\n<p><img src=\"6.jpg\" alt></p>\n<p>监督学习，非监督学习，半监督学习的区别是训练数据是否有标记。</p>\n</li>\n</ol>\n<ol start=\"5\">\n<li><p>机器学习的应用场景</p>\n<p><img src=\"5.jpg\" alt></p>\n</li>\n</ol>\n<h2 id=\"机器学习三要素\"><a href=\"#机器学习三要素\" class=\"headerlink\" title=\"机器学习三要素\"></a>机器学习三要素</h2><p>​        机器学习的方法由模型，策略，算法构成。</p>\n<ol>\n<li><p>模型</p>\n<p>假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，这个函数的集合就称为假设空间。</p>\n<p>模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的概率分布或决策函数。</p>\n<p>例如：线性回归算法，它的模型就是一个线性函数，即</p>\n<p>$$<br>f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b<br>$$<br>一般用向量形式写成   $f(x) = w^Tx +b$, 其中$w = (w_1, w_2, …, w_3)$.</p>\n<p>$w$和$d$确定之后，模型就确定了。</p>\n<p>根据$w$和$b$的所有取值所组成的集合就是线性回归算法的假设空间。</p>\n</li>\n<li><p>策略</p>\n<p>应用某个评估指标, 从假设空间中选择一个最优的模型。</p>\n<p>对于给定的输入$X$, 模型$f(X)$给出相应的输出$Y$, 这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。记作$L(Y, f(X))$</p>\n<p>机器学习中常用的损失函数有以下几种:</p>\n<p>(1) 0-1损失函数(0-1 loss function)<br>$$<br>L(Y,f(X)) =<br>\\begin{cases}<br>0, &amp; \\text{Y = f(X)}  \\<br>1, &amp; \\text{Y $\\neq$ f(X)}<br>\\end{cases}<br>$$</p>\n<p>(2) 平方损失函数(quadratic loss function)<br>$$<br>L(Y, f(X)) = (Y - f(X))^2<br>$$<br>(3) 绝对损失函数<br>$$<br>L(Y, f(X)) = |Y - f(X)|<br>$$<br>(4) Huber损失—-平滑绝对误差<br>$$<br>L_\\delta(Y,f(X)) =<br>\\begin{cases}<br>{\\frac 12}(y-f(x))^2, &amp; for|y-f(x)|\\le \\delta  \\<br>\\delta|y-f(x)| - {\\frac 12}{\\delta}^2, &amp; \\text{otherwise}<br>\\end{cases}<br>$$</p>\n<p>(5) 对数损失函数(logarithmic loss function)<br>$$<br>L(Y, P(Y|X)) = -logP(Y|X)<br>$$<br>损失函数值越小，模型就越好。<strong>损失函数值最小的模型就是最优模型</strong>。</p>\n</li>\n</ol>\n<p>   举例: 线性回归, $$f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b$$ , 均方误差为<br>   $$<br>   E(w, b) = {\\frac 1n}\\sum_{i=1}^n (f(x_i) - Y)^2<br>   $$<br>   均方误差的几何意义就是欧几里得距离。</p>\n<p>   <img src=\"12.png\" alt></p>\n<p>   <img src=\"14.png\" alt></p>\n<ol start=\"3\">\n<li><p>算法</p>\n<p>算法是指学习模型的具体计算方法。</p>\n<p>机器学习常用优化算法:</p>\n<p>(1) 梯度下降</p>\n<p>​    随机梯度下降(<code>Stochastic Gradient Descent, SGD</code>)</p>\n<p>​    批量梯度下降(<code>Batch Gradient Descent, BGD</code>)</p>\n<p>​    小批量梯度下降(<code>Mini-batch Gradient Descent, MBGD</code>)</p>\n<p>(2) 梯度下降的变体</p>\n<p>​    <code>Momentum</code>、<code>Adagrad</code>、<code>Adadelta</code>、<code>RMSprop</code>、<code>Adam</code></p>\n<p>(3) 牛顿法和拟牛顿法</p>\n</li>\n</ol>\n<p>​        举例：线性回归的优化算法可以使用梯度下降或最小二乘法</p>\n<p>​        在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之后最小</p>\n<p>​        求解$w$和$b$使$E(w, b) = \\sum_{i=1}^n(y_i - wx_i -b)$最小化，可以将$E(w, b)$分别对$w$和$b$求导，等于0，可以得到$w$和$b$的值。</p>\n<h2 id=\"机器学习的训练步骤\"><a href=\"#机器学习的训练步骤\" class=\"headerlink\" title=\"机器学习的训练步骤\"></a>机器学习的训练步骤</h2><h3 id=\"1-明确问题和目标\"><a href=\"#1-明确问题和目标\" class=\"headerlink\" title=\"1. 明确问题和目标\"></a>1. 明确问题和目标</h3><p>​    需要解决什么问题，达到什么目标</p>\n<h3 id=\"2-确定输入，收集数据\"><a href=\"#2-确定输入，收集数据\" class=\"headerlink\" title=\"2.确定输入，收集数据\"></a>2.确定输入，收集数据</h3><p>​    通过多种途径得到一个有限的训练数据的集合</p>\n<p>​    <a href=\"https://www.kaggle.com/datasets\" target=\"_blank\" rel=\"noopener\">Kaggle数据集</a></p>\n<p>​    <a href=\"https://registry.opendata.aws\" target=\"_blank\" rel=\"noopener\">亚马逊数据集</a></p>\n<p>​    <a href=\"https://archive.ics.uci.edu/ml/datasets.html\" target=\"_blank\" rel=\"noopener\">UCI机器学习库</a></p>\n<p>​    <a href=\"https://toolbox.google.com/datasetsearch\" target=\"_blank\" rel=\"noopener\">谷歌的数据集搜素引擎</a></p>\n<p>​    <a href=\"https://msropendata.com/\" target=\"_blank\" rel=\"noopener\">微软数据集</a></p>\n<p>​    <a href=\"https://github.com/awesomedata/awesome-public-datasets\" target=\"_blank\" rel=\"noopener\">Awesome公共数据集</a></p>\n<p>​    <a href=\"https://www.visualdata.io/\" target=\"_blank\" rel=\"noopener\">计算机视觉数据集</a></p>\n<p>​    <a href=\"http://image-net.org/\" target=\"_blank\" rel=\"noopener\">ImageNet</a></p>\n<p>​    <a href=\"http://cocodataset.org/\" target=\"_blank\" rel=\"noopener\">MS COCO</a></p>\n<h3 id=\"3-确定输出，选择算法\"><a href=\"#3-确定输出，选择算法\" class=\"headerlink\" title=\"3.确定输出，选择算法\"></a>3.确定输出，选择算法</h3><p>​    根据输入输出数据的类型决定使用的算法类型，分类还是回归？</p>\n<p>​    输入变量与输出变量均为连续变量的预测问题称为回归问题；</p>\n<p>​    输出变量为有限个离散变量的预测问题称为分类问题；</p>\n<p>​    在对应的算法类型中选择一个或多个算法。</p>\n<h3 id=\"4-特征工程\"><a href=\"#4-特征工程\" class=\"headerlink\" title=\"4.特征工程\"></a>4.特征工程</h3><ul>\n<li><p>数据预处理</p>\n<ul>\n<li>缺失数据—&gt; 删除 和 填充 (平均数，众数)</li>\n<li>处理特征数据—&gt; 正规化 (归一化，正则化，白化)</li>\n<li>处理类别数据—&gt;独热编码</li>\n<li>数据集划分—&gt;训练、验证、测试</li>\n</ul>\n</li>\n<li><p>数据降维</p>\n<ul>\n<li><p>特征选择</p>\n<ul>\n<li>使用L1正则化进行数据稀疏化</li>\n<li>序列特征选择算法  SBS</li>\n<li>通过随机森林判定特征的重要性</li>\n</ul>\n</li>\n<li><p>特征提取 (将特征压缩到一个低维空间，而不是像特征选择那样完全剔除不相关的特征)</p>\n<ul>\n<li>PCA  主成分分析</li>\n<li>线性判别    </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"5-建立模型\"><a href=\"#5-建立模型\" class=\"headerlink\" title=\"5. 建立模型\"></a>5. 建立模型</h3><ul>\n<li><p>模型空间</p>\n</li>\n<li><p>损失函数</p>\n</li>\n<li><p>优化算法</p>\n</li>\n<li><p>评估标准</p>\n</li>\n</ul>\n<ol>\n<li><p>模型空间</p>\n<p>确定了算法也就确定了模型空间，模型空间包含了算法的所有可能</p>\n</li>\n<li><p>损失函数</p>\n<p>根据具体的算法和输出决定损坏函数</p>\n</li>\n<li><p>优化算法</p>\n<p>选择优化算法</p>\n</li>\n<li><p>评估指标:</p>\n</li>\n</ol>\n<p><img src=\"7.png\" alt></p>\n<ul>\n<li><p>分类算法</p>\n<ul>\n<li>准确率</li>\n<li>精确率和召回率(查准率和查全率)</li>\n<li>ROC和AUC</li>\n<li>$F_1$和$F_{\\beta}$</li>\n</ul>\n</li>\n<li><p>回归算法</p>\n<ul>\n<li><p>平均绝对误差</p>\n</li>\n<li><p>均方误差</p>\n</li>\n<li><p>R2分数</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-确定最优模型\"><a href=\"#6-确定最优模型\" class=\"headerlink\" title=\"6.确定最优模型\"></a>6.确定最优模型</h3><p>​    不断重复<strong>训练模型/评估模型/选择模型</strong>的步骤指导选择最优的模型</p>\n<ul>\n<li><p>模型选择数据集划分:</p>\n<p>​        训练集训练模型</p>\n<p>​        验证集评估模型</p>\n<p>​        测试集测试模型</p>\n<p>​        <img src=\"13.png\" alt></p>\n</li>\n<li><p>使用k折交叉验证评估模型性能</p>\n<ul>\n<li>holdout方法</li>\n<li>k折交叉验证</li>\n</ul>\n<p>通常情况下，我们将k折交叉验证用于模型的调优，也就是找到使得模型泛化性能最优的超参值。一旦找到了满意的超参值，我们就可<br>以在全部的训练数据上重新训练模型，并使用独立的测试数据集对模型性能做出最终评价。 </p>\n</li>\n<li><p>通过学习及验证曲线来调试算法</p>\n<ul>\n<li>使用学习曲线判定偏差和方差问题</li>\n<li>使用验证曲线判定过拟合与欠拟合</li>\n</ul>\n</li>\n<li><p>使用网格搜索调优机器学习模型</p>\n<ul>\n<li>使用网格搜索调优超参数</li>\n<li>通过嵌套交叉验证选择模型</li>\n<li>网格搜索（grid search），它通过寻找最优的超参值的组合以进一步提高模型的性能。 </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-应用实际问题\"><a href=\"#7-应用实际问题\" class=\"headerlink\" title=\"7.应用实际问题\"></a>7.应用实际问题</h3><p>​    利用学习的最优模型对新数据进行预测或分析</p>\n<h1 id=\"三、参考\"><a href=\"#三、参考\" class=\"headerlink\" title=\"三、参考\"></a>三、参考</h1><p>​    《统计学习方法》李航</p>\n<p>​     《Python机器学习》[美] [塞巴斯蒂安·拉施卡]著  高明 徐莹 陶虎成译</p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、人工智能\"><a href=\"#一、人工智能\" class=\"headerlink\" title=\"一、人工智能\"></a>一、人工智能</h1><ol>\n<li><p>什么是人工智能</p>\n<p>​        <img src=\"3.jpg\" alt=\"3\"></p>\n<p>​        人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。<br>​        人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。</p>\n</li>\n<li><p>强人工智能和弱人工智能</p>\n<p>​        早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。<br>​        人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。<br>​        我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类；或者人脸识别。</p>\n</li>\n<li><p>人工智能，机器学习和深度学习的关系</p>\n<p><img src=\"1.png\" alt></p>\n</li>\n</ol>\n<p>   机器学习是人工智能的一种实现方式，也是最重要的实现方式。目前机器学习的方法被大量的应用解决人工智能的问题。</p>\n<p>   深度学习是机器学习现在比较火的一个方向，其本身是神经网络算法的衍生，在图像、语音等富媒体的分类和识别上取得了非常好的效果。</p>\n<p>   总的来说，深度学习是机器学习的一个子集，机器学习是人工智能的一个子集。</p>\n<h1 id=\"二、机器学习\"><a href=\"#二、机器学习\" class=\"headerlink\" title=\"二、机器学习\"></a>二、机器学习</h1><h2 id=\"机器学习的概念\"><a href=\"#机器学习的概念\" class=\"headerlink\" title=\"机器学习的概念\"></a>机器学习的概念</h2><ol>\n<li><p>什么是机器学习</p>\n<p>机器学习就是机器像人类一样学习，人能从过去的经验中学习，对于机器来说过去的经验就是记录的数据。机器理解大量的数据然后归纳出模型来对数据进行预测和分析。</p>\n<p><img src=\"11.png\" alt></p>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>机器学习的对象</p>\n<p>机器学习的对象是数据(data), 它从数据出发，提取数据的特征，抽取数据的模型，发现数据的知识，又回到对数据的分析与预测中去。</p>\n<p>作为机器学习的对象，数据包括各种数字、文字、图像、音频、视频数据以及它们的组合。</p>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>机器学习的目的</p>\n<p>机器学习用于对数据进行预测和分析，特别是对未知新数据进行预测和分析。对数据的预测可以让计算机更加智能化；对数据的分析可以让人们获取新的知识。</p>\n<p>对数据的预测和分析是通过构建模型实现的。机器学习总的目标就是考虑学习什么样的模型和如何学习模型，已使模型对数据进行准确的预测和分析，同时也要尽可能地提升学习的效率。</p>\n</li>\n</ol>\n<ol start=\"4\">\n<li><p>机器学习的分类</p>\n<p>机器学习由监督学习、非监督学习、半监督学习和强化学习等组成。</p>\n<p><img src=\"6.jpg\" alt></p>\n<p>监督学习，非监督学习，半监督学习的区别是训练数据是否有标记。</p>\n</li>\n</ol>\n<ol start=\"5\">\n<li><p>机器学习的应用场景</p>\n<p><img src=\"5.jpg\" alt></p>\n</li>\n</ol>\n<h2 id=\"机器学习三要素\"><a href=\"#机器学习三要素\" class=\"headerlink\" title=\"机器学习三要素\"></a>机器学习三要素</h2><p>​        机器学习的方法由模型，策略，算法构成。</p>\n<ol>\n<li><p>模型</p>\n<p>假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，这个函数的集合就称为假设空间。</p>\n<p>模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的概率分布或决策函数。</p>\n<p>例如：线性回归算法，它的模型就是一个线性函数，即</p>\n<p>$$<br>f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b<br>$$<br>一般用向量形式写成   $f(x) = w^Tx +b$, 其中$w = (w_1, w_2, …, w_3)$.</p>\n<p>$w$和$d$确定之后，模型就确定了。</p>\n<p>根据$w$和$b$的所有取值所组成的集合就是线性回归算法的假设空间。</p>\n</li>\n<li><p>策略</p>\n<p>应用某个评估指标, 从假设空间中选择一个最优的模型。</p>\n<p>对于给定的输入$X$, 模型$f(X)$给出相应的输出$Y$, 这个输出的预测值$f(X)$与真实值$Y$可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量预测错误的程度。记作$L(Y, f(X))$</p>\n<p>机器学习中常用的损失函数有以下几种:</p>\n<p>(1) 0-1损失函数(0-1 loss function)<br>$$<br>L(Y,f(X)) =<br>\\begin{cases}<br>0, &amp; \\text{Y = f(X)}  \\<br>1, &amp; \\text{Y $\\neq$ f(X)}<br>\\end{cases}<br>$$</p>\n<p>(2) 平方损失函数(quadratic loss function)<br>$$<br>L(Y, f(X)) = (Y - f(X))^2<br>$$<br>(3) 绝对损失函数<br>$$<br>L(Y, f(X)) = |Y - f(X)|<br>$$<br>(4) Huber损失—-平滑绝对误差<br>$$<br>L_\\delta(Y,f(X)) =<br>\\begin{cases}<br>{\\frac 12}(y-f(x))^2, &amp; for|y-f(x)|\\le \\delta  \\<br>\\delta|y-f(x)| - {\\frac 12}{\\delta}^2, &amp; \\text{otherwise}<br>\\end{cases}<br>$$</p>\n<p>(5) 对数损失函数(logarithmic loss function)<br>$$<br>L(Y, P(Y|X)) = -logP(Y|X)<br>$$<br>损失函数值越小，模型就越好。<strong>损失函数值最小的模型就是最优模型</strong>。</p>\n</li>\n</ol>\n<p>   举例: 线性回归, $$f(x) = w_1x_1 + w_2x_2 + … + w_nx_n + b$$ , 均方误差为<br>   $$<br>   E(w, b) = {\\frac 1n}\\sum_{i=1}^n (f(x_i) - Y)^2<br>   $$<br>   均方误差的几何意义就是欧几里得距离。</p>\n<p>   <img src=\"12.png\" alt></p>\n<p>   <img src=\"14.png\" alt></p>\n<ol start=\"3\">\n<li><p>算法</p>\n<p>算法是指学习模型的具体计算方法。</p>\n<p>机器学习常用优化算法:</p>\n<p>(1) 梯度下降</p>\n<p>​    随机梯度下降(<code>Stochastic Gradient Descent, SGD</code>)</p>\n<p>​    批量梯度下降(<code>Batch Gradient Descent, BGD</code>)</p>\n<p>​    小批量梯度下降(<code>Mini-batch Gradient Descent, MBGD</code>)</p>\n<p>(2) 梯度下降的变体</p>\n<p>​    <code>Momentum</code>、<code>Adagrad</code>、<code>Adadelta</code>、<code>RMSprop</code>、<code>Adam</code></p>\n<p>(3) 牛顿法和拟牛顿法</p>\n</li>\n</ol>\n<p>​        举例：线性回归的优化算法可以使用梯度下降或最小二乘法</p>\n<p>​        在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之后最小</p>\n<p>​        求解$w$和$b$使$E(w, b) = \\sum_{i=1}^n(y_i - wx_i -b)$最小化，可以将$E(w, b)$分别对$w$和$b$求导，等于0，可以得到$w$和$b$的值。</p>\n<h2 id=\"机器学习的训练步骤\"><a href=\"#机器学习的训练步骤\" class=\"headerlink\" title=\"机器学习的训练步骤\"></a>机器学习的训练步骤</h2><h3 id=\"1-明确问题和目标\"><a href=\"#1-明确问题和目标\" class=\"headerlink\" title=\"1. 明确问题和目标\"></a>1. 明确问题和目标</h3><p>​    需要解决什么问题，达到什么目标</p>\n<h3 id=\"2-确定输入，收集数据\"><a href=\"#2-确定输入，收集数据\" class=\"headerlink\" title=\"2.确定输入，收集数据\"></a>2.确定输入，收集数据</h3><p>​    通过多种途径得到一个有限的训练数据的集合</p>\n<p>​    <a href=\"https://www.kaggle.com/datasets\" target=\"_blank\" rel=\"noopener\">Kaggle数据集</a></p>\n<p>​    <a href=\"https://registry.opendata.aws\" target=\"_blank\" rel=\"noopener\">亚马逊数据集</a></p>\n<p>​    <a href=\"https://archive.ics.uci.edu/ml/datasets.html\" target=\"_blank\" rel=\"noopener\">UCI机器学习库</a></p>\n<p>​    <a href=\"https://toolbox.google.com/datasetsearch\" target=\"_blank\" rel=\"noopener\">谷歌的数据集搜素引擎</a></p>\n<p>​    <a href=\"https://msropendata.com/\" target=\"_blank\" rel=\"noopener\">微软数据集</a></p>\n<p>​    <a href=\"https://github.com/awesomedata/awesome-public-datasets\" target=\"_blank\" rel=\"noopener\">Awesome公共数据集</a></p>\n<p>​    <a href=\"https://www.visualdata.io/\" target=\"_blank\" rel=\"noopener\">计算机视觉数据集</a></p>\n<p>​    <a href=\"http://image-net.org/\" target=\"_blank\" rel=\"noopener\">ImageNet</a></p>\n<p>​    <a href=\"http://cocodataset.org/\" target=\"_blank\" rel=\"noopener\">MS COCO</a></p>\n<h3 id=\"3-确定输出，选择算法\"><a href=\"#3-确定输出，选择算法\" class=\"headerlink\" title=\"3.确定输出，选择算法\"></a>3.确定输出，选择算法</h3><p>​    根据输入输出数据的类型决定使用的算法类型，分类还是回归？</p>\n<p>​    输入变量与输出变量均为连续变量的预测问题称为回归问题；</p>\n<p>​    输出变量为有限个离散变量的预测问题称为分类问题；</p>\n<p>​    在对应的算法类型中选择一个或多个算法。</p>\n<h3 id=\"4-特征工程\"><a href=\"#4-特征工程\" class=\"headerlink\" title=\"4.特征工程\"></a>4.特征工程</h3><ul>\n<li><p>数据预处理</p>\n<ul>\n<li>缺失数据—&gt; 删除 和 填充 (平均数，众数)</li>\n<li>处理特征数据—&gt; 正规化 (归一化，正则化，白化)</li>\n<li>处理类别数据—&gt;独热编码</li>\n<li>数据集划分—&gt;训练、验证、测试</li>\n</ul>\n</li>\n<li><p>数据降维</p>\n<ul>\n<li><p>特征选择</p>\n<ul>\n<li>使用L1正则化进行数据稀疏化</li>\n<li>序列特征选择算法  SBS</li>\n<li>通过随机森林判定特征的重要性</li>\n</ul>\n</li>\n<li><p>特征提取 (将特征压缩到一个低维空间，而不是像特征选择那样完全剔除不相关的特征)</p>\n<ul>\n<li>PCA  主成分分析</li>\n<li>线性判别    </li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"5-建立模型\"><a href=\"#5-建立模型\" class=\"headerlink\" title=\"5. 建立模型\"></a>5. 建立模型</h3><ul>\n<li><p>模型空间</p>\n</li>\n<li><p>损失函数</p>\n</li>\n<li><p>优化算法</p>\n</li>\n<li><p>评估标准</p>\n</li>\n</ul>\n<ol>\n<li><p>模型空间</p>\n<p>确定了算法也就确定了模型空间，模型空间包含了算法的所有可能</p>\n</li>\n<li><p>损失函数</p>\n<p>根据具体的算法和输出决定损坏函数</p>\n</li>\n<li><p>优化算法</p>\n<p>选择优化算法</p>\n</li>\n<li><p>评估指标:</p>\n</li>\n</ol>\n<p><img src=\"7.png\" alt></p>\n<ul>\n<li><p>分类算法</p>\n<ul>\n<li>准确率</li>\n<li>精确率和召回率(查准率和查全率)</li>\n<li>ROC和AUC</li>\n<li>$F_1$和$F_{\\beta}$</li>\n</ul>\n</li>\n<li><p>回归算法</p>\n<ul>\n<li><p>平均绝对误差</p>\n</li>\n<li><p>均方误差</p>\n</li>\n<li><p>R2分数</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-确定最优模型\"><a href=\"#6-确定最优模型\" class=\"headerlink\" title=\"6.确定最优模型\"></a>6.确定最优模型</h3><p>​    不断重复<strong>训练模型/评估模型/选择模型</strong>的步骤指导选择最优的模型</p>\n<ul>\n<li><p>模型选择数据集划分:</p>\n<p>​        训练集训练模型</p>\n<p>​        验证集评估模型</p>\n<p>​        测试集测试模型</p>\n<p>​        <img src=\"13.png\" alt></p>\n</li>\n<li><p>使用k折交叉验证评估模型性能</p>\n<ul>\n<li>holdout方法</li>\n<li>k折交叉验证</li>\n</ul>\n<p>通常情况下，我们将k折交叉验证用于模型的调优，也就是找到使得模型泛化性能最优的超参值。一旦找到了满意的超参值，我们就可<br>以在全部的训练数据上重新训练模型，并使用独立的测试数据集对模型性能做出最终评价。 </p>\n</li>\n<li><p>通过学习及验证曲线来调试算法</p>\n<ul>\n<li>使用学习曲线判定偏差和方差问题</li>\n<li>使用验证曲线判定过拟合与欠拟合</li>\n</ul>\n</li>\n<li><p>使用网格搜索调优机器学习模型</p>\n<ul>\n<li>使用网格搜索调优超参数</li>\n<li>通过嵌套交叉验证选择模型</li>\n<li>网格搜索（grid search），它通过寻找最优的超参值的组合以进一步提高模型的性能。 </li>\n</ul>\n</li>\n</ul>\n<h3 id=\"7-应用实际问题\"><a href=\"#7-应用实际问题\" class=\"headerlink\" title=\"7.应用实际问题\"></a>7.应用实际问题</h3><p>​    利用学习的最优模型对新数据进行预测或分析</p>\n<h1 id=\"三、参考\"><a href=\"#三、参考\" class=\"headerlink\" title=\"三、参考\"></a>三、参考</h1><p>​    《统计学习方法》李航</p>\n<p>​     《Python机器学习》[美] [塞巴斯蒂安·拉施卡]著  高明 徐莹 陶虎成译</p>\n"},{"title":"机器学习之评估指标","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2020-01-04T06:48:57.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、分类模型\n\n## 1、混淆矩阵\n\n在机器学习领域，混淆矩阵(confusion matrix), 又称为可能性表格或者错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）。其每一列代表预测值，每一行代表的是实际的类别。\n\n![](混淆矩阵.png)\n\n```python\nfrom skleran.metrics import confusion_matrix\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n```\n\n\n\n## 2、预测误差和准确率\n\n预测误差（error，ERR）和准确率（accuracy，ACC）都提供了误分类样本数量的相关信息。误差可以理解为预测错误样本的数量与所有被预测样本数量的比值，而准确率计算方法则是正确预测样本的数量与所有被预测样本数量的比值:\n$$\nERR = \\frac {FP + FN}{FP + FN + TP + TN}\n$$\n预测准确率也可以通过误差直接计算： \n$$\nACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR\n$$\n\n```python\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n```\n\n对于类别数量不均衡的分类问题来说，真正率（TPR）与假正率（FPR）是非常有用的性能指标： \n$$\nFPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\n$$\n\n$$\nTPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n$$\n\n## 3、精确率和召回率\n\n**精确率**是针对我们**预测结果**而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是:\n$$\nP = \\frac{TP}{TP + FP}\n$$\n\n\n而**召回率**是针对我们原来的**样本**而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。\n$$\nR = \\frac {TP}{TP + FN}\n$$\n\n\n在信息检索领域，精确率和召回率又被称为**查准率**和**查全率**，\n\n查准率＝检索出的相关信息量 / 检索出的信息总量\n查全率＝检索出的相关信息量 / 系统中的相关信息总量  \n\n\n\n## 4、敏感性和特异性\n\n医疗领域的混淆矩阵:\n\n![](02.jpg)\n\n\n\n敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记\n\n- TP：（真阳性）被**正确**诊断为患病的病人。\n- TN：（真阴性）被**正确**诊断为健康的健康人。\n- FP：（假阳性）被**错误**诊断为患病的健康人。\n- FN：（假阴性）被**错误**诊断为健康的病人。\n\n那么：\n$$\n敏感性 =\\frac{TP}{TP+FN}\n$$\n\n$$\n特异性 = \\frac{TN}{TN+FP}\n$$\n\n\n\n**敏感性**和**特异性**虽然与**查准率**和**查全率**相似，但并不相同。其定义如下：\n\n在癌症示例中，敏感性和特异性指：\n\n- 敏感性：在**患有**癌症的所有人中，诊断正确的人有多少？\n- 特异性：在**未患**癌症的所有人中，诊断正确的人有多少？\n\n查准率和查全率的定义如下：\n\n- 查全率：在**被诊断**患有癌症的所有人中，多少人确实**得了癌症**？\n- 查准率：在**患有癌症**的所有人中，多少人**被诊断**患有癌症？\n\n从这里可以看出，敏感性就是查全率，但特异性并不是查准率。\n\n## 5、$F_1$和$F_\\beta$\n\n把精确率和召回率合成一个值，根据精确率和召回率权重的不同可以分为$F_1$和$F_\\beta$\n\n算术平均数  Arithmetic Mean = (x+y)/2\n调和平均数  Harmonic Mean = (2xy)/(x+y)\n调和平均数始终小于算术平均数，因为调和平均数更接近较小的数\n\n$F_1$值的计算公式如下:\n$$\nF_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2*precision*recall}{presicion + recall}\n$$\n$F_1$值就是精确率和召回率的调和平均值，$F_1$值认为精确率和召回率一样重要。\n\n\n\n$F_\\beta$值的计算公式如下:\n$$\nF_1 = \\frac{1 + \\beta^2}{\\frac{1}{precision} + \\frac{\\beta^2}{recall}} = \\frac{(1+\\beta^2)*precision*recall}{\\beta^2*presicion + recall}\n$$\nβ 的界限在 0 和 ∞ 之间。如果β=0, 则得出精度。如果β=∞, 则得出召回率\n在β=1时，$F_β$就是$F_1$值，此时$F_β$认为精确率和召回率一样重要；当β>1时，$F_1$认为召回率更重要；当0<β<1时，认为精确$F_β$率更重要。除了$F_1$值之外，常用的还有$F_2$和$F_{0.5}$。\n\n## 6、ROC曲线及AUC值\n\nAUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图所示：\n\n![](02.png)\n\n　ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive}$，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $recall_{negative}$)如下所示：\n$$\nTPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = recall_{positive}\n$$\n\n$$\nFPR = \\frac{FP}{FP + TN} = \\frac{FP+TN-TN}{FP+TN} = 1 - \\frac{TN}{FP + TN} = 1 - recall_{negative}\n$$\n\n\n\n通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θ取值范围为[0,1]。\n\n　　ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive}$值，横坐标FPR就是(1 - $recall_{negative}$)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。\n\n　　图展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。\n\n\n\nROC曲线上几个关键点的解释： \n\t\t( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型 \n\t\t( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型 \n\t\t( TPR=1,FPR=0 ) 理想模型\n\n\n\n# 二、回归模型\n\n## 1、平均绝对误差(Mean Absolute Error)\n\n```python\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_absolute_error(y, guesses)\n```\n\nMAE问题：绝对值函数是不可微分的，不利于使用梯度下降等方法\n解决：均方误差\n\n## 2、平均平方误差(Mean Squared Error)\n\n```python\nfrom sklearn.metrics import mean_square_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_square_error(y, guesses)\n```\n\n\n\n## 3、决定系数($R^2$ )\n\n$R^2$分数通过将我们的模型与最简单的可能模型相比得出。那么思考一下，拟合一堆点的最简单的可能模型是什么呢？那就是取所有值的平均值,然后我们可以计算出这个模型的均方误差。\n\tR2 = 1 - (线性回归模型误差/简单模型误差)\n\t如果这个模型不太好 这两个误差将很接近而这个量应接近 1，那么整个 R2 分数应接近 0。\n\t如果模型较好 那么线性回归模型对的均方误差应比简单模型的均方误差小很多，那么这个比例就很小。而 R2 分数将非常接近 1，总结来说 如果 R2 分数接近 1 模型就不错。\n\t\n\n\n```python\nfrom sklearn.metrics import r2_score\nr2_score(y_true, y_pred)\n```\n\n\n# 三、参考\n\nhttps://www.zhihu.com/question/19645541/answer/91694636\n\nhttps://www.zhihu.com/question/30750849\n\nhttps://www.cnblogs.com/wuliytTaotao/p/9285227.html\n\nhttps://www.cnblogs.com/gatherstars/p/6084696.html\n\nhttps://blog.csdn.net/weixin_41043240/article/details/80265577","source":"_posts/机器学习之评估指标.md","raw":"---\ntitle: 机器学习之评估指标\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2020-01-04 14:48:57\npassword:\nsummary:\ntags:\n- ML\n- DL\ncategories: 机器学习\n---\n\n\n\n# 一、分类模型\n\n## 1、混淆矩阵\n\n在机器学习领域，混淆矩阵(confusion matrix), 又称为可能性表格或者错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）。其每一列代表预测值，每一行代表的是实际的类别。\n\n![](混淆矩阵.png)\n\n```python\nfrom skleran.metrics import confusion_matrix\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)\n```\n\n\n\n## 2、预测误差和准确率\n\n预测误差（error，ERR）和准确率（accuracy，ACC）都提供了误分类样本数量的相关信息。误差可以理解为预测错误样本的数量与所有被预测样本数量的比值，而准确率计算方法则是正确预测样本的数量与所有被预测样本数量的比值:\n$$\nERR = \\frac {FP + FN}{FP + FN + TP + TN}\n$$\n预测准确率也可以通过误差直接计算： \n$$\nACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR\n$$\n\n```python\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n```\n\n对于类别数量不均衡的分类问题来说，真正率（TPR）与假正率（FPR）是非常有用的性能指标： \n$$\nFPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}\n$$\n\n$$\nTPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n$$\n\n## 3、精确率和召回率\n\n**精确率**是针对我们**预测结果**而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是:\n$$\nP = \\frac{TP}{TP + FP}\n$$\n\n\n而**召回率**是针对我们原来的**样本**而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。\n$$\nR = \\frac {TP}{TP + FN}\n$$\n\n\n在信息检索领域，精确率和召回率又被称为**查准率**和**查全率**，\n\n查准率＝检索出的相关信息量 / 检索出的信息总量\n查全率＝检索出的相关信息量 / 系统中的相关信息总量  \n\n\n\n## 4、敏感性和特异性\n\n医疗领域的混淆矩阵:\n\n![](02.jpg)\n\n\n\n敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记\n\n- TP：（真阳性）被**正确**诊断为患病的病人。\n- TN：（真阴性）被**正确**诊断为健康的健康人。\n- FP：（假阳性）被**错误**诊断为患病的健康人。\n- FN：（假阴性）被**错误**诊断为健康的病人。\n\n那么：\n$$\n敏感性 =\\frac{TP}{TP+FN}\n$$\n\n$$\n特异性 = \\frac{TN}{TN+FP}\n$$\n\n\n\n**敏感性**和**特异性**虽然与**查准率**和**查全率**相似，但并不相同。其定义如下：\n\n在癌症示例中，敏感性和特异性指：\n\n- 敏感性：在**患有**癌症的所有人中，诊断正确的人有多少？\n- 特异性：在**未患**癌症的所有人中，诊断正确的人有多少？\n\n查准率和查全率的定义如下：\n\n- 查全率：在**被诊断**患有癌症的所有人中，多少人确实**得了癌症**？\n- 查准率：在**患有癌症**的所有人中，多少人**被诊断**患有癌症？\n\n从这里可以看出，敏感性就是查全率，但特异性并不是查准率。\n\n## 5、$F_1$和$F_\\beta$\n\n把精确率和召回率合成一个值，根据精确率和召回率权重的不同可以分为$F_1$和$F_\\beta$\n\n算术平均数  Arithmetic Mean = (x+y)/2\n调和平均数  Harmonic Mean = (2xy)/(x+y)\n调和平均数始终小于算术平均数，因为调和平均数更接近较小的数\n\n$F_1$值的计算公式如下:\n$$\nF_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2*precision*recall}{presicion + recall}\n$$\n$F_1$值就是精确率和召回率的调和平均值，$F_1$值认为精确率和召回率一样重要。\n\n\n\n$F_\\beta$值的计算公式如下:\n$$\nF_1 = \\frac{1 + \\beta^2}{\\frac{1}{precision} + \\frac{\\beta^2}{recall}} = \\frac{(1+\\beta^2)*precision*recall}{\\beta^2*presicion + recall}\n$$\nβ 的界限在 0 和 ∞ 之间。如果β=0, 则得出精度。如果β=∞, 则得出召回率\n在β=1时，$F_β$就是$F_1$值，此时$F_β$认为精确率和召回率一样重要；当β>1时，$F_1$认为召回率更重要；当0<β<1时，认为精确$F_β$率更重要。除了$F_1$值之外，常用的还有$F_2$和$F_{0.5}$。\n\n## 6、ROC曲线及AUC值\n\nAUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图所示：\n\n![](02.png)\n\n　ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive}$，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $recall_{negative}$)如下所示：\n$$\nTPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = recall_{positive}\n$$\n\n$$\nFPR = \\frac{FP}{FP + TN} = \\frac{FP+TN-TN}{FP+TN} = 1 - \\frac{TN}{FP + TN} = 1 - recall_{negative}\n$$\n\n\n\n通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θ取值范围为[0,1]。\n\n　　ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive}$值，横坐标FPR就是(1 - $recall_{negative}$)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。\n\n　　图展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。\n\n\n\nROC曲线上几个关键点的解释： \n\t\t( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型 \n\t\t( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型 \n\t\t( TPR=1,FPR=0 ) 理想模型\n\n\n\n# 二、回归模型\n\n## 1、平均绝对误差(Mean Absolute Error)\n\n```python\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_absolute_error(y, guesses)\n```\n\nMAE问题：绝对值函数是不可微分的，不利于使用梯度下降等方法\n解决：均方误差\n\n## 2、平均平方误差(Mean Squared Error)\n\n```python\nfrom sklearn.metrics import mean_square_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_square_error(y, guesses)\n```\n\n\n\n## 3、决定系数($R^2$ )\n\n$R^2$分数通过将我们的模型与最简单的可能模型相比得出。那么思考一下，拟合一堆点的最简单的可能模型是什么呢？那就是取所有值的平均值,然后我们可以计算出这个模型的均方误差。\n\tR2 = 1 - (线性回归模型误差/简单模型误差)\n\t如果这个模型不太好 这两个误差将很接近而这个量应接近 1，那么整个 R2 分数应接近 0。\n\t如果模型较好 那么线性回归模型对的均方误差应比简单模型的均方误差小很多，那么这个比例就很小。而 R2 分数将非常接近 1，总结来说 如果 R2 分数接近 1 模型就不错。\n\t\n\n\n```python\nfrom sklearn.metrics import r2_score\nr2_score(y_true, y_pred)\n```\n\n\n# 三、参考\n\nhttps://www.zhihu.com/question/19645541/answer/91694636\n\nhttps://www.zhihu.com/question/30750849\n\nhttps://www.cnblogs.com/wuliytTaotao/p/9285227.html\n\nhttps://www.cnblogs.com/gatherstars/p/6084696.html\n\nhttps://blog.csdn.net/weixin_41043240/article/details/80265577","slug":"机器学习之评估指标","published":1,"updated":"2020-01-04T11:49:41.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tse000lzsv5bvsgyyi2","content":"<h1 id=\"一、分类模型\"><a href=\"#一、分类模型\" class=\"headerlink\" title=\"一、分类模型\"></a>一、分类模型</h1><h2 id=\"1、混淆矩阵\"><a href=\"#1、混淆矩阵\" class=\"headerlink\" title=\"1、混淆矩阵\"></a>1、混淆矩阵</h2><p>在机器学习领域，混淆矩阵(confusion matrix), 又称为可能性表格或者错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）。其每一列代表预测值，每一行代表的是实际的类别。</p>\n<p><img src=\"%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png\" alt></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> skleran<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> confusion_matrix\nconfmat <span class=\"token operator\">=</span> confusion_matrix<span class=\"token punctuation\">(</span>y_true<span class=\"token operator\">=</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token operator\">=</span>y_pred<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>confmat<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"2、预测误差和准确率\"><a href=\"#2、预测误差和准确率\" class=\"headerlink\" title=\"2、预测误差和准确率\"></a>2、预测误差和准确率</h2><p>预测误差（error，ERR）和准确率（accuracy，ACC）都提供了误分类样本数量的相关信息。误差可以理解为预测错误样本的数量与所有被预测样本数量的比值，而准确率计算方法则是正确预测样本的数量与所有被预测样本数量的比值:<br>$$<br>ERR = \\frac {FP + FN}{FP + FN + TP + TN}<br>$$<br>预测准确率也可以通过误差直接计算：<br>$$<br>ACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR<br>$$</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> accuracy_score\naccuracy_score<span class=\"token punctuation\">(</span>y_true<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p>对于类别数量不均衡的分类问题来说，真正率（TPR）与假正率（FPR）是非常有用的性能指标：<br>$$<br>FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}<br>$$</p>\n<p>$$<br>TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}<br>$$</p>\n<h2 id=\"3、精确率和召回率\"><a href=\"#3、精确率和召回率\" class=\"headerlink\" title=\"3、精确率和召回率\"></a>3、精确率和召回率</h2><p><strong>精确率</strong>是针对我们<strong>预测结果</strong>而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是:<br>$$<br>P = \\frac{TP}{TP + FP}<br>$$</p>\n<p>而<strong>召回率</strong>是针对我们原来的<strong>样本</strong>而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。<br>$$<br>R = \\frac {TP}{TP + FN}<br>$$</p>\n<p>在信息检索领域，精确率和召回率又被称为<strong>查准率</strong>和<strong>查全率</strong>，</p>\n<p>查准率＝检索出的相关信息量 / 检索出的信息总量<br>查全率＝检索出的相关信息量 / 系统中的相关信息总量  </p>\n<h2 id=\"4、敏感性和特异性\"><a href=\"#4、敏感性和特异性\" class=\"headerlink\" title=\"4、敏感性和特异性\"></a>4、敏感性和特异性</h2><p>医疗领域的混淆矩阵:</p>\n<p><img src=\"02.jpg\" alt></p>\n<p>敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记</p>\n<ul>\n<li>TP：（真阳性）被<strong>正确</strong>诊断为患病的病人。</li>\n<li>TN：（真阴性）被<strong>正确</strong>诊断为健康的健康人。</li>\n<li>FP：（假阳性）被<strong>错误</strong>诊断为患病的健康人。</li>\n<li>FN：（假阴性）被<strong>错误</strong>诊断为健康的病人。</li>\n</ul>\n<p>那么：<br>$$<br>敏感性 =\\frac{TP}{TP+FN}<br>$$</p>\n<p>$$<br>特异性 = \\frac{TN}{TN+FP}<br>$$</p>\n<p><strong>敏感性</strong>和<strong>特异性</strong>虽然与<strong>查准率</strong>和<strong>查全率</strong>相似，但并不相同。其定义如下：</p>\n<p>在癌症示例中，敏感性和特异性指：</p>\n<ul>\n<li>敏感性：在<strong>患有</strong>癌症的所有人中，诊断正确的人有多少？</li>\n<li>特异性：在<strong>未患</strong>癌症的所有人中，诊断正确的人有多少？</li>\n</ul>\n<p>查准率和查全率的定义如下：</p>\n<ul>\n<li>查全率：在<strong>被诊断</strong>患有癌症的所有人中，多少人确实<strong>得了癌症</strong>？</li>\n<li>查准率：在<strong>患有癌症</strong>的所有人中，多少人<strong>被诊断</strong>患有癌症？</li>\n</ul>\n<p>从这里可以看出，敏感性就是查全率，但特异性并不是查准率。</p>\n<h2 id=\"5、-F-1-和-F-beta\"><a href=\"#5、-F-1-和-F-beta\" class=\"headerlink\" title=\"5、$F_1$和$F_\\beta$\"></a>5、$F_1$和$F_\\beta$</h2><p>把精确率和召回率合成一个值，根据精确率和召回率权重的不同可以分为$F_1$和$F_\\beta$</p>\n<p>算术平均数  Arithmetic Mean = (x+y)/2<br>调和平均数  Harmonic Mean = (2xy)/(x+y)<br>调和平均数始终小于算术平均数，因为调和平均数更接近较小的数</p>\n<p>$F_1$值的计算公式如下:<br>$$<br>F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2<em>precision</em>recall}{presicion + recall}<br>$$<br>$F_1$值就是精确率和召回率的调和平均值，$F_1$值认为精确率和召回率一样重要。</p>\n<p>$F_\\beta$值的计算公式如下:<br>$$<br>F_1 = \\frac{1 + \\beta^2}{\\frac{1}{precision} + \\frac{\\beta^2}{recall}} = \\frac{(1+\\beta^2)<em>precision</em>recall}{\\beta^2*presicion + recall}<br>$$<br>β 的界限在 0 和 ∞ 之间。如果β=0, 则得出精度。如果β=∞, 则得出召回率<br>在β=1时，$F_β$就是$F_1$值，此时$F_β$认为精确率和召回率一样重要；当β&gt;1时，$F_1$认为召回率更重要；当0&lt;β&lt;1时，认为精确$F_β$率更重要。除了$F_1$值之外，常用的还有$F_2$和$F_{0.5}$。</p>\n<h2 id=\"6、ROC曲线及AUC值\"><a href=\"#6、ROC曲线及AUC值\" class=\"headerlink\" title=\"6、ROC曲线及AUC值\"></a>6、ROC曲线及AUC值</h2><p>AUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图所示：</p>\n<p><img src=\"02.png\" alt></p>\n<p>　ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive}$，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $recall_{negative}$)如下所示：<br>$$<br>TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = recall_{positive}<br>$$</p>\n<p>$$<br>FPR = \\frac{FP}{FP + TN} = \\frac{FP+TN-TN}{FP+TN} = 1 - \\frac{TN}{FP + TN} = 1 - recall_{negative}<br>$$</p>\n<p>通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θ取值范围为[0,1]。</p>\n<p>　　ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive}$值，横坐标FPR就是(1 - $recall_{negative}$)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。</p>\n<p>　　图展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。</p>\n<p>ROC曲线上几个关键点的解释：<br>        ( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型<br>        ( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型<br>        ( TPR=1,FPR=0 ) 理想模型</p>\n<h1 id=\"二、回归模型\"><a href=\"#二、回归模型\" class=\"headerlink\" title=\"二、回归模型\"></a>二、回归模型</h1><h2 id=\"1、平均绝对误差-Mean-Absolute-Error\"><a href=\"#1、平均绝对误差-Mean-Absolute-Error\" class=\"headerlink\" title=\"1、平均绝对误差(Mean Absolute Error)\"></a>1、平均绝对误差(Mean Absolute Error)</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_absolute_error\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRression\nclassifier <span class=\"token operator\">=</span> LinearRression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclassifier<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\nguesses <span class=\"token operator\">=</span> classifier<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nerror <span class=\"token operator\">=</span> mean_absolute_error<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> guesses<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>MAE问题：绝对值函数是不可微分的，不利于使用梯度下降等方法<br>解决：均方误差</p>\n<h2 id=\"2、平均平方误差-Mean-Squared-Error\"><a href=\"#2、平均平方误差-Mean-Squared-Error\" class=\"headerlink\" title=\"2、平均平方误差(Mean Squared Error)\"></a>2、平均平方误差(Mean Squared Error)</h2><pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> mean_square_error\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRression\nclassifier <span class=\"token operator\">=</span> LinearRression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nclassifier<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">)</span>\nguesses <span class=\"token operator\">=</span> classifier<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\nerror <span class=\"token operator\">=</span> mean_square_error<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">,</span> guesses<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"3、决定系数-R-2\"><a href=\"#3、决定系数-R-2\" class=\"headerlink\" title=\"3、决定系数($R^2$ )\"></a>3、决定系数($R^2$ )</h2><p>$R^2$分数通过将我们的模型与最简单的可能模型相比得出。那么思考一下，拟合一堆点的最简单的可能模型是什么呢？那就是取所有值的平均值,然后我们可以计算出这个模型的均方误差。<br>    R2 = 1 - (线性回归模型误差/简单模型误差)<br>    如果这个模型不太好 这两个误差将很接近而这个量应接近 1，那么整个 R2 分数应接近 0。<br>    如果模型较好 那么线性回归模型对的均方误差应比简单模型的均方误差小很多，那么这个比例就很小。而 R2 分数将非常接近 1，总结来说 如果 R2 分数接近 1 模型就不错。</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> r2_score\nr2_score<span class=\"token punctuation\">(</span>y_true<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<h1 id=\"三、参考\"><a href=\"#三、参考\" class=\"headerlink\" title=\"三、参考\"></a>三、参考</h1><p><a href=\"https://www.zhihu.com/question/19645541/answer/91694636\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/19645541/answer/91694636</a></p>\n<p><a href=\"https://www.zhihu.com/question/30750849\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/30750849</a></p>\n<p><a href=\"https://www.cnblogs.com/wuliytTaotao/p/9285227.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/wuliytTaotao/p/9285227.html</a></p>\n<p><a href=\"https://www.cnblogs.com/gatherstars/p/6084696.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/gatherstars/p/6084696.html</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_41043240/article/details/80265577\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/weixin_41043240/article/details/80265577</a></p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、分类模型\"><a href=\"#一、分类模型\" class=\"headerlink\" title=\"一、分类模型\"></a>一、分类模型</h1><h2 id=\"1、混淆矩阵\"><a href=\"#1、混淆矩阵\" class=\"headerlink\" title=\"1、混淆矩阵\"></a>1、混淆矩阵</h2><p>在机器学习领域，混淆矩阵(confusion matrix), 又称为可能性表格或者错误矩阵。它是一种特定的矩阵用来呈现算法性能的可视化效果，通常是监督学习（非监督学习，通常用匹配矩阵：matching matrix）。其每一列代表预测值，每一行代表的是实际的类别。</p>\n<p><img src=\"%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png\" alt></p>\n<pre><code class=\"python\">from skleran.metrics import confusion_matrix\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)</code></pre>\n<h2 id=\"2、预测误差和准确率\"><a href=\"#2、预测误差和准确率\" class=\"headerlink\" title=\"2、预测误差和准确率\"></a>2、预测误差和准确率</h2><p>预测误差（error，ERR）和准确率（accuracy，ACC）都提供了误分类样本数量的相关信息。误差可以理解为预测错误样本的数量与所有被预测样本数量的比值，而准确率计算方法则是正确预测样本的数量与所有被预测样本数量的比值:<br>$$<br>ERR = \\frac {FP + FN}{FP + FN + TP + TN}<br>$$<br>预测准确率也可以通过误差直接计算：<br>$$<br>ACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR<br>$$</p>\n<pre><code class=\"python\">from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)</code></pre>\n<p>对于类别数量不均衡的分类问题来说，真正率（TPR）与假正率（FPR）是非常有用的性能指标：<br>$$<br>FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}<br>$$</p>\n<p>$$<br>TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}<br>$$</p>\n<h2 id=\"3、精确率和召回率\"><a href=\"#3、精确率和召回率\" class=\"headerlink\" title=\"3、精确率和召回率\"></a>3、精确率和召回率</h2><p><strong>精确率</strong>是针对我们<strong>预测结果</strong>而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是:<br>$$<br>P = \\frac{TP}{TP + FP}<br>$$</p>\n<p>而<strong>召回率</strong>是针对我们原来的<strong>样本</strong>而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。<br>$$<br>R = \\frac {TP}{TP + FN}<br>$$</p>\n<p>在信息检索领域，精确率和召回率又被称为<strong>查准率</strong>和<strong>查全率</strong>，</p>\n<p>查准率＝检索出的相关信息量 / 检索出的信息总量<br>查全率＝检索出的相关信息量 / 系统中的相关信息总量  </p>\n<h2 id=\"4、敏感性和特异性\"><a href=\"#4、敏感性和特异性\" class=\"headerlink\" title=\"4、敏感性和特异性\"></a>4、敏感性和特异性</h2><p>医疗领域的混淆矩阵:</p>\n<p><img src=\"02.jpg\" alt></p>\n<p>敏感性和特异性是这个矩阵中的行。更具体地说，如果我们做以下标记</p>\n<ul>\n<li>TP：（真阳性）被<strong>正确</strong>诊断为患病的病人。</li>\n<li>TN：（真阴性）被<strong>正确</strong>诊断为健康的健康人。</li>\n<li>FP：（假阳性）被<strong>错误</strong>诊断为患病的健康人。</li>\n<li>FN：（假阴性）被<strong>错误</strong>诊断为健康的病人。</li>\n</ul>\n<p>那么：<br>$$<br>敏感性 =\\frac{TP}{TP+FN}<br>$$</p>\n<p>$$<br>特异性 = \\frac{TN}{TN+FP}<br>$$</p>\n<p><strong>敏感性</strong>和<strong>特异性</strong>虽然与<strong>查准率</strong>和<strong>查全率</strong>相似，但并不相同。其定义如下：</p>\n<p>在癌症示例中，敏感性和特异性指：</p>\n<ul>\n<li>敏感性：在<strong>患有</strong>癌症的所有人中，诊断正确的人有多少？</li>\n<li>特异性：在<strong>未患</strong>癌症的所有人中，诊断正确的人有多少？</li>\n</ul>\n<p>查准率和查全率的定义如下：</p>\n<ul>\n<li>查全率：在<strong>被诊断</strong>患有癌症的所有人中，多少人确实<strong>得了癌症</strong>？</li>\n<li>查准率：在<strong>患有癌症</strong>的所有人中，多少人<strong>被诊断</strong>患有癌症？</li>\n</ul>\n<p>从这里可以看出，敏感性就是查全率，但特异性并不是查准率。</p>\n<h2 id=\"5、-F-1-和-F-beta\"><a href=\"#5、-F-1-和-F-beta\" class=\"headerlink\" title=\"5、$F_1$和$F_\\beta$\"></a>5、$F_1$和$F_\\beta$</h2><p>把精确率和召回率合成一个值，根据精确率和召回率权重的不同可以分为$F_1$和$F_\\beta$</p>\n<p>算术平均数  Arithmetic Mean = (x+y)/2<br>调和平均数  Harmonic Mean = (2xy)/(x+y)<br>调和平均数始终小于算术平均数，因为调和平均数更接近较小的数</p>\n<p>$F_1$值的计算公式如下:<br>$$<br>F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2<em>precision</em>recall}{presicion + recall}<br>$$<br>$F_1$值就是精确率和召回率的调和平均值，$F_1$值认为精确率和召回率一样重要。</p>\n<p>$F_\\beta$值的计算公式如下:<br>$$<br>F_1 = \\frac{1 + \\beta^2}{\\frac{1}{precision} + \\frac{\\beta^2}{recall}} = \\frac{(1+\\beta^2)<em>precision</em>recall}{\\beta^2*presicion + recall}<br>$$<br>β 的界限在 0 和 ∞ 之间。如果β=0, 则得出精度。如果β=∞, 则得出召回率<br>在β=1时，$F_β$就是$F_1$值，此时$F_β$认为精确率和召回率一样重要；当β&gt;1时，$F_1$认为召回率更重要；当0&lt;β&lt;1时，认为精确$F_β$率更重要。除了$F_1$值之外，常用的还有$F_2$和$F_{0.5}$。</p>\n<h2 id=\"6、ROC曲线及AUC值\"><a href=\"#6、ROC曲线及AUC值\" class=\"headerlink\" title=\"6、ROC曲线及AUC值\"></a>6、ROC曲线及AUC值</h2><p>AUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图所示：</p>\n<p><img src=\"02.png\" alt></p>\n<p>　ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive}$，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $recall_{negative}$)如下所示：<br>$$<br>TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = recall_{positive}<br>$$</p>\n<p>$$<br>FPR = \\frac{FP}{FP + TN} = \\frac{FP+TN-TN}{FP+TN} = 1 - \\frac{TN}{FP + TN} = 1 - recall_{negative}<br>$$</p>\n<p>通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θ取值范围为[0,1]。</p>\n<p>　　ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive}$值，横坐标FPR就是(1 - $recall_{negative}$)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。</p>\n<p>　　图展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。</p>\n<p>ROC曲线上几个关键点的解释：<br>        ( TPR=0,FPR=0 ) 把每个实例都预测为负类的模型<br>        ( TPR=1,FPR=1 ) 把每个实例都预测为正类的模型<br>        ( TPR=1,FPR=0 ) 理想模型</p>\n<h1 id=\"二、回归模型\"><a href=\"#二、回归模型\" class=\"headerlink\" title=\"二、回归模型\"></a>二、回归模型</h1><h2 id=\"1、平均绝对误差-Mean-Absolute-Error\"><a href=\"#1、平均绝对误差-Mean-Absolute-Error\" class=\"headerlink\" title=\"1、平均绝对误差(Mean Absolute Error)\"></a>1、平均绝对误差(Mean Absolute Error)</h2><pre><code class=\"python\">from sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_absolute_error(y, guesses)</code></pre>\n<p>MAE问题：绝对值函数是不可微分的，不利于使用梯度下降等方法<br>解决：均方误差</p>\n<h2 id=\"2、平均平方误差-Mean-Squared-Error\"><a href=\"#2、平均平方误差-Mean-Squared-Error\" class=\"headerlink\" title=\"2、平均平方误差(Mean Squared Error)\"></a>2、平均平方误差(Mean Squared Error)</h2><pre><code class=\"python\">from sklearn.metrics import mean_square_error\nfrom sklearn.linear_model import LinearRression\nclassifier = LinearRression()\nclassifier.fit(X, y)\nguesses = classifier.predict(X)\nerror = mean_square_error(y, guesses)</code></pre>\n<h2 id=\"3、决定系数-R-2\"><a href=\"#3、决定系数-R-2\" class=\"headerlink\" title=\"3、决定系数($R^2$ )\"></a>3、决定系数($R^2$ )</h2><p>$R^2$分数通过将我们的模型与最简单的可能模型相比得出。那么思考一下，拟合一堆点的最简单的可能模型是什么呢？那就是取所有值的平均值,然后我们可以计算出这个模型的均方误差。<br>    R2 = 1 - (线性回归模型误差/简单模型误差)<br>    如果这个模型不太好 这两个误差将很接近而这个量应接近 1，那么整个 R2 分数应接近 0。<br>    如果模型较好 那么线性回归模型对的均方误差应比简单模型的均方误差小很多，那么这个比例就很小。而 R2 分数将非常接近 1，总结来说 如果 R2 分数接近 1 模型就不错。</p>\n<pre><code class=\"python\">from sklearn.metrics import r2_score\nr2_score(y_true, y_pred)</code></pre>\n<h1 id=\"三、参考\"><a href=\"#三、参考\" class=\"headerlink\" title=\"三、参考\"></a>三、参考</h1><p><a href=\"https://www.zhihu.com/question/19645541/answer/91694636\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/19645541/answer/91694636</a></p>\n<p><a href=\"https://www.zhihu.com/question/30750849\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/30750849</a></p>\n<p><a href=\"https://www.cnblogs.com/wuliytTaotao/p/9285227.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/wuliytTaotao/p/9285227.html</a></p>\n<p><a href=\"https://www.cnblogs.com/gatherstars/p/6084696.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/gatherstars/p/6084696.html</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_41043240/article/details/80265577\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/weixin_41043240/article/details/80265577</a></p>\n"},{"title":"深度学习之卷积神经网络","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2020-01-06T05:32:08.000Z","password":null,"summary":null,"_content":"\n\n\n\n\n\n\n# 一、卷积神经网络概述\n\n\n\n## 1、CNN简介\n\nCNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，\nCNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。 \n\n\n\n## 2、多层神经网络的问题\n\n- 参数较多，多层神经网络采用全连接的方式，稍微大点的图片计算复杂程度就会很大。\n- 丢失空间信息，多层神经网络将图片像素矩阵展平为向量时丢失了图片中包含的二维信息。这种像素之间的位置信息可以帮助我们发现像素中规律。\n\nCNN通过稀疏互联的层级来解决这个问题，这种局部连接层包含更少的权重并且在空间内共享。\n\n\n\n## 3、CNN整体架构\n\n![](深度学习之卷积神经网络/01.png)\n\nCNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。 然后跟着全连接神经网络\"Affine-ReLU\"组合 , 最后还是\"Affine-Softmax\"层输出最终结果（概率）。Affine为全连接层用来对卷积层提取的特征进行分类。\n\n\n\n# 二、卷积层\n\n## 1、输入输出\n\n卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。 \n\n## 2、卷积运算\n\n卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。 卷积运算对输入数据应用滤波器（卷积核）。\n\n![](深度学习之卷积神经网络/02.png)\n\n![](深度学习之卷积神经网络/03.png)\n\n\n\n​\t对于输入数据，卷积运算以一定间隔滑动**滤波器**，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。\n\n![](深度学习之卷积神经网络/04.png)\n\n​\t在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。包含偏置的卷积运算的处理流如上图。\n\n\n\n## 3、填充\n\n![](深度学习之卷积神经网络/05.png)\n\n\n\n在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。 \n\n使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。 \n\n\n\n## 4、步幅\n\n应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。 \n\n![](深度学习之卷积神经网络/06.png)\n\n在上图中，对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。像这样，步幅可以指定应用滤波器的间隔 。\n\n综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过公式进行计算： \n\n\n$$\nOH = \\frac{H + 2P - FH}{S} + 1\n$$\n\n$$\nOW = \\frac{W+ 2P - FW}{S} + 1\n$$\n\n当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。 \n\n## 5、3维数据的卷积运算\n\n![](深度学习之卷积神经网络/07.png)\n\n通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 \n\n需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。 \n\n## 6、批处理\n\n我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。 \n\n![](深度学习之卷积神经网络/08.png)\n\n这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。 \n\n\n\n# 三、池化层\n\n池化层的主要作用是降低特征图的维度，避免过拟合。CNN中主要有两种池化层，最大池化层和全局平均池化层。\n\n## 1、最大池化层\n\n![](深度学习之卷积神经网络/09.png)\n\n\n\n\"Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。\n\n\n\n除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。 \n\n\n\n## 2、池化层的特征\n\n1. 没有要学习的参数\n\n   池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。 \n\n2. 通道数不发生变化\n\n   经过池化运算，输入数据和输出数据的通道数不会发生变化。所示，计算是按通道独立进行的。 \n\n3. 对微小的位置变化具有鲁棒性（健壮）\n\n   输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。 ","source":"_posts/深度学习之卷积神经网络.md","raw":"---\ntitle: 深度学习之卷积神经网络\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2020-01-06 13:32:08\npassword:\nsummary:\ntags: \n- DL\n- ML\n- Python\ncategories: 深度学习\n---\n\n\n\n\n\n\n\n# 一、卷积神经网络概述\n\n\n\n## 1、CNN简介\n\nCNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，\nCNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。 \n\n\n\n## 2、多层神经网络的问题\n\n- 参数较多，多层神经网络采用全连接的方式，稍微大点的图片计算复杂程度就会很大。\n- 丢失空间信息，多层神经网络将图片像素矩阵展平为向量时丢失了图片中包含的二维信息。这种像素之间的位置信息可以帮助我们发现像素中规律。\n\nCNN通过稀疏互联的层级来解决这个问题，这种局部连接层包含更少的权重并且在空间内共享。\n\n\n\n## 3、CNN整体架构\n\n![](深度学习之卷积神经网络/01.png)\n\nCNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。 然后跟着全连接神经网络\"Affine-ReLU\"组合 , 最后还是\"Affine-Softmax\"层输出最终结果（概率）。Affine为全连接层用来对卷积层提取的特征进行分类。\n\n\n\n# 二、卷积层\n\n## 1、输入输出\n\n卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。 \n\n## 2、卷积运算\n\n卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。 卷积运算对输入数据应用滤波器（卷积核）。\n\n![](深度学习之卷积神经网络/02.png)\n\n![](深度学习之卷积神经网络/03.png)\n\n\n\n​\t对于输入数据，卷积运算以一定间隔滑动**滤波器**，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。\n\n![](深度学习之卷积神经网络/04.png)\n\n​\t在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。包含偏置的卷积运算的处理流如上图。\n\n\n\n## 3、填充\n\n![](深度学习之卷积神经网络/05.png)\n\n\n\n在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。 \n\n使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。 \n\n\n\n## 4、步幅\n\n应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。 \n\n![](深度学习之卷积神经网络/06.png)\n\n在上图中，对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。像这样，步幅可以指定应用滤波器的间隔 。\n\n综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过公式进行计算： \n\n\n$$\nOH = \\frac{H + 2P - FH}{S} + 1\n$$\n\n$$\nOW = \\frac{W+ 2P - FW}{S} + 1\n$$\n\n当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。 \n\n## 5、3维数据的卷积运算\n\n![](深度学习之卷积神经网络/07.png)\n\n通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 \n\n需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。 \n\n## 6、批处理\n\n我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。 \n\n![](深度学习之卷积神经网络/08.png)\n\n这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。 \n\n\n\n# 三、池化层\n\n池化层的主要作用是降低特征图的维度，避免过拟合。CNN中主要有两种池化层，最大池化层和全局平均池化层。\n\n## 1、最大池化层\n\n![](深度学习之卷积神经网络/09.png)\n\n\n\n\"Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。\n\n\n\n除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。 \n\n\n\n## 2、池化层的特征\n\n1. 没有要学习的参数\n\n   池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。 \n\n2. 通道数不发生变化\n\n   经过池化运算，输入数据和输出数据的通道数不会发生变化。所示，计算是按通道独立进行的。 \n\n3. 对微小的位置变化具有鲁棒性（健壮）\n\n   输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。 ","slug":"深度学习之卷积神经网络","published":1,"updated":"2020-01-06T09:15:12.561Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tsg000mzsv5mtkyj6zd","content":"<h1 id=\"一、卷积神经网络概述\"><a href=\"#一、卷积神经网络概述\" class=\"headerlink\" title=\"一、卷积神经网络概述\"></a>一、卷积神经网络概述</h1><h2 id=\"1、CNN简介\"><a href=\"#1、CNN简介\" class=\"headerlink\" title=\"1、CNN简介\"></a>1、CNN简介</h2><p>CNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，<br>CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。 </p>\n<h2 id=\"2、多层神经网络的问题\"><a href=\"#2、多层神经网络的问题\" class=\"headerlink\" title=\"2、多层神经网络的问题\"></a>2、多层神经网络的问题</h2><ul>\n<li>参数较多，多层神经网络采用全连接的方式，稍微大点的图片计算复杂程度就会很大。</li>\n<li>丢失空间信息，多层神经网络将图片像素矩阵展平为向量时丢失了图片中包含的二维信息。这种像素之间的位置信息可以帮助我们发现像素中规律。</li>\n</ul>\n<p>CNN通过稀疏互联的层级来解决这个问题，这种局部连接层包含更少的权重并且在空间内共享。</p>\n<h2 id=\"3、CNN整体架构\"><a href=\"#3、CNN整体架构\" class=\"headerlink\" title=\"3、CNN整体架构\"></a>3、CNN整体架构</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/01.png\" alt></p>\n<p>CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。 然后跟着全连接神经网络”Affine-ReLU”组合 , 最后还是”Affine-Softmax”层输出最终结果（概率）。Affine为全连接层用来对卷积层提取的特征进行分类。</p>\n<h1 id=\"二、卷积层\"><a href=\"#二、卷积层\" class=\"headerlink\" title=\"二、卷积层\"></a>二、卷积层</h1><h2 id=\"1、输入输出\"><a href=\"#1、输入输出\" class=\"headerlink\" title=\"1、输入输出\"></a>1、输入输出</h2><p>卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。 </p>\n<h2 id=\"2、卷积运算\"><a href=\"#2、卷积运算\" class=\"headerlink\" title=\"2、卷积运算\"></a>2、卷积运算</h2><p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。 卷积运算对输入数据应用滤波器（卷积核）。</p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02.png\" alt></p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/03.png\" alt></p>\n<p>​    对于输入数据，卷积运算以一定间隔滑动<strong>滤波器</strong>，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。</p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/04.png\" alt></p>\n<p>​    在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。包含偏置的卷积运算的处理流如上图。</p>\n<h2 id=\"3、填充\"><a href=\"#3、填充\" class=\"headerlink\" title=\"3、填充\"></a>3、填充</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/05.png\" alt></p>\n<p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。 </p>\n<p>使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。 </p>\n<h2 id=\"4、步幅\"><a href=\"#4、步幅\" class=\"headerlink\" title=\"4、步幅\"></a>4、步幅</h2><p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。 </p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/06.png\" alt></p>\n<p>在上图中，对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。像这样，步幅可以指定应用滤波器的间隔 。</p>\n<p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过公式进行计算： </p>\n<p>$$<br>OH = \\frac{H + 2P - FH}{S} + 1<br>$$</p>\n<p>$$<br>OW = \\frac{W+ 2P - FW}{S} + 1<br>$$</p>\n<p>当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。 </p>\n<h2 id=\"5、3维数据的卷积运算\"><a href=\"#5、3维数据的卷积运算\" class=\"headerlink\" title=\"5、3维数据的卷积运算\"></a>5、3维数据的卷积运算</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/07.png\" alt></p>\n<p>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 </p>\n<p>需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。 </p>\n<h2 id=\"6、批处理\"><a href=\"#6、批处理\" class=\"headerlink\" title=\"6、批处理\"></a>6、批处理</h2><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。 </p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/08.png\" alt></p>\n<p>这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。 </p>\n<h1 id=\"三、池化层\"><a href=\"#三、池化层\" class=\"headerlink\" title=\"三、池化层\"></a>三、池化层</h1><p>池化层的主要作用是降低特征图的维度，避免过拟合。CNN中主要有两种池化层，最大池化层和全局平均池化层。</p>\n<h2 id=\"1、最大池化层\"><a href=\"#1、最大池化层\" class=\"headerlink\" title=\"1、最大池化层\"></a>1、最大池化层</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/09.png\" alt></p>\n<p>“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。</p>\n<p>除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。 </p>\n<h2 id=\"2、池化层的特征\"><a href=\"#2、池化层的特征\" class=\"headerlink\" title=\"2、池化层的特征\"></a>2、池化层的特征</h2><ol>\n<li><p>没有要学习的参数</p>\n<p>池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。 </p>\n</li>\n<li><p>通道数不发生变化</p>\n<p>经过池化运算，输入数据和输出数据的通道数不会发生变化。所示，计算是按通道独立进行的。 </p>\n</li>\n<li><p>对微小的位置变化具有鲁棒性（健壮）</p>\n<p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。 </p>\n</li>\n</ol>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、卷积神经网络概述\"><a href=\"#一、卷积神经网络概述\" class=\"headerlink\" title=\"一、卷积神经网络概述\"></a>一、卷积神经网络概述</h1><h2 id=\"1、CNN简介\"><a href=\"#1、CNN简介\" class=\"headerlink\" title=\"1、CNN简介\"></a>1、CNN简介</h2><p>CNN和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，<br>CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。 </p>\n<h2 id=\"2、多层神经网络的问题\"><a href=\"#2、多层神经网络的问题\" class=\"headerlink\" title=\"2、多层神经网络的问题\"></a>2、多层神经网络的问题</h2><ul>\n<li>参数较多，多层神经网络采用全连接的方式，稍微大点的图片计算复杂程度就会很大。</li>\n<li>丢失空间信息，多层神经网络将图片像素矩阵展平为向量时丢失了图片中包含的二维信息。这种像素之间的位置信息可以帮助我们发现像素中规律。</li>\n</ul>\n<p>CNN通过稀疏互联的层级来解决这个问题，这种局部连接层包含更少的权重并且在空间内共享。</p>\n<h2 id=\"3、CNN整体架构\"><a href=\"#3、CNN整体架构\" class=\"headerlink\" title=\"3、CNN整体架构\"></a>3、CNN整体架构</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/01.png\" alt></p>\n<p>CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。 然后跟着全连接神经网络”Affine-ReLU”组合 , 最后还是”Affine-Softmax”层输出最终结果（概率）。Affine为全连接层用来对卷积层提取的特征进行分类。</p>\n<h1 id=\"二、卷积层\"><a href=\"#二、卷积层\" class=\"headerlink\" title=\"二、卷积层\"></a>二、卷积层</h1><h2 id=\"1、输入输出\"><a href=\"#1、输入输出\" class=\"headerlink\" title=\"1、输入输出\"></a>1、输入输出</h2><p>卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。 </p>\n<h2 id=\"2、卷积运算\"><a href=\"#2、卷积运算\" class=\"headerlink\" title=\"2、卷积运算\"></a>2、卷积运算</h2><p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。 卷积运算对输入数据应用滤波器（卷积核）。</p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02.png\" alt></p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/03.png\" alt></p>\n<p>​    对于输入数据，卷积运算以一定间隔滑动<strong>滤波器</strong>，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。</p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/04.png\" alt></p>\n<p>​    在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。包含偏置的卷积运算的处理流如上图。</p>\n<h2 id=\"3、填充\"><a href=\"#3、填充\" class=\"headerlink\" title=\"3、填充\"></a>3、填充</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/05.png\" alt></p>\n<p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。 </p>\n<p>使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。 </p>\n<h2 id=\"4、步幅\"><a href=\"#4、步幅\" class=\"headerlink\" title=\"4、步幅\"></a>4、步幅</h2><p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如下图所示，应用滤波器的窗口的间隔变为2个元素。 </p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/06.png\" alt></p>\n<p>在上图中，对输入大小为(7, 7)的数据，以步幅2应用了滤波器。通过将步幅设为2，输出大小变为(3, 3)。像这样，步幅可以指定应用滤波器的间隔 。</p>\n<p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。此时，输出大小可通过公式进行计算： </p>\n<p>$$<br>OH = \\frac{H + 2P - FH}{S} + 1<br>$$</p>\n<p>$$<br>OW = \\frac{W+ 2P - FW}{S} + 1<br>$$</p>\n<p>当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。 </p>\n<h2 id=\"5、3维数据的卷积运算\"><a href=\"#5、3维数据的卷积运算\" class=\"headerlink\" title=\"5、3维数据的卷积运算\"></a>5、3维数据的卷积运算</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/07.png\" alt></p>\n<p>通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 </p>\n<p>需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。 </p>\n<h2 id=\"6、批处理\"><a href=\"#6、批处理\" class=\"headerlink\" title=\"6、批处理\"></a>6、批处理</h2><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。 </p>\n<p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/08.png\" alt></p>\n<p>这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。 </p>\n<h1 id=\"三、池化层\"><a href=\"#三、池化层\" class=\"headerlink\" title=\"三、池化层\"></a>三、池化层</h1><p>池化层的主要作用是降低特征图的维度，避免过拟合。CNN中主要有两种池化层，最大池化层和全局平均池化层。</p>\n<h2 id=\"1、最大池化层\"><a href=\"#1、最大池化层\" class=\"headerlink\" title=\"1、最大池化层\"></a>1、最大池化层</h2><p><img src=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/09.png\" alt></p>\n<p>“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从2 × 2的区域中取出最大的元素。此外，这个例子中将步幅设为了2，所以2 × 2的窗口的移动间隔为2个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。</p>\n<p>除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，Average池化则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。 </p>\n<h2 id=\"2、池化层的特征\"><a href=\"#2、池化层的特征\" class=\"headerlink\" title=\"2、池化层的特征\"></a>2、池化层的特征</h2><ol>\n<li><p>没有要学习的参数</p>\n<p>池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。 </p>\n</li>\n<li><p>通道数不发生变化</p>\n<p>经过池化运算，输入数据和输出数据的通道数不会发生变化。所示，计算是按通道独立进行的。 </p>\n</li>\n<li><p>对微小的位置变化具有鲁棒性（健壮）</p>\n<p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。 </p>\n</li>\n</ol>\n"},{"title":"深度学习之CNN模型演化","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-12-29T16:47:24.000Z","password":null,"summary":null,"_content":"\n\n\n#  前沿\n\n\n\n# 一、LeNet\n\n1998年LeCun发布了[LeNet][http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。\n\n​\t![](c1.png)\n\n\n\n和“现在的CNN”相比， LeNet有几个不同点。\n\n- 第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。\n\n- 第二个不同点在于池化层。原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。\n\n```python\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam\n\n#load the MNIST dataset from keras datasets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n#Process data\nX_train = X_train.reshape(-1, 28, 28, 1) # Expend dimension for 1 cahnnel image\nX_test = X_test.reshape(-1, 28, 28, 1)  # Expend dimension for 1 cahnnel image\nX_train = X_train / 255 # Normalize\nX_test = X_test / 255 # Normalize\n\n#One hot encoding\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n#Build LetNet model with Keras\ndef LetNet(width, height, depth, classes):\n    # initialize the model\n    model = Sequential()\n\n    # first layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=6, strides=(1,1), activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # second layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=16, strides=(1,1), activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(120,activation = 'tanh'))\n    model.add(Dense(84,activation = 'tanh'))\n\n    # softmax classifier\n    model.add(Dense(classes))\n    model.add(Activation(\"softmax\"))\n\n    return model\n\nLetNet_model = LetNet(28,28,1,10)\nLetNet_model.summary()\nLetNet_model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Strat training\nHistory = LetNet_model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n\n\n# 二、AlexNet\n\n2012年，Alex Krizhevsky发表了AlexNet，相对比LeNet它的网络层次更加深，从LeNet的5层到AlexNet的8层，更重要的是AlexNet还赢得了2012年的ImageNet竞赛的第一。AlexNet不仅比LeNet的神经网络层数更多更深，并且可以学习更复杂的图像高维特征。\n\n​\t![](c2.png)\n\nAlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然\n结构上AlexNet和LeNet没有大的不同，但有以下几点差异。\n\n- 激活函数使用ReLU。\n- 使用进行局部正规化的LRN（Local Response Normalization）层\n- 使用Dropout\n- 引入max pooling\n\n```python\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build AlexNet model\ndef AlexNet(width, height, depth, classes):\n    \n    model = Sequential()\n    \n    #First Convolution and Pooling layer\n    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(width,height,depth),padding='valid',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Second Convolution and Pooling layer\n    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Three Convolution layer and Pooling Layer\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation='relu'))\n    model.add(Dropout(0.5))\n    \n    #Classfication layer\n    model.add(Dense(classes,activation='softmax'))\n\n    return model\n  \nAlexNet_model = AlexNet(224,224,3,17)\nAlexNet_model.summary()\nAlexNet_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Start training using dataaugumentation generator\nHistory = AlexNet_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n# 三、Network-in-network\n\n2013年年尾，Min Lin提出了在卷积后面再跟一个1x1的卷积核对图像进行卷积，这就是[Network-in-network](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400)的核心思想了。NiN在每次卷积完之后使用，目的是为了在进入下一层的时候合并更多的卷积特征，减少网络参数、同样的内存可以存储更大的网络。\n\n**1x1卷积核的作用**\n\n- 缩放通道的大小\n\n  通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。\n\n- 增加非线性\n\n  1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。\n\n- 减少参数\n\n  在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。\n\n\n\n# 四、VGG\n\nVGG 在 2014 年的ILSVRC比赛中最终获得了第 2 名的成绩.\n\n​\t![](c3.png)\n\n[VGG](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.1556)的创新是使用3x3的小型卷积核连续卷积。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。\n\n​\t![](c4.png)\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build VGG16Net model\ndef VGG16Net(width, height, depth, classes):\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu'))\n    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(17,activation='softmax'))\n    \n    return model\n  \nVGG16_model = VGG16Net(224,224,3,17)\nVGG16_model.summary()\nVGG16_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Start training using dataaugumentation generator\nHistory = VGG16_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n# 五、GoogLeNet\n\n2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个[GoogleNet](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842)了（也叫做Inception V1）。在 2014 年的ILSVRC比赛中最终获得了第 1名的成绩.\n\n​\t![](c6.png)\n\n​\t![](c5.png)\n\nGoogLeNet的特征:\n\n-  Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果\n- 最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n  \n#Define Inception structure\ndef Inception(x,nb_filter_para):\n    (branch1,branch2,branch3,branch4)= nb_filter_para\n    branch1x1 = Conv2D(branch1[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n\n    branch3x3 = Conv2D(branch2[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n    branch3x3 = Conv2D(branch2[1],(3,3), padding='same',strides=(1,1),name=None)(branch3x3)\n\n    branch5x5 = Conv2D(branch3[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n    branch5x5 = Conv2D(branch3[1],(1,1), padding='same',strides=(1,1),name=None)(branch5x5)\n\n    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n    branchpool = Conv2D(branch4[0],(1,1),padding='same',strides=(1,1),name=None)(branchpool)\n\n    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n\n    return x\n  \n#Build InceptionV1 model\ndef InceptionV1(width, height, depth, classes):\n    \n    inpt = Input(shape=(width,height,depth))\n\n    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n\n    x = Inception(x,[(64,),(96,128),(16,32),(32,)]) #Inception 3a 28x28x256\n    x = Inception(x,[(128,),(128,192),(32,96),(64,)]) #Inception 3b 28x28x480\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #14x14x480\n\n    x = Inception(x,[(192,),(96,208),(16,48),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(160,),(112,224),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(128,),(128,256),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(112,),(144,288),(32,64),(64,)]) #Inception 4a 14x14x528\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 4a 14x14x832\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #7x7x832\n\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 5a 7x7x832\n    x = Inception(x,[(384,),(192,384),(48,128),(128,)]) #Inception 5b 7x7x1024\n\n    #Using AveragePooling replace flatten\n    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)\n    x =Flatten()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(1000,activation='relu')(x)\n    x = Dense(classes,activation='softmax')(x)\n    \n    model=Model(input=inpt,output=x)\n    \n    return model\n\nInceptionV1_model = InceptionV1(224,224,3,17)\nInceptionV1_model.summary()\n\nInceptionV1_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\nHistory = InceptionV1_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n\n```\n\n\n\n# 六、Inception V3\n\nChristian 和他的团队都是非常高产的研究人员。2015 年 2 月，**Batch-normalized Inception** 被引入作为**Inception V2**。\n\n\n\n2015年12月，他们发布了一个新版本的GoogLeNet([Inception V3](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.00567))模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：\n\n\n\n- 通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps\n- 当网络层数深度增加时，特征的数量或层的宽度也相对应地增加\n- 在每一层使用宽度增加以增加下一层之前的特征的组合\n- **只使用3x3卷积**\n\n因此最后的模型就变成这样了：\n\n​\t![](c7.png)\n\n\n\n# 七、ResNet\n\n2015年12月[ResNet](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf)发表了，时间上大概与Inception v3网络一起发表的。\n\n我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，会出现梯度消失、梯度爆炸、网络退化，导致最终性能不佳。 ResNet中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。 \n\n![](09.png)\n\n图，在连续2层的卷积层中，将输入x跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出$F(x)$变成了$F(x) + x$。通过引入这种快捷结构，即使加深层，也能高效地学习。 \n\n因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。\n\n![](c8.png)\n\nResNet通过以2个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到150层以上，识别精度也会持续提高。并且，在ILSVRC大赛中， ResNet的错误识别率为3.5%（前5类中包含正确解这一精度下的错误识别率），令人称奇。 \n\n\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n  \n#Define Residual Block for ResNet34(2 convolution layers)\ndef Residual_Block(input_model,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut =False):\n    x = Conv2d_BN(input_model,nb_filter=nb_filter,kernel_size=kernel_size,strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size,padding='same')\n    \n    #need convolution on shortcut for add different channel\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(input_model,nb_filter=nb_filter,strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,input_model])\n        return x\n    \n#Built ResNet34\ndef ResNet34(width, height, depth, classes):\n    \n    Img = Input(shape=(width,height,depth))\n    \n    x = Conv2d_BN(Img,64,(7,7),strides=(2,2),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)  \n\n    #Residual conv2_x ouput 56x56x64 \n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    \n    #Residual conv3_x ouput 28x28x128 \n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    \n    #Residual conv4_x ouput 14x14x256\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    \n    #Residual conv5_x ouput 7x7x512\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n\n\n    #Using AveragePooling replace flatten\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(classes,activation='softmax')(x)\n    \n    model=Model(input=Img,output=x)\n    return model  \n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n\n\n# 八、Inception v4 和 Inception-ResNet\n\n2016年2月\n\nInception v4 和 Inception -ResNet 在同一篇论文[《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》][https://arxiv.org/abs/1602.07261].首先说明一下Inception v4**没有**使用残差学习的思想, 而出自同一篇论文的Inception-Resnet-v1和Inception-Resnet-v2才是Inception module与残差学习的结合产物。Inception-ResNet和Inception v4网络结构都是基于Inception v3的改进。\n\n**Inception v4中的三个基本模块**：\n\n\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v1.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v2.png\" width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v3.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. 左图是基本的Inception v2/v3模块，使用两个3x3卷积代替5x5卷积，并且使用average pooling，该模块主要处理尺寸为35x35的feature map；\n\n2. 中图模块使用1xn和nx1卷积代替nxn卷积，同样使用average pooling，该模块主要处理尺寸为17x17的feature map；\n\n3. 右图在原始的8x8处理模块上将3x3卷积用1x3卷积和3x1卷积。 \n\n总的来说，Inception v4中基本的Inception module还是沿袭了Inception v2/v3的结构，只是结构看起来更加简洁统一，并且使用更多的Inception module，实验效果也更好。\n\n下图左图为Inception v4的网络结构，右图为Inception v4的Stem模块：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v10.png\"  width=\"250\" height=\"350\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v11.png\" width=\"250\" height=\"350\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n**Inception-Resnet-v1基本模块**：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v4.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v5.png\" width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v6.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息；\n2. Inception module每个分支都没有使用pooling；\n3. 每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。\n\n  \n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v12.png\"  width=\"250\" height=\"350\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v13.png\" width=\"250\" height=\"350\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**Inception-Resnet-v2基本模块：**：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v7.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v8.png\" width=\"180\" height=\"240\" ></div><div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v9.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\nInception-Resnet-v2网络结构同Inception-Resnet-v1，Stem模块同Inception v4\n\n\n\n\n\n# 九、Xception\n\n2016年８月\n\n[Xception](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02357)是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。\n\n\n\n**结构的变形过程如下**：\n\n- 在 Inception 中，特征可以通过 1×1卷积，3×3卷积，5×5 卷积，pooling 等进行提取，Inception 结构将特征类型的选择留给网络自己训练，也就是将一个输入同时输给几种提取特征方式，然后做 concat 。Inception-v3的结构图如下:\n\n  ![](x1.png)\n\n- 对 Inception-v3 进行简化，去除 Inception-v3 中的 avg pool 后，输入的下一步操作就都是 1×1卷积：\n\n  ![](x2.png)\n\n- 提取 1×1卷积的公共部分：\n\n  ![](x3.png)\n\n- Xception（**极致的 Inception**）：先进行普通卷积操作，再对 1×1卷积后的每个channel分别进行 3×3卷积操作，最后将结果 concat：\n\n  ![](x4.png)\n\n**深度可分离卷积 Depthwise Separable Convolution**\n\n传统卷积的实现过程：\n\n![](x5.png)\n\nDepthwise Separable Convolution 的实现过程：\n\n![](x6.png)\n\n\n\n**Depthwise Separable Convolution 与 极致的 Inception 区别：**\n\n极致的 Inception：\n\n​\t第一步：普通 1×1卷积。\n\n​\t第二步：对 1×1卷积结果的每个 channel，分别进行 3×3卷积操作，并将结果 concat。\n\nDepthwise Separable Convolution：\n\n​\t第一步：Depthwise 卷积，对输入的每个channel，分别进行 3×3 卷积操作，并将结果 concat。\n\n​\t第二步：Pointwise 卷积，对 Depthwise 卷积中的 concat 结果，进行 1×1卷积操作。\n\n两种操作的循序不一致：Inception 先进行 1×1卷积，再进行 3×3卷积；Depthwise Separable Convolution 先进行 3×3卷积，再进行 1×11×1 卷积。\n\n\n\n\n# 参考\n\n​\thttps://www.cnblogs.com/CZiFan/p/9490565.html\n\n​\thttps://www.zhihu.com/question/53727257/answer/136261195\n\n​\thttps://blog.csdn.net/lk3030/article/details/84847879\n\n​\thttps://blog.csdn.net/zzc15806/article/details/83504130\n\n\n\n[https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306](https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-cnn演化史-alexnet-vgg-inception-resnet-keras-coding-668f74879306)\n\n","source":"_posts/深度学习之CNN模型演化.md","raw":"---\ntitle: 深度学习之CNN模型演化\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-12-30 00:47:24\npassword:\nsummary:\ntags:\n- DL\n- Python\ncategories: 深度学习\n---\n\n\n\n#  前沿\n\n\n\n# 一、LeNet\n\n1998年LeCun发布了[LeNet][http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。\n\n​\t![](c1.png)\n\n\n\n和“现在的CNN”相比， LeNet有几个不同点。\n\n- 第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。\n\n- 第二个不同点在于池化层。原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。\n\n```python\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam\n\n#load the MNIST dataset from keras datasets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n#Process data\nX_train = X_train.reshape(-1, 28, 28, 1) # Expend dimension for 1 cahnnel image\nX_test = X_test.reshape(-1, 28, 28, 1)  # Expend dimension for 1 cahnnel image\nX_train = X_train / 255 # Normalize\nX_test = X_test / 255 # Normalize\n\n#One hot encoding\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n#Build LetNet model with Keras\ndef LetNet(width, height, depth, classes):\n    # initialize the model\n    model = Sequential()\n\n    # first layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=6, strides=(1,1), activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # second layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=16, strides=(1,1), activation='tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(120,activation = 'tanh'))\n    model.add(Dense(84,activation = 'tanh'))\n\n    # softmax classifier\n    model.add(Dense(classes))\n    model.add(Activation(\"softmax\"))\n\n    return model\n\nLetNet_model = LetNet(28,28,1,10)\nLetNet_model.summary()\nLetNet_model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Strat training\nHistory = LetNet_model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n\n\n# 二、AlexNet\n\n2012年，Alex Krizhevsky发表了AlexNet，相对比LeNet它的网络层次更加深，从LeNet的5层到AlexNet的8层，更重要的是AlexNet还赢得了2012年的ImageNet竞赛的第一。AlexNet不仅比LeNet的神经网络层数更多更深，并且可以学习更复杂的图像高维特征。\n\n​\t![](c2.png)\n\nAlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然\n结构上AlexNet和LeNet没有大的不同，但有以下几点差异。\n\n- 激活函数使用ReLU。\n- 使用进行局部正规化的LRN（Local Response Normalization）层\n- 使用Dropout\n- 引入max pooling\n\n```python\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build AlexNet model\ndef AlexNet(width, height, depth, classes):\n    \n    model = Sequential()\n    \n    #First Convolution and Pooling layer\n    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(width,height,depth),padding='valid',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Second Convolution and Pooling layer\n    model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Three Convolution layer and Pooling Layer\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n    \n    #Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation='relu'))\n    model.add(Dropout(0.5))\n    \n    #Classfication layer\n    model.add(Dense(classes,activation='softmax'))\n\n    return model\n  \nAlexNet_model = AlexNet(224,224,3,17)\nAlexNet_model.summary()\nAlexNet_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Start training using dataaugumentation generator\nHistory = AlexNet_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n# 三、Network-in-network\n\n2013年年尾，Min Lin提出了在卷积后面再跟一个1x1的卷积核对图像进行卷积，这就是[Network-in-network](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400)的核心思想了。NiN在每次卷积完之后使用，目的是为了在进入下一层的时候合并更多的卷积特征，减少网络参数、同样的内存可以存储更大的网络。\n\n**1x1卷积核的作用**\n\n- 缩放通道的大小\n\n  通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。\n\n- 增加非线性\n\n  1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。\n\n- 减少参数\n\n  在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。\n\n\n\n# 四、VGG\n\nVGG 在 2014 年的ILSVRC比赛中最终获得了第 2 名的成绩.\n\n​\t![](c3.png)\n\n[VGG](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.1556)的创新是使用3x3的小型卷积核连续卷积。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。\n\n​\t![](c4.png)\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build VGG16Net model\ndef VGG16Net(width, height, depth, classes):\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu'))\n    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(17,activation='softmax'))\n    \n    return model\n  \nVGG16_model = VGG16Net(224,224,3,17)\nVGG16_model.summary()\nVGG16_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n#Start training using dataaugumentation generator\nHistory = VGG16_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n# 五、GoogLeNet\n\n2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个[GoogleNet](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842)了（也叫做Inception V1）。在 2014 年的ILSVRC比赛中最终获得了第 1名的成绩.\n\n​\t![](c6.png)\n\n​\t![](c5.png)\n\nGoogLeNet的特征:\n\n-  Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果\n- 最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n  \n#Define Inception structure\ndef Inception(x,nb_filter_para):\n    (branch1,branch2,branch3,branch4)= nb_filter_para\n    branch1x1 = Conv2D(branch1[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n\n    branch3x3 = Conv2D(branch2[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n    branch3x3 = Conv2D(branch2[1],(3,3), padding='same',strides=(1,1),name=None)(branch3x3)\n\n    branch5x5 = Conv2D(branch3[0],(1,1), padding='same',strides=(1,1),name=None)(x)\n    branch5x5 = Conv2D(branch3[1],(1,1), padding='same',strides=(1,1),name=None)(branch5x5)\n\n    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n    branchpool = Conv2D(branch4[0],(1,1),padding='same',strides=(1,1),name=None)(branchpool)\n\n    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n\n    return x\n  \n#Build InceptionV1 model\ndef InceptionV1(width, height, depth, classes):\n    \n    inpt = Input(shape=(width,height,depth))\n\n    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n\n    x = Inception(x,[(64,),(96,128),(16,32),(32,)]) #Inception 3a 28x28x256\n    x = Inception(x,[(128,),(128,192),(32,96),(64,)]) #Inception 3b 28x28x480\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #14x14x480\n\n    x = Inception(x,[(192,),(96,208),(16,48),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(160,),(112,224),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(128,),(128,256),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(112,),(144,288),(32,64),(64,)]) #Inception 4a 14x14x528\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 4a 14x14x832\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) #7x7x832\n\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 5a 7x7x832\n    x = Inception(x,[(384,),(192,384),(48,128),(128,)]) #Inception 5b 7x7x1024\n\n    #Using AveragePooling replace flatten\n    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)\n    x =Flatten()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(1000,activation='relu')(x)\n    x = Dense(classes,activation='softmax')(x)\n    \n    model=Model(input=inpt,output=x)\n    \n    return model\n\nInceptionV1_model = InceptionV1(224,224,3,17)\nInceptionV1_model.summary()\n\nInceptionV1_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = 'categorical_crossentropy',metrics=['accuracy'])\nHistory = InceptionV1_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n\n```\n\n\n\n# 六、Inception V3\n\nChristian 和他的团队都是非常高产的研究人员。2015 年 2 月，**Batch-normalized Inception** 被引入作为**Inception V2**。\n\n\n\n2015年12月，他们发布了一个新版本的GoogLeNet([Inception V3](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.00567))模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：\n\n\n\n- 通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps\n- 当网络层数深度增加时，特征的数量或层的宽度也相对应地增加\n- 在每一层使用宽度增加以增加下一层之前的特征的组合\n- **只使用3x3卷积**\n\n因此最后的模型就变成这样了：\n\n​\t![](c7.png)\n\n\n\n# 七、ResNet\n\n2015年12月[ResNet](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf)发表了，时间上大概与Inception v3网络一起发表的。\n\n我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，会出现梯度消失、梯度爆炸、网络退化，导致最终性能不佳。 ResNet中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。 \n\n![](09.png)\n\n图，在连续2层的卷积层中，将输入x跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出$F(x)$变成了$F(x) + x$。通过引入这种快捷结构，即使加深层，也能高效地学习。 \n\n因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。\n\n![](c8.png)\n\nResNet通过以2个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到150层以上，识别精度也会持续提高。并且，在ILSVRC大赛中， ResNet的错误识别率为3.5%（前5类中包含正确解这一精度下的错误识别率），令人称奇。 \n\n\n\n```python\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + '_bn'\n        conv_name = name + '_conv'\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n  \n#Define Residual Block for ResNet34(2 convolution layers)\ndef Residual_Block(input_model,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut =False):\n    x = Conv2d_BN(input_model,nb_filter=nb_filter,kernel_size=kernel_size,strides=strides,padding='same')\n    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size,padding='same')\n    \n    #need convolution on shortcut for add different channel\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(input_model,nb_filter=nb_filter,strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,input_model])\n        return x\n    \n#Built ResNet34\ndef ResNet34(width, height, depth, classes):\n    \n    Img = Input(shape=(width,height,depth))\n    \n    x = Conv2d_BN(Img,64,(7,7),strides=(2,2),padding='same')\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)  \n\n    #Residual conv2_x ouput 56x56x64 \n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    \n    #Residual conv3_x ouput 28x28x128 \n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    \n    #Residual conv4_x ouput 14x14x256\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    \n    #Residual conv5_x ouput 7x7x512\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n\n\n    #Using AveragePooling replace flatten\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(classes,activation='softmax')(x)\n    \n    model=Model(input=Img,output=x)\n    return model  \n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,2)\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.show()\n```\n\n\n\n\n\n# 八、Inception v4 和 Inception-ResNet\n\n2016年2月\n\nInception v4 和 Inception -ResNet 在同一篇论文[《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》][https://arxiv.org/abs/1602.07261].首先说明一下Inception v4**没有**使用残差学习的思想, 而出自同一篇论文的Inception-Resnet-v1和Inception-Resnet-v2才是Inception module与残差学习的结合产物。Inception-ResNet和Inception v4网络结构都是基于Inception v3的改进。\n\n**Inception v4中的三个基本模块**：\n\n\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v1.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v2.png\" width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v3.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. 左图是基本的Inception v2/v3模块，使用两个3x3卷积代替5x5卷积，并且使用average pooling，该模块主要处理尺寸为35x35的feature map；\n\n2. 中图模块使用1xn和nx1卷积代替nxn卷积，同样使用average pooling，该模块主要处理尺寸为17x17的feature map；\n\n3. 右图在原始的8x8处理模块上将3x3卷积用1x3卷积和3x1卷积。 \n\n总的来说，Inception v4中基本的Inception module还是沿袭了Inception v2/v3的结构，只是结构看起来更加简洁统一，并且使用更多的Inception module，实验效果也更好。\n\n下图左图为Inception v4的网络结构，右图为Inception v4的Stem模块：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v10.png\"  width=\"250\" height=\"350\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v11.png\" width=\"250\" height=\"350\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n**Inception-Resnet-v1基本模块**：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v4.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v5.png\" width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v6.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1. Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息；\n2. Inception module每个分支都没有使用pooling；\n3. 每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。\n\n  \n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v12.png\"  width=\"250\" height=\"350\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v13.png\" width=\"250\" height=\"350\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**Inception-Resnet-v2基本模块：**：\n\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v7.png\"  width=\"180\" height=\"240\" ></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v8.png\" width=\"180\" height=\"240\" ></div><div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v9.png\" width=\"180\" height=\"240\" ></div><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n\n\n\nInception-Resnet-v2网络结构同Inception-Resnet-v1，Stem模块同Inception v4\n\n\n\n\n\n# 九、Xception\n\n2016年８月\n\n[Xception](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02357)是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。\n\n\n\n**结构的变形过程如下**：\n\n- 在 Inception 中，特征可以通过 1×1卷积，3×3卷积，5×5 卷积，pooling 等进行提取，Inception 结构将特征类型的选择留给网络自己训练，也就是将一个输入同时输给几种提取特征方式，然后做 concat 。Inception-v3的结构图如下:\n\n  ![](x1.png)\n\n- 对 Inception-v3 进行简化，去除 Inception-v3 中的 avg pool 后，输入的下一步操作就都是 1×1卷积：\n\n  ![](x2.png)\n\n- 提取 1×1卷积的公共部分：\n\n  ![](x3.png)\n\n- Xception（**极致的 Inception**）：先进行普通卷积操作，再对 1×1卷积后的每个channel分别进行 3×3卷积操作，最后将结果 concat：\n\n  ![](x4.png)\n\n**深度可分离卷积 Depthwise Separable Convolution**\n\n传统卷积的实现过程：\n\n![](x5.png)\n\nDepthwise Separable Convolution 的实现过程：\n\n![](x6.png)\n\n\n\n**Depthwise Separable Convolution 与 极致的 Inception 区别：**\n\n极致的 Inception：\n\n​\t第一步：普通 1×1卷积。\n\n​\t第二步：对 1×1卷积结果的每个 channel，分别进行 3×3卷积操作，并将结果 concat。\n\nDepthwise Separable Convolution：\n\n​\t第一步：Depthwise 卷积，对输入的每个channel，分别进行 3×3 卷积操作，并将结果 concat。\n\n​\t第二步：Pointwise 卷积，对 Depthwise 卷积中的 concat 结果，进行 1×1卷积操作。\n\n两种操作的循序不一致：Inception 先进行 1×1卷积，再进行 3×3卷积；Depthwise Separable Convolution 先进行 3×3卷积，再进行 1×11×1 卷积。\n\n\n\n\n# 参考\n\n​\thttps://www.cnblogs.com/CZiFan/p/9490565.html\n\n​\thttps://www.zhihu.com/question/53727257/answer/136261195\n\n​\thttps://blog.csdn.net/lk3030/article/details/84847879\n\n​\thttps://blog.csdn.net/zzc15806/article/details/83504130\n\n\n\n[https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306](https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-cnn演化史-alexnet-vgg-inception-resnet-keras-coding-668f74879306)\n\n","slug":"深度学习之CNN模型演化","published":1,"updated":"2020-01-07T17:20:50.854Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tsl000qzsv5cr7lezre","content":"<h1 id=\"前沿\"><a href=\"#前沿\" class=\"headerlink\" title=\"前沿\"></a>前沿</h1><h1 id=\"一、LeNet\"><a href=\"#一、LeNet\" class=\"headerlink\" title=\"一、LeNet\"></a>一、LeNet</h1><p>1998年LeCun发布了[LeNet][<a href=\"http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。\" target=\"_blank\" rel=\"noopener\">http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。</a></p>\n<p>​    <img src=\"c1.png\" alt></p>\n<p>和“现在的CNN”相比， LeNet有几个不同点。</p>\n<ul>\n<li><p>第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。</p>\n</li>\n<li><p>第二个不同点在于池化层。原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。</p>\n</li>\n</ul>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> mnist\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> np_utils\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> MaxPooling2D<span class=\"token punctuation\">,</span> Flatten\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\" spellcheck=\"true\">#load the MNIST dataset from keras datasets</span>\n<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> mnist<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Process data</span>\nX_train <span class=\"token operator\">=</span> X_train<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\"># Expend dimension for 1 cahnnel image</span>\nX_test <span class=\"token operator\">=</span> X_test<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\" spellcheck=\"true\"># Expend dimension for 1 cahnnel image</span>\nX_train <span class=\"token operator\">=</span> X_train <span class=\"token operator\">/</span> <span class=\"token number\">255</span> <span class=\"token comment\" spellcheck=\"true\"># Normalize</span>\nX_test <span class=\"token operator\">=</span> X_test <span class=\"token operator\">/</span> <span class=\"token number\">255</span> <span class=\"token comment\" spellcheck=\"true\"># Normalize</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#One hot encoding</span>\ny_train <span class=\"token operator\">=</span> np_utils<span class=\"token punctuation\">.</span>to_categorical<span class=\"token punctuation\">(</span>y_train<span class=\"token punctuation\">,</span> num_classes<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ny_test <span class=\"token operator\">=</span> np_utils<span class=\"token punctuation\">.</span>to_categorical<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> num_classes<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Build LetNet model with Keras</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">LetNet</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">,</span> classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\" spellcheck=\"true\"># initialize the model</span>\n    model <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># first layer, convolution and pooling</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> filters<span class=\"token operator\">=</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'tanh'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># second layer, convolution and pooling</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> filters<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token string\">'tanh'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># Fully connection layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">120</span><span class=\"token punctuation\">,</span>activation <span class=\"token operator\">=</span> <span class=\"token string\">'tanh'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">84</span><span class=\"token punctuation\">,</span>activation <span class=\"token operator\">=</span> <span class=\"token string\">'tanh'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\"># softmax classifier</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span>classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Activation<span class=\"token punctuation\">(</span><span class=\"token string\">\"softmax\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> model\n\nLetNet_model <span class=\"token operator\">=</span> LetNet<span class=\"token punctuation\">(</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span><span class=\"token number\">28</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\nLetNet_model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nLetNet_model<span class=\"token punctuation\">.</span>compile<span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.001</span><span class=\"token punctuation\">,</span> beta_1<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">,</span> beta_2<span class=\"token operator\">=</span><span class=\"token number\">0.999</span><span class=\"token punctuation\">,</span> epsilon<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">08</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>loss <span class=\"token operator\">=</span> <span class=\"token string\">'categorical_crossentropy'</span><span class=\"token punctuation\">,</span>metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Strat training</span>\nHistory <span class=\"token operator\">=</span> LetNet_model<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span>validation_data<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Plot Loss and accuracy</span>\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"二、AlexNet\"><a href=\"#二、AlexNet\" class=\"headerlink\" title=\"二、AlexNet\"></a>二、AlexNet</h1><p>2012年，Alex Krizhevsky发表了AlexNet，相对比LeNet它的网络层次更加深，从LeNet的5层到AlexNet的8层，更重要的是AlexNet还赢得了2012年的ImageNet竞赛的第一。AlexNet不仅比LeNet的神经网络层数更多更深，并且可以学习更复杂的图像高维特征。</p>\n<p>​    <img src=\"c2.png\" alt></p>\n<p>AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然<br>结构上AlexNet和LeNet没有大的不同，但有以下几点差异。</p>\n<ul>\n<li>激活函数使用ReLU。</li>\n<li>使用进行局部正规化的LRN（Local Response Normalization）层</li>\n<li>使用Dropout</li>\n<li>引入max pooling</li>\n</ul>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> mnist\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> np_utils\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> MaxPooling2D<span class=\"token punctuation\">,</span> Flatten<span class=\"token punctuation\">,</span>Dropout\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\" spellcheck=\"true\">#Load oxflower17 dataset</span>\n<span class=\"token keyword\">import</span> tflearn<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>oxflower17 <span class=\"token keyword\">as</span> oxflower17\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\nx<span class=\"token punctuation\">,</span> y <span class=\"token operator\">=</span> oxflower17<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>one_hot<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Split train and test data</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>shuffle <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Data augumentation with Keras tools</span>\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>image <span class=\"token keyword\">import</span> ImageDataGenerator\nimg_gen <span class=\"token operator\">=</span> ImageDataGenerator<span class=\"token punctuation\">(</span>\n    rescale<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">.</span><span class=\"token operator\">/</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span>\n    shear_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    zoom_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    horizontal_flip<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Build AlexNet model</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">AlexNet</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">,</span> classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    model <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#First Convolution and Pooling layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">96</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">11</span><span class=\"token punctuation\">,</span><span class=\"token number\">11</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span>height<span class=\"token punctuation\">,</span>depth<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'valid'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Second Convolution and Pooling layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Three Convolution layer and Pooling Layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">384</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">384</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Fully connection layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Classfication layer</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span>classes<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> model\n\nAlexNet_model <span class=\"token operator\">=</span> AlexNet<span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">17</span><span class=\"token punctuation\">)</span>\nAlexNet_model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nAlexNet_model<span class=\"token punctuation\">.</span>compile<span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.00001</span><span class=\"token punctuation\">,</span> beta_1<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">,</span> beta_2<span class=\"token operator\">=</span><span class=\"token number\">0.999</span><span class=\"token punctuation\">,</span> epsilon<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">08</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>loss <span class=\"token operator\">=</span> <span class=\"token string\">'categorical_crossentropy'</span><span class=\"token punctuation\">,</span>metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Start training using dataaugumentation generator</span>\nHistory <span class=\"token operator\">=</span> AlexNet_model<span class=\"token punctuation\">.</span>fit_generator<span class=\"token punctuation\">(</span>img_gen<span class=\"token punctuation\">.</span>flow<span class=\"token punctuation\">(</span>X_train<span class=\"token operator\">*</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> batch_size <span class=\"token operator\">=</span> <span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                      steps_per_epoch <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">=</span> <span class=\"token number\">30</span> <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Plot Loss and Accuracy</span>\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"三、Network-in-network\"><a href=\"#三、Network-in-network\" class=\"headerlink\" title=\"三、Network-in-network\"></a>三、Network-in-network</h1><p>2013年年尾，Min Lin提出了在卷积后面再跟一个1x1的卷积核对图像进行卷积，这就是<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"noopener\">Network-in-network</a>的核心思想了。NiN在每次卷积完之后使用，目的是为了在进入下一层的时候合并更多的卷积特征，减少网络参数、同样的内存可以存储更大的网络。</p>\n<p><strong>1x1卷积核的作用</strong></p>\n<ul>\n<li><p>缩放通道的大小</p>\n<p>通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。</p>\n</li>\n<li><p>增加非线性</p>\n<p>1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。</p>\n</li>\n<li><p>减少参数</p>\n<p>在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。</p>\n</li>\n</ul>\n<h1 id=\"四、VGG\"><a href=\"#四、VGG\" class=\"headerlink\" title=\"四、VGG\"></a>四、VGG</h1><p>VGG 在 2014 年的ILSVRC比赛中最终获得了第 2 名的成绩.</p>\n<p>​    <img src=\"c3.png\" alt></p>\n<p><a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.1556\" target=\"_blank\" rel=\"noopener\">VGG</a>的创新是使用3x3的小型卷积核连续卷积。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。</p>\n<p>​    <img src=\"c4.png\" alt></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> mnist\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> np_utils\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> MaxPooling2D<span class=\"token punctuation\">,</span> Flatten<span class=\"token punctuation\">,</span>Dropout\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\" spellcheck=\"true\">#Load oxflower17 dataset</span>\n<span class=\"token keyword\">import</span> tflearn<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>oxflower17 <span class=\"token keyword\">as</span> oxflower17\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\nx<span class=\"token punctuation\">,</span> y <span class=\"token operator\">=</span> oxflower17<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>one_hot<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Split train and test data</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>shuffle <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Data augumentation with Keras tools</span>\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>image <span class=\"token keyword\">import</span> ImageDataGenerator\nimg_gen <span class=\"token operator\">=</span> ImageDataGenerator<span class=\"token punctuation\">(</span>\n    rescale<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">.</span><span class=\"token operator\">/</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span>\n    shear_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    zoom_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    horizontal_flip<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Build VGG16Net model</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">VGG16Net</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">,</span> classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    model <span class=\"token operator\">=</span> Sequential<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>input_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Conv2D<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">17</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> model\n\nVGG16_model <span class=\"token operator\">=</span> VGG16Net<span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">17</span><span class=\"token punctuation\">)</span>\nVGG16_model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nVGG16_model<span class=\"token punctuation\">.</span>compile<span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.00001</span><span class=\"token punctuation\">,</span> beta_1<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">,</span> beta_2<span class=\"token operator\">=</span><span class=\"token number\">0.999</span><span class=\"token punctuation\">,</span> epsilon<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">08</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>loss <span class=\"token operator\">=</span> <span class=\"token string\">'categorical_crossentropy'</span><span class=\"token punctuation\">,</span>metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Start training using dataaugumentation generator</span>\nHistory <span class=\"token operator\">=</span> VGG16_model<span class=\"token punctuation\">.</span>fit_generator<span class=\"token punctuation\">(</span>img_gen<span class=\"token punctuation\">.</span>flow<span class=\"token punctuation\">(</span>X_train<span class=\"token operator\">*</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> batch_size <span class=\"token operator\">=</span> <span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                      steps_per_epoch <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">=</span> <span class=\"token number\">30</span> <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Plot Loss and Accuracy</span>\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"五、GoogLeNet\"><a href=\"#五、GoogLeNet\" class=\"headerlink\" title=\"五、GoogLeNet\"></a>五、GoogLeNet</h1><p>2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842\" target=\"_blank\" rel=\"noopener\">GoogleNet</a>了（也叫做Inception V1）。在 2014 年的ILSVRC比赛中最终获得了第 1名的成绩.</p>\n<p>​    <img src=\"c6.png\" alt></p>\n<p>​    <img src=\"c5.png\" alt></p>\n<p>GoogLeNet的特征:</p>\n<ul>\n<li>Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果</li>\n<li>最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。</li>\n</ul>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> mnist\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> np_utils\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> MaxPooling2D<span class=\"token punctuation\">,</span> Flatten<span class=\"token punctuation\">,</span>Dropout<span class=\"token punctuation\">,</span>BatchNormalization<span class=\"token punctuation\">,</span>AveragePooling2D<span class=\"token punctuation\">,</span>concatenate<span class=\"token punctuation\">,</span>Input<span class=\"token punctuation\">,</span> concatenate\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Model<span class=\"token punctuation\">,</span>load_model\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\" spellcheck=\"true\">#Load oxflower17 dataset</span>\n<span class=\"token keyword\">import</span> tflearn<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>oxflower17 <span class=\"token keyword\">as</span> oxflower17\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\nx<span class=\"token punctuation\">,</span> y <span class=\"token operator\">=</span> oxflower17<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>one_hot<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Split train and test data</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>shuffle <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Data augumentation with Keras tools</span>\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>image <span class=\"token keyword\">import</span> ImageDataGenerator\nimg_gen <span class=\"token operator\">=</span> ImageDataGenerator<span class=\"token punctuation\">(</span>\n    rescale<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">.</span><span class=\"token operator\">/</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span>\n    shear_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    zoom_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    horizontal_flip<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Define convolution with batchnromalization</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">Conv2d_BN</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> name <span class=\"token keyword\">is</span> <span class=\"token operator\">not</span> None<span class=\"token punctuation\">:</span>\n        bn_name <span class=\"token operator\">=</span> name <span class=\"token operator\">+</span> <span class=\"token string\">'_bn'</span>\n        conv_name <span class=\"token operator\">=</span> name <span class=\"token operator\">+</span> <span class=\"token string\">'_conv'</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        bn_name <span class=\"token operator\">=</span> None\n        conv_name <span class=\"token operator\">=</span> None\n\n    x <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span>padding<span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>conv_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> BatchNormalization<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>bn_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\" spellcheck=\"true\">#Define Inception structure</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">Inception</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter_para<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">(</span>branch1<span class=\"token punctuation\">,</span>branch2<span class=\"token punctuation\">,</span>branch3<span class=\"token punctuation\">,</span>branch4<span class=\"token punctuation\">)</span><span class=\"token operator\">=</span> nb_filter_para\n    branch1x1 <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch1<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n    branch3x3 <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch2<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    branch3x3 <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch2<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>branch3x3<span class=\"token punctuation\">)</span>\n\n    branch5x5 <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch3<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    branch5x5 <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch3<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>branch5x5<span class=\"token punctuation\">)</span>\n\n    branchpool <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    branchpool <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>branch4<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>branchpool<span class=\"token punctuation\">)</span>\n\n    x <span class=\"token operator\">=</span> concatenate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>branch1x1<span class=\"token punctuation\">,</span>branch3x3<span class=\"token punctuation\">,</span>branch5x5<span class=\"token punctuation\">,</span>branchpool<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\" spellcheck=\"true\">#Build InceptionV1 model</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">InceptionV1</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">,</span> classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    inpt <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span>height<span class=\"token punctuation\">,</span>depth<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    x <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>inpt<span class=\"token punctuation\">,</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token number\">192</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">96</span><span class=\"token punctuation\">,</span><span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span><span class=\"token number\">32</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 3a 28x28x256</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token number\">192</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">96</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 3b 28x28x480</span>\n    x <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#14x14x480</span>\n\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">192</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">96</span><span class=\"token punctuation\">,</span><span class=\"token number\">208</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span><span class=\"token number\">48</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 4a 14x14x512</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">160</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">112</span><span class=\"token punctuation\">,</span><span class=\"token number\">224</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">24</span><span class=\"token punctuation\">,</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 4a 14x14x512</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">24</span><span class=\"token punctuation\">,</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 4a 14x14x512</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">112</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">144</span><span class=\"token punctuation\">,</span><span class=\"token number\">288</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">64</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 4a 14x14x528</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">160</span><span class=\"token punctuation\">,</span><span class=\"token number\">320</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 4a 14x14x832</span>\n    x <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#7x7x832</span>\n\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">160</span><span class=\"token punctuation\">,</span><span class=\"token number\">320</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span><span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 5a 7x7x832</span>\n    x <span class=\"token operator\">=</span> Inception<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token number\">384</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">192</span><span class=\"token punctuation\">,</span><span class=\"token number\">384</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">48</span><span class=\"token punctuation\">,</span><span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">#Inception 5b 7x7x1024</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Using AveragePooling replace flatten</span>\n    x <span class=\"token operator\">=</span> AveragePooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>classes<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n    model<span class=\"token operator\">=</span>Model<span class=\"token punctuation\">(</span>input<span class=\"token operator\">=</span>inpt<span class=\"token punctuation\">,</span>output<span class=\"token operator\">=</span>x<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> model\n\nInceptionV1_model <span class=\"token operator\">=</span> InceptionV1<span class=\"token punctuation\">(</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">224</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">17</span><span class=\"token punctuation\">)</span>\nInceptionV1_model<span class=\"token punctuation\">.</span>summary<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nInceptionV1_model<span class=\"token punctuation\">.</span>compile<span class=\"token punctuation\">(</span>optimizer<span class=\"token operator\">=</span>Adam<span class=\"token punctuation\">(</span>lr<span class=\"token operator\">=</span><span class=\"token number\">0.00001</span><span class=\"token punctuation\">,</span> beta_1<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">,</span> beta_2<span class=\"token operator\">=</span><span class=\"token number\">0.999</span><span class=\"token punctuation\">,</span> epsilon<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">08</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>loss <span class=\"token operator\">=</span> <span class=\"token string\">'categorical_crossentropy'</span><span class=\"token punctuation\">,</span>metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nHistory <span class=\"token operator\">=</span> InceptionV1_model<span class=\"token punctuation\">.</span>fit_generator<span class=\"token punctuation\">(</span>img_gen<span class=\"token punctuation\">.</span>flow<span class=\"token punctuation\">(</span>X_train<span class=\"token operator\">*</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> batch_size <span class=\"token operator\">=</span> <span class=\"token number\">16</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>steps_per_epoch <span class=\"token operator\">=</span> len<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token number\">16</span><span class=\"token punctuation\">,</span> validation_data <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span>y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> epochs <span class=\"token operator\">=</span> <span class=\"token number\">30</span> <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Plot Loss and accuracy</span>\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"六、Inception-V3\"><a href=\"#六、Inception-V3\" class=\"headerlink\" title=\"六、Inception V3\"></a>六、Inception V3</h1><p>Christian 和他的团队都是非常高产的研究人员。2015 年 2 月，<strong>Batch-normalized Inception</strong> 被引入作为<strong>Inception V2</strong>。</p>\n<p>2015年12月，他们发布了一个新版本的GoogLeNet(<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.00567\" target=\"_blank\" rel=\"noopener\">Inception V3</a>)模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：</p>\n<ul>\n<li>通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps</li>\n<li>当网络层数深度增加时，特征的数量或层的宽度也相对应地增加</li>\n<li>在每一层使用宽度增加以增加下一层之前的特征的组合</li>\n<li><strong>只使用3x3卷积</strong></li>\n</ul>\n<p>因此最后的模型就变成这样了：</p>\n<p>​    <img src=\"c7.png\" alt></p>\n<h1 id=\"七、ResNet\"><a href=\"#七、ResNet\" class=\"headerlink\" title=\"七、ResNet\"></a>七、ResNet</h1><p>2015年12月<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>发表了，时间上大概与Inception v3网络一起发表的。</p>\n<p>我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，会出现梯度消失、梯度爆炸、网络退化，导致最终性能不佳。 ResNet中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。 </p>\n<p><img src=\"09.png\" alt></p>\n<p>图，在连续2层的卷积层中，将输入x跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出$F(x)$变成了$F(x) + x$。通过引入这种快捷结构，即使加深层，也能高效地学习。 </p>\n<p>因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。</p>\n<p><img src=\"c8.png\" alt></p>\n<p>ResNet通过以2个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到150层以上，识别精度也会持续提高。并且，在ILSVRC大赛中， ResNet的错误识别率为3.5%（前5类中包含正确解这一精度下的错误识别率），令人称奇。 </p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> keras\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> mnist\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>utils <span class=\"token keyword\">import</span> np_utils\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Sequential\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>layers <span class=\"token keyword\">import</span> Dense<span class=\"token punctuation\">,</span> Activation<span class=\"token punctuation\">,</span> Conv2D<span class=\"token punctuation\">,</span> MaxPooling2D<span class=\"token punctuation\">,</span> Flatten<span class=\"token punctuation\">,</span>Dropout<span class=\"token punctuation\">,</span>BatchNormalization<span class=\"token punctuation\">,</span>AveragePooling2D<span class=\"token punctuation\">,</span>concatenate<span class=\"token punctuation\">,</span>Input<span class=\"token punctuation\">,</span> concatenate\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> Model<span class=\"token punctuation\">,</span>load_model\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>optimizers <span class=\"token keyword\">import</span> Adam\n\n<span class=\"token comment\" spellcheck=\"true\">#Load oxflower17 dataset</span>\n<span class=\"token keyword\">import</span> tflearn<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>oxflower17 <span class=\"token keyword\">as</span> oxflower17\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\nx<span class=\"token punctuation\">,</span> y <span class=\"token operator\">=</span> oxflower17<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>one_hot<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Split train and test data</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>shuffle <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Data augumentation with Keras tools</span>\n<span class=\"token keyword\">from</span> keras<span class=\"token punctuation\">.</span>preprocessing<span class=\"token punctuation\">.</span>image <span class=\"token keyword\">import</span> ImageDataGenerator\nimg_gen <span class=\"token operator\">=</span> ImageDataGenerator<span class=\"token punctuation\">(</span>\n    rescale<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">.</span><span class=\"token operator\">/</span><span class=\"token number\">255</span><span class=\"token punctuation\">,</span>\n    shear_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    zoom_range<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span>\n    horizontal_flip<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\" spellcheck=\"true\">#Define convolution with batchnromalization</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">Conv2d_BN</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>None<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> name <span class=\"token keyword\">is</span> <span class=\"token operator\">not</span> None<span class=\"token punctuation\">:</span>\n        bn_name <span class=\"token operator\">=</span> name <span class=\"token operator\">+</span> <span class=\"token string\">'_bn'</span>\n        conv_name <span class=\"token operator\">=</span> name <span class=\"token operator\">+</span> <span class=\"token string\">'_conv'</span>\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        bn_name <span class=\"token operator\">=</span> None\n        conv_name <span class=\"token operator\">=</span> None\n\n    x <span class=\"token operator\">=</span> Conv2D<span class=\"token punctuation\">(</span>nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span>padding<span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>conv_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> BatchNormalization<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span>name<span class=\"token operator\">=</span>bn_name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\" spellcheck=\"true\">#Define Residual Block for ResNet34(2 convolution layers)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">Residual_Block</span><span class=\"token punctuation\">(</span>input_model<span class=\"token punctuation\">,</span>nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> with_conv_shortcut <span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    x <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>input_model<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span>nb_filter<span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span>kernel_size<span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> nb_filter<span class=\"token operator\">=</span>nb_filter<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span>kernel_size<span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#need convolution on shortcut for add different channel</span>\n    <span class=\"token keyword\">if</span> with_conv_shortcut<span class=\"token punctuation\">:</span>\n        shortcut <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>input_model<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span>nb_filter<span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span>kernel_size<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> add<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">,</span>shortcut<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x\n    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">=</span> add<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">,</span>input_model<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\" spellcheck=\"true\">#Built ResNet34</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">ResNet34</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span> height<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">,</span> classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    Img <span class=\"token operator\">=</span> Input<span class=\"token punctuation\">(</span>shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>width<span class=\"token punctuation\">,</span>height<span class=\"token punctuation\">,</span>depth<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    x <span class=\"token operator\">=</span> Conv2d_BN<span class=\"token punctuation\">(</span>Img<span class=\"token punctuation\">,</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> MaxPooling2D<span class=\"token punctuation\">(</span>pool_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>  \n\n    <span class=\"token comment\" spellcheck=\"true\">#Residual conv2_x ouput 56x56x64 </span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Residual conv3_x ouput 28x28x128 </span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>with_conv_shortcut<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token comment\" spellcheck=\"true\"># need do convolution to add different channel</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Residual conv4_x ouput 14x14x256</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>with_conv_shortcut<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token comment\" spellcheck=\"true\"># need do convolution to add different channel</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">#Residual conv5_x ouput 7x7x512</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>with_conv_shortcut<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Residual_Block<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>nb_filter<span class=\"token operator\">=</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span>kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n\n    <span class=\"token comment\" spellcheck=\"true\">#Using AveragePooling replace flatten</span>\n    x <span class=\"token operator\">=</span> GlobalAveragePooling2D<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n    x <span class=\"token operator\">=</span> Dense<span class=\"token punctuation\">(</span>classes<span class=\"token punctuation\">,</span>activation<span class=\"token operator\">=</span><span class=\"token string\">'softmax'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n    model<span class=\"token operator\">=</span>Model<span class=\"token punctuation\">(</span>input<span class=\"token operator\">=</span>Img<span class=\"token punctuation\">,</span>output<span class=\"token operator\">=</span>x<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> model  \n\n<span class=\"token comment\" spellcheck=\"true\">#Plot Loss and accuracy</span>\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">15</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_acc'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'accuracy'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\n\nplt<span class=\"token punctuation\">.</span>subplot<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>plot<span class=\"token punctuation\">(</span>History<span class=\"token punctuation\">.</span>history<span class=\"token punctuation\">[</span><span class=\"token string\">'val_loss'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'model loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'loss'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'epoch'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>legend<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token string\">'train'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'test'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> loc<span class=\"token operator\">=</span><span class=\"token string\">'upper left'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h1 id=\"八、Inception-v4-和-Inception-ResNet\"><a href=\"#八、Inception-v4-和-Inception-ResNet\" class=\"headerlink\" title=\"八、Inception v4 和 Inception-ResNet\"></a>八、Inception v4 和 Inception-ResNet</h1><p>2016年2月</p>\n<p>Inception v4 和 Inception -ResNet 在同一篇论文[《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》][<a href=\"https://arxiv.org/abs/1602.07261].首先说明一下Inception\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1602.07261].首先说明一下Inception</a> v4<strong>没有</strong>使用残差学习的思想, 而出自同一篇论文的Inception-Resnet-v1和Inception-Resnet-v2才是Inception module与残差学习的结合产物。Inception-ResNet和Inception v4网络结构都是基于Inception v3的改进。</p>\n<p><strong>Inception v4中的三个基本模块</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v1.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v2.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v3.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<ol>\n<li><p>左图是基本的Inception v2/v3模块，使用两个3x3卷积代替5x5卷积，并且使用average pooling，该模块主要处理尺寸为35x35的feature map；</p>\n</li>\n<li><p>中图模块使用1xn和nx1卷积代替nxn卷积，同样使用average pooling，该模块主要处理尺寸为17x17的feature map；</p>\n</li>\n<li><p>右图在原始的8x8处理模块上将3x3卷积用1x3卷积和3x1卷积。 </p>\n</li>\n</ol>\n<p>总的来说，Inception v4中基本的Inception module还是沿袭了Inception v2/v3的结构，只是结构看起来更加简洁统一，并且使用更多的Inception module，实验效果也更好。</p>\n<p>下图左图为Inception v4的网络结构，右图为Inception v4的Stem模块：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v10.png\" width=\"250\" height=\"350\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v11.png\" width=\"250\" height=\"350\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n<p><strong>Inception-Resnet-v1基本模块</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v4.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v5.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v6.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<ol>\n<li>Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息；</li>\n<li>Inception module每个分支都没有使用pooling；</li>\n<li>每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。</li>\n</ol>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v12.png\" width=\"250\" height=\"350\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v13.png\" width=\"250\" height=\"350\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<p><strong>Inception-Resnet-v2基本模块：</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v7.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v8.png\" width=\"180\" height=\"240\"></div><div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v9.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n<p>Inception-Resnet-v2网络结构同Inception-Resnet-v1，Stem模块同Inception v4</p>\n<h1 id=\"九、Xception\"><a href=\"#九、Xception\" class=\"headerlink\" title=\"九、Xception\"></a>九、Xception</h1><p>2016年８月</p>\n<p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02357\" target=\"_blank\" rel=\"noopener\">Xception</a>是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>\n<p><strong>结构的变形过程如下</strong>：</p>\n<ul>\n<li><p>在 Inception 中，特征可以通过 1×1卷积，3×3卷积，5×5 卷积，pooling 等进行提取，Inception 结构将特征类型的选择留给网络自己训练，也就是将一个输入同时输给几种提取特征方式，然后做 concat 。Inception-v3的结构图如下:</p>\n<p><img src=\"x1.png\" alt></p>\n</li>\n<li><p>对 Inception-v3 进行简化，去除 Inception-v3 中的 avg pool 后，输入的下一步操作就都是 1×1卷积：</p>\n<p><img src=\"x2.png\" alt></p>\n</li>\n<li><p>提取 1×1卷积的公共部分：</p>\n<p><img src=\"x3.png\" alt></p>\n</li>\n<li><p>Xception（<strong>极致的 Inception</strong>）：先进行普通卷积操作，再对 1×1卷积后的每个channel分别进行 3×3卷积操作，最后将结果 concat：</p>\n<p><img src=\"x4.png\" alt></p>\n</li>\n</ul>\n<p><strong>深度可分离卷积 Depthwise Separable Convolution</strong></p>\n<p>传统卷积的实现过程：</p>\n<p><img src=\"x5.png\" alt></p>\n<p>Depthwise Separable Convolution 的实现过程：</p>\n<p><img src=\"x6.png\" alt></p>\n<p><strong>Depthwise Separable Convolution 与 极致的 Inception 区别：</strong></p>\n<p>极致的 Inception：</p>\n<p>​    第一步：普通 1×1卷积。</p>\n<p>​    第二步：对 1×1卷积结果的每个 channel，分别进行 3×3卷积操作，并将结果 concat。</p>\n<p>Depthwise Separable Convolution：</p>\n<p>​    第一步：Depthwise 卷积，对输入的每个channel，分别进行 3×3 卷积操作，并将结果 concat。</p>\n<p>​    第二步：Pointwise 卷积，对 Depthwise 卷积中的 concat 结果，进行 1×1卷积操作。</p>\n<p>两种操作的循序不一致：Inception 先进行 1×1卷积，再进行 3×3卷积；Depthwise Separable Convolution 先进行 3×3卷积，再进行 1×11×1 卷积。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>​    <a href=\"https://www.cnblogs.com/CZiFan/p/9490565.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/CZiFan/p/9490565.html</a></p>\n<p>​    <a href=\"https://www.zhihu.com/question/53727257/answer/136261195\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/53727257/answer/136261195</a></p>\n<p>​    <a href=\"https://blog.csdn.net/lk3030/article/details/84847879\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/lk3030/article/details/84847879</a></p>\n<p>​    <a href=\"https://blog.csdn.net/zzc15806/article/details/83504130\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/zzc15806/article/details/83504130</a></p>\n<p><a href=\"https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-cnn演化史-alexnet-vgg-inception-resnet-keras-coding-668f74879306\" target=\"_blank\" rel=\"noopener\">https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306</a></p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"前沿\"><a href=\"#前沿\" class=\"headerlink\" title=\"前沿\"></a>前沿</h1><h1 id=\"一、LeNet\"><a href=\"#一、LeNet\" class=\"headerlink\" title=\"一、LeNet\"></a>一、LeNet</h1><p>1998年LeCun发布了[LeNet][<a href=\"http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。\" target=\"_blank\" rel=\"noopener\">http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf]网络架构，从而揭开了深度学习的神秘面纱。</a></p>\n<p>​    <img src=\"c1.png\" alt></p>\n<p>和“现在的CNN”相比， LeNet有几个不同点。</p>\n<ul>\n<li><p>第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。</p>\n</li>\n<li><p>第二个不同点在于池化层。原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。</p>\n</li>\n</ul>\n<pre><code class=\"python\">import numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam\n\n#load the MNIST dataset from keras datasets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n#Process data\nX_train = X_train.reshape(-1, 28, 28, 1) # Expend dimension for 1 cahnnel image\nX_test = X_test.reshape(-1, 28, 28, 1)  # Expend dimension for 1 cahnnel image\nX_train = X_train / 255 # Normalize\nX_test = X_test / 255 # Normalize\n\n#One hot encoding\ny_train = np_utils.to_categorical(y_train, num_classes=10)\ny_test = np_utils.to_categorical(y_test, num_classes=10)\n\n#Build LetNet model with Keras\ndef LetNet(width, height, depth, classes):\n    # initialize the model\n    model = Sequential()\n\n    # first layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=6, strides=(1,1), activation=&#39;tanh&#39;))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # second layer, convolution and pooling\n    model.add(Conv2D(input_shape=(width, height, depth), kernel_size=(5, 5), filters=16, strides=(1,1), activation=&#39;tanh&#39;))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(120,activation = &#39;tanh&#39;))\n    model.add(Dense(84,activation = &#39;tanh&#39;))\n\n    # softmax classifier\n    model.add(Dense(classes))\n    model.add(Activation(&quot;softmax&quot;))\n\n    return model\n\nLetNet_model = LetNet(28,28,1,10)\nLetNet_model.summary()\nLetNet_model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])\n\n#Strat training\nHistory = LetNet_model.fit(X_train, y_train, epochs=5, batch_size=32,validation_data=(X_test, y_test))\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history[&#39;acc&#39;])\nplt.plot(History.history[&#39;val_acc&#39;])\nplt.title(&#39;model accuracy&#39;)\nplt.ylabel(&#39;accuracy&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\n\nplt.subplot(1,2,2)\nplt.plot(History.history[&#39;loss&#39;])\nplt.plot(History.history[&#39;val_loss&#39;])\nplt.title(&#39;model loss&#39;)\nplt.ylabel(&#39;loss&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\nplt.show()\nplt.show()</code></pre>\n<h1 id=\"二、AlexNet\"><a href=\"#二、AlexNet\" class=\"headerlink\" title=\"二、AlexNet\"></a>二、AlexNet</h1><p>2012年，Alex Krizhevsky发表了AlexNet，相对比LeNet它的网络层次更加深，从LeNet的5层到AlexNet的8层，更重要的是AlexNet还赢得了2012年的ImageNet竞赛的第一。AlexNet不仅比LeNet的神经网络层数更多更深，并且可以学习更复杂的图像高维特征。</p>\n<p>​    <img src=\"c2.png\" alt></p>\n<p>AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然<br>结构上AlexNet和LeNet没有大的不同，但有以下几点差异。</p>\n<ul>\n<li>激活函数使用ReLU。</li>\n<li>使用进行局部正规化的LRN（Local Response Normalization）层</li>\n<li>使用Dropout</li>\n<li>引入max pooling</li>\n</ul>\n<pre><code class=\"python\">import numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build AlexNet model\ndef AlexNet(width, height, depth, classes):\n\n    model = Sequential()\n\n    #First Convolution and Pooling layer\n    model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(width,height,depth),padding=&#39;valid&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n\n    #Second Convolution and Pooling layer\n    model.add(Conv2D(256,(5,5),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n\n    #Three Convolution layer and Pooling Layer\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(384,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n\n    #Fully connection layer\n    model.add(Flatten())\n    model.add(Dense(4096,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n\n    #Classfication layer\n    model.add(Dense(classes,activation=&#39;softmax&#39;))\n\n    return model\n\nAlexNet_model = AlexNet(224,224,3,17)\nAlexNet_model.summary()\nAlexNet_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])\n\n#Start training using dataaugumentation generator\nHistory = AlexNet_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history[&#39;acc&#39;])\nplt.plot(History.history[&#39;val_acc&#39;])\nplt.title(&#39;model accuracy&#39;)\nplt.ylabel(&#39;accuracy&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\n\nplt.subplot(1,2,2)\nplt.plot(History.history[&#39;loss&#39;])\nplt.plot(History.history[&#39;val_loss&#39;])\nplt.title(&#39;model loss&#39;)\nplt.ylabel(&#39;loss&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\nplt.show()\nplt.show()</code></pre>\n<h1 id=\"三、Network-in-network\"><a href=\"#三、Network-in-network\" class=\"headerlink\" title=\"三、Network-in-network\"></a>三、Network-in-network</h1><p>2013年年尾，Min Lin提出了在卷积后面再跟一个1x1的卷积核对图像进行卷积，这就是<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"noopener\">Network-in-network</a>的核心思想了。NiN在每次卷积完之后使用，目的是为了在进入下一层的时候合并更多的卷积特征，减少网络参数、同样的内存可以存储更大的网络。</p>\n<p><strong>1x1卷积核的作用</strong></p>\n<ul>\n<li><p>缩放通道的大小</p>\n<p>通过控制卷积核的数量达到通道数大小的放缩。而池化层只能改变高度和宽度，无法改变通道数。</p>\n</li>\n<li><p>增加非线性</p>\n<p>1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性，使得网络可以表达更加复杂的特征。</p>\n</li>\n<li><p>减少参数</p>\n<p>在Inception Network中，由于需要进行较多的卷积运算，计算量很大，可以通过引入1×1确保效果的同时减少计算量。</p>\n</li>\n</ul>\n<h1 id=\"四、VGG\"><a href=\"#四、VGG\" class=\"headerlink\" title=\"四、VGG\"></a>四、VGG</h1><p>VGG 在 2014 年的ILSVRC比赛中最终获得了第 2 名的成绩.</p>\n<p>​    <img src=\"c3.png\" alt></p>\n<p><a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.1556\" target=\"_blank\" rel=\"noopener\">VGG</a>的创新是使用3x3的小型卷积核连续卷积。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。</p>\n<p>​    <img src=\"c4.png\" alt></p>\n<pre><code class=\"python\">\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Build VGG16Net model\ndef VGG16Net(width, height, depth, classes):\n\n    model = Sequential()\n\n    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(64,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(128,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(256,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(Conv2D(512,(3,3),strides=(1,1),padding=&#39;same&#39;,activation=&#39;relu&#39;))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Flatten())\n    model.add(Dense(4096,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n    model.add(Dense(4096,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n    model.add(Dense(1000,activation=&#39;relu&#39;))\n    model.add(Dropout(0.5))\n    model.add(Dense(17,activation=&#39;softmax&#39;))\n\n    return model\n\nVGG16_model = VGG16Net(224,224,3,17)\nVGG16_model.summary()\nVGG16_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])\n\n#Start training using dataaugumentation generator\nHistory = VGG16_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),\n                                      steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and Accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history[&#39;acc&#39;])\nplt.plot(History.history[&#39;val_acc&#39;])\nplt.title(&#39;model accuracy&#39;)\nplt.ylabel(&#39;accuracy&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\n\nplt.subplot(1,2,2)\nplt.plot(History.history[&#39;loss&#39;])\nplt.plot(History.history[&#39;val_loss&#39;])\nplt.title(&#39;model loss&#39;)\nplt.ylabel(&#39;loss&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\nplt.show()\nplt.show()</code></pre>\n<h1 id=\"五、GoogLeNet\"><a href=\"#五、GoogLeNet\" class=\"headerlink\" title=\"五、GoogLeNet\"></a>五、GoogLeNet</h1><p>2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.4842\" target=\"_blank\" rel=\"noopener\">GoogleNet</a>了（也叫做Inception V1）。在 2014 年的ILSVRC比赛中最终获得了第 1名的成绩.</p>\n<p>​    <img src=\"c6.png\" alt></p>\n<p>​    <img src=\"c5.png\" alt></p>\n<p>GoogLeNet的特征:</p>\n<ul>\n<li>Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果</li>\n<li>最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。</li>\n</ul>\n<pre><code class=\"python\">\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding=&#39;same&#39;,strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + &#39;_bn&#39;\n        conv_name = name + &#39;_conv&#39;\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=&#39;relu&#39;,name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n\n#Define Inception structure\ndef Inception(x,nb_filter_para):\n    (branch1,branch2,branch3,branch4)= nb_filter_para\n    branch1x1 = Conv2D(branch1[0],(1,1), padding=&#39;same&#39;,strides=(1,1),name=None)(x)\n\n    branch3x3 = Conv2D(branch2[0],(1,1), padding=&#39;same&#39;,strides=(1,1),name=None)(x)\n    branch3x3 = Conv2D(branch2[1],(3,3), padding=&#39;same&#39;,strides=(1,1),name=None)(branch3x3)\n\n    branch5x5 = Conv2D(branch3[0],(1,1), padding=&#39;same&#39;,strides=(1,1),name=None)(x)\n    branch5x5 = Conv2D(branch3[1],(1,1), padding=&#39;same&#39;,strides=(1,1),name=None)(branch5x5)\n\n    branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding=&#39;same&#39;)(x)\n    branchpool = Conv2D(branch4[0],(1,1),padding=&#39;same&#39;,strides=(1,1),name=None)(branchpool)\n\n    x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3)\n\n    return x\n\n#Build InceptionV1 model\ndef InceptionV1(width, height, depth, classes):\n\n    inpt = Input(shape=(width,height,depth))\n\n    x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding=&#39;same&#39;)\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding=&#39;same&#39;)(x)\n    x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding=&#39;same&#39;)\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding=&#39;same&#39;)(x)\n\n    x = Inception(x,[(64,),(96,128),(16,32),(32,)]) #Inception 3a 28x28x256\n    x = Inception(x,[(128,),(128,192),(32,96),(64,)]) #Inception 3b 28x28x480\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding=&#39;same&#39;)(x) #14x14x480\n\n    x = Inception(x,[(192,),(96,208),(16,48),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(160,),(112,224),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(128,),(128,256),(24,64),(64,)]) #Inception 4a 14x14x512\n    x = Inception(x,[(112,),(144,288),(32,64),(64,)]) #Inception 4a 14x14x528\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 4a 14x14x832\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding=&#39;same&#39;)(x) #7x7x832\n\n    x = Inception(x,[(256,),(160,320),(32,128),(128,)]) #Inception 5a 7x7x832\n    x = Inception(x,[(384,),(192,384),(48,128),(128,)]) #Inception 5b 7x7x1024\n\n    #Using AveragePooling replace flatten\n    x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding=&#39;same&#39;)(x)\n    x =Flatten()(x)\n    x = Dropout(0.4)(x)\n    x = Dense(1000,activation=&#39;relu&#39;)(x)\n    x = Dense(classes,activation=&#39;softmax&#39;)(x)\n\n    model=Model(input=inpt,output=x)\n\n    return model\n\nInceptionV1_model = InceptionV1(224,224,3,17)\nInceptionV1_model.summary()\n\nInceptionV1_model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss = &#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;])\nHistory = InceptionV1_model.fit_generator(img_gen.flow(X_train*255, y_train, batch_size = 16),steps_per_epoch = len(X_train)/16, validation_data = (X_test,y_test), epochs = 30 )\n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history[&#39;acc&#39;])\nplt.plot(History.history[&#39;val_acc&#39;])\nplt.title(&#39;model accuracy&#39;)\nplt.ylabel(&#39;accuracy&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\n\nplt.subplot(1,2,2)\nplt.plot(History.history[&#39;loss&#39;])\nplt.plot(History.history[&#39;val_loss&#39;])\nplt.title(&#39;model loss&#39;)\nplt.ylabel(&#39;loss&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\nplt.show()\nplt.show()\n</code></pre>\n<h1 id=\"六、Inception-V3\"><a href=\"#六、Inception-V3\" class=\"headerlink\" title=\"六、Inception V3\"></a>六、Inception V3</h1><p>Christian 和他的团队都是非常高产的研究人员。2015 年 2 月，<strong>Batch-normalized Inception</strong> 被引入作为<strong>Inception V2</strong>。</p>\n<p>2015年12月，他们发布了一个新版本的GoogLeNet(<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1512.00567\" target=\"_blank\" rel=\"noopener\">Inception V3</a>)模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：</p>\n<ul>\n<li>通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps</li>\n<li>当网络层数深度增加时，特征的数量或层的宽度也相对应地增加</li>\n<li>在每一层使用宽度增加以增加下一层之前的特征的组合</li>\n<li><strong>只使用3x3卷积</strong></li>\n</ul>\n<p>因此最后的模型就变成这样了：</p>\n<p>​    <img src=\"c7.png\" alt></p>\n<h1 id=\"七、ResNet\"><a href=\"#七、ResNet\" class=\"headerlink\" title=\"七、ResNet\"></a>七、ResNet</h1><p>2015年12月<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385v1.pdf\" target=\"_blank\" rel=\"noopener\">ResNet</a>发表了，时间上大概与Inception v3网络一起发表的。</p>\n<p>我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，会出现梯度消失、梯度爆炸、网络退化，导致最终性能不佳。 ResNet中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。 </p>\n<p><img src=\"09.png\" alt></p>\n<p>图，在连续2层的卷积层中，将输入x跳着连接至2层后的输出。这里的重点是，通过快捷结构，原来的2层卷积层的输出$F(x)$变成了$F(x) + x$。通过引入这种快捷结构，即使加深层，也能高效地学习。 </p>\n<p>因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。</p>\n<p><img src=\"c8.png\" alt></p>\n<p>ResNet通过以2个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到150层以上，识别精度也会持续提高。并且，在ILSVRC大赛中， ResNet的错误识别率为3.5%（前5类中包含正确解这一精度下的错误识别率），令人称奇。 </p>\n<pre><code class=\"python\">\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate\nfrom keras.models import Model,load_model\nfrom keras.optimizers import Adam\n\n#Load oxflower17 dataset\nimport tflearn.datasets.oxflower17 as oxflower17\nfrom sklearn.model_selection import train_test_split\nx, y = oxflower17.load_data(one_hot=True)\n\n#Split train and test data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle = True)\n\n#Data augumentation with Keras tools\nfrom keras.preprocessing.image import ImageDataGenerator\nimg_gen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n    )\n\n#Define convolution with batchnromalization\ndef Conv2d_BN(x, nb_filter,kernel_size, padding=&#39;same&#39;,strides=(1,1),name=None):\n    if name is not None:\n        bn_name = name + &#39;_bn&#39;\n        conv_name = name + &#39;_conv&#39;\n    else:\n        bn_name = None\n        conv_name = None\n\n    x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation=&#39;relu&#39;,name=conv_name)(x)\n    x = BatchNormalization(axis=3,name=bn_name)(x)\n    return x\n\n#Define Residual Block for ResNet34(2 convolution layers)\ndef Residual_Block(input_model,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut =False):\n    x = Conv2d_BN(input_model,nb_filter=nb_filter,kernel_size=kernel_size,strides=strides,padding=&#39;same&#39;)\n    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size,padding=&#39;same&#39;)\n\n    #need convolution on shortcut for add different channel\n    if with_conv_shortcut:\n        shortcut = Conv2d_BN(input_model,nb_filter=nb_filter,strides=strides,kernel_size=kernel_size)\n        x = add([x,shortcut])\n        return x\n    else:\n        x = add([x,input_model])\n        return x\n\n#Built ResNet34\ndef ResNet34(width, height, depth, classes):\n\n    Img = Input(shape=(width,height,depth))\n\n    x = Conv2d_BN(Img,64,(7,7),strides=(2,2),padding=&#39;same&#39;)\n    x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding=&#39;same&#39;)(x)  \n\n    #Residual conv2_x ouput 56x56x64 \n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=64,kernel_size=(3,3))\n\n    #Residual conv3_x ouput 28x28x128 \n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=128,kernel_size=(3,3))\n\n    #Residual conv4_x ouput 14x14x256\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)# need do convolution to add different channel\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=256,kernel_size=(3,3))\n\n    #Residual conv5_x ouput 7x7x512\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n    x = Residual_Block(x,nb_filter=512,kernel_size=(3,3))\n\n\n    #Using AveragePooling replace flatten\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(classes,activation=&#39;softmax&#39;)(x)\n\n    model=Model(input=Img,output=x)\n    return model  \n\n#Plot Loss and accuracy\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.plot(History.history[&#39;acc&#39;])\nplt.plot(History.history[&#39;val_acc&#39;])\nplt.title(&#39;model accuracy&#39;)\nplt.ylabel(&#39;accuracy&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\n\nplt.subplot(1,2,2)\nplt.plot(History.history[&#39;loss&#39;])\nplt.plot(History.history[&#39;val_loss&#39;])\nplt.title(&#39;model loss&#39;)\nplt.ylabel(&#39;loss&#39;)\nplt.xlabel(&#39;epoch&#39;)\nplt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)\nplt.show()\nplt.show()</code></pre>\n<h1 id=\"八、Inception-v4-和-Inception-ResNet\"><a href=\"#八、Inception-v4-和-Inception-ResNet\" class=\"headerlink\" title=\"八、Inception v4 和 Inception-ResNet\"></a>八、Inception v4 和 Inception-ResNet</h1><p>2016年2月</p>\n<p>Inception v4 和 Inception -ResNet 在同一篇论文[《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》][<a href=\"https://arxiv.org/abs/1602.07261].首先说明一下Inception\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1602.07261].首先说明一下Inception</a> v4<strong>没有</strong>使用残差学习的思想, 而出自同一篇论文的Inception-Resnet-v1和Inception-Resnet-v2才是Inception module与残差学习的结合产物。Inception-ResNet和Inception v4网络结构都是基于Inception v3的改进。</p>\n<p><strong>Inception v4中的三个基本模块</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v1.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v2.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v3.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<ol>\n<li><p>左图是基本的Inception v2/v3模块，使用两个3x3卷积代替5x5卷积，并且使用average pooling，该模块主要处理尺寸为35x35的feature map；</p>\n</li>\n<li><p>中图模块使用1xn和nx1卷积代替nxn卷积，同样使用average pooling，该模块主要处理尺寸为17x17的feature map；</p>\n</li>\n<li><p>右图在原始的8x8处理模块上将3x3卷积用1x3卷积和3x1卷积。 </p>\n</li>\n</ol>\n<p>总的来说，Inception v4中基本的Inception module还是沿袭了Inception v2/v3的结构，只是结构看起来更加简洁统一，并且使用更多的Inception module，实验效果也更好。</p>\n<p>下图左图为Inception v4的网络结构，右图为Inception v4的Stem模块：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v10.png\" width=\"250\" height=\"350\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v11.png\" width=\"250\" height=\"350\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n<p><strong>Inception-Resnet-v1基本模块</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v4.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v5.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v6.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<ol>\n<li>Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息；</li>\n<li>Inception module每个分支都没有使用pooling；</li>\n<li>每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。</li>\n</ol>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v12.png\" width=\"250\" height=\"350\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v13.png\" width=\"250\" height=\"350\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<p><strong>Inception-Resnet-v2基本模块：</strong>：</p>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v7.png\" width=\"180\" height=\"240\"></div>\n<div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v8.png\" width=\"180\" height=\"240\"></div><div style=\"float:left;border:solid 1px 000;margin:2px;\"><img src=\"v9.png\" width=\"180\" height=\"240\"></div><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n\n\n\n<p>Inception-Resnet-v2网络结构同Inception-Resnet-v1，Stem模块同Inception v4</p>\n<h1 id=\"九、Xception\"><a href=\"#九、Xception\" class=\"headerlink\" title=\"九、Xception\"></a>九、Xception</h1><p>2016年８月</p>\n<p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.02357\" target=\"_blank\" rel=\"noopener\">Xception</a>是google继Inception后提出的对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>\n<p><strong>结构的变形过程如下</strong>：</p>\n<ul>\n<li><p>在 Inception 中，特征可以通过 1×1卷积，3×3卷积，5×5 卷积，pooling 等进行提取，Inception 结构将特征类型的选择留给网络自己训练，也就是将一个输入同时输给几种提取特征方式，然后做 concat 。Inception-v3的结构图如下:</p>\n<p><img src=\"x1.png\" alt></p>\n</li>\n<li><p>对 Inception-v3 进行简化，去除 Inception-v3 中的 avg pool 后，输入的下一步操作就都是 1×1卷积：</p>\n<p><img src=\"x2.png\" alt></p>\n</li>\n<li><p>提取 1×1卷积的公共部分：</p>\n<p><img src=\"x3.png\" alt></p>\n</li>\n<li><p>Xception（<strong>极致的 Inception</strong>）：先进行普通卷积操作，再对 1×1卷积后的每个channel分别进行 3×3卷积操作，最后将结果 concat：</p>\n<p><img src=\"x4.png\" alt></p>\n</li>\n</ul>\n<p><strong>深度可分离卷积 Depthwise Separable Convolution</strong></p>\n<p>传统卷积的实现过程：</p>\n<p><img src=\"x5.png\" alt></p>\n<p>Depthwise Separable Convolution 的实现过程：</p>\n<p><img src=\"x6.png\" alt></p>\n<p><strong>Depthwise Separable Convolution 与 极致的 Inception 区别：</strong></p>\n<p>极致的 Inception：</p>\n<p>​    第一步：普通 1×1卷积。</p>\n<p>​    第二步：对 1×1卷积结果的每个 channel，分别进行 3×3卷积操作，并将结果 concat。</p>\n<p>Depthwise Separable Convolution：</p>\n<p>​    第一步：Depthwise 卷积，对输入的每个channel，分别进行 3×3 卷积操作，并将结果 concat。</p>\n<p>​    第二步：Pointwise 卷积，对 Depthwise 卷积中的 concat 结果，进行 1×1卷积操作。</p>\n<p>两种操作的循序不一致：Inception 先进行 1×1卷积，再进行 3×3卷积；Depthwise Separable Convolution 先进行 3×3卷积，再进行 1×11×1 卷积。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p>​    <a href=\"https://www.cnblogs.com/CZiFan/p/9490565.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/CZiFan/p/9490565.html</a></p>\n<p>​    <a href=\"https://www.zhihu.com/question/53727257/answer/136261195\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/53727257/answer/136261195</a></p>\n<p>​    <a href=\"https://blog.csdn.net/lk3030/article/details/84847879\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/lk3030/article/details/84847879</a></p>\n<p>​    <a href=\"https://blog.csdn.net/zzc15806/article/details/83504130\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/zzc15806/article/details/83504130</a></p>\n<p><a href=\"https://medium.com/雞雞與兔兔的工程世界/機器學習-ml-note-cnn演化史-alexnet-vgg-inception-resnet-keras-coding-668f74879306\" target=\"_blank\" rel=\"noopener\">https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-cnn%E6%BC%94%E5%8C%96%E5%8F%B2-alexnet-vgg-inception-resnet-keras-coding-668f74879306</a></p>\n"},{"title":"深度学习之优化算法","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-28T09:00:22.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、前言\n\n​\t神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化(optimization).\n\n​\t使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法(stochastic gradient descent)，简称SGD。 但SGD也有它的缺点，根据不同的问题，也存在比SGD更好的方法。\n\n# 二、SGD\n\n深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.\n\n数学公式\n$$\nW \\leftarrow W - \\eta \\frac {\\partial L}{\\partial W}\n$$\n\n\n**代码实现**\n\n```python\nclass SGD:\n\n    \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        \"\"\"\n        更新权重\n        :param params: 权重, 字典，params['W1'], ..\n        :param grads: 梯度, 字典, grads['w1']\n        :return:\n        \"\"\"\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n```\n\n\n\n**SGD的缺点**\n\n1. 选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.\n2. SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.\n\n​\t![](sgd.png)\n\n​\t为了改正SGD的缺点，下面我们将使用Momentum、 AdaGrad、Adam这3种方法来取代SGD。\n\n# 三、Momentum\n\n​\tMomentum是\"动量\"的意思。动量方法旨在加速学习，特别是在面对小而连续的梯度但是含有很多噪声的时候。动量模拟了物体运动的惯性，即在更新的时候在一定程度上会考虑之前更新的方向，同时利用当前batch的梯度微调最终的结果。这样则可以在一定程度上增加稳定性，从而更快的学习。\n\n**数学公式**\n$$\n\\upsilon \\leftarrow \\alpha \\upsilon - \\eta \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W + \\upsilon\n$$\n\n\n\n**代码实现**\n\n```python\nclass Momentum:\n\n    \"\"\"Momentum SGD\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None: # 初始化v\n            self.v = {}\n            for key, val in params.items():                                \n                self.v[key] = np.zeros_like(val)\n                \n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n            params[key] += self.v[key]\n```\n\n\n\n![](mom.png)\n\n​\t\t\t\t\t\t\t\t**基于Momentum的最优化的更新路径**\n\n​\t和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。\n\n\n\n**特点**\n\n1. 下降初期，使用上一次参数更新，当下降方向一致时能够加速学习。\n2. 下降中后期，在局部最小值附近来回震荡，gradient$\\rightarrow$0\n3. 在梯度改变方向时，能减少更新。\n4. 总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。\n\n\n\n# 四、AdaGrad\n\n​\t在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。\n\n​\t在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。\n\n​\t和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。\n\n**数学公式**\n$$\nh \\leftarrow h + \\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}\n$$\n\n在更新参数时，通过乘以$\\frac {1}{\\sqrt h}$ ，就可以调整学习的尺度\n\n\n\n**代码实现**\n\n```python\nclass AdaGrad:\n\n    \"\"\"AdaGrad\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n            \n # 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7\n```\n\n​\t![](adagrad.png)\n\n​\t\t\t\t\t\t\t\t**基于AdaGrad的最优化的更新路径**\n\n​\t由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。\n\n**特点**\n\n1. 前期放大梯度，加速学习，后期约束梯度\n2. 适合处理稀疏梯度\n\n**缺点**\n\n​\t中后期，分母上梯度的平方的积累将会越来越大，使gradient-->0, 使得训练提前结束。\n\n\n\n# 五、RMSProp\n\nRMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度。\n\n**数学公式**\n$$\nh \\leftarrow \\alpha h + (1-\\alpha)\\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}\n$$\n\n**代码实现**\n\n```python\nclass RMSprop:\n\n    \"\"\"RMSprop\"\"\"\n\n    def __init__(self, lr=0.01, decay_rate = 0.99):\n        self.lr = lr\n        self.decay_rate = decay_rate\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] *= self.decay_rate\n            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n```\n\n**优点**\n\n1. 由于采用了梯度平方的指数加权平均，改进了AdaGrad在深度学习中过早结束的问题\n2. 适用于处理非平稳过程，-对RNN效果较好\n\n\n\n\n\n# 六、Adam\n\n​\tAdam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正.\n\n​\t![](adam.png)\n\n​\tAdam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\\beta_1$和二次momentum系数$\\beta_2$。根据论文，标准的设定值是$\\beta_1$为0.9， $\\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。\n\n**特点**\n\n1. 结合了AdaGrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点\n2.  对内存需求较小（指数加权平均不需要存储大量的值就能平均）\n3. 为不同的参数计算不同的自适应学习率\n4. 适用于大多非凸优化 - 适用于大数据集和高维空间\n\n\n\n# 七、不同算法比较\n\n​\t![](v1.webp)\n\n\n\n![](v2.webp)\n\n\n\n1. 如果数据是稀疏的，就用自适应算法, 即AdaGrad, Adadelta, RMSProp, Adam\n2. RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。\n3. Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。\n4. SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。\n\n# 八、参考\n\n  《深度学习入门: 基于Python的理论与实现》\n\n​\thttps://blog.csdn.net/qq_28031525/article/details/79535942\n\n​\thttps://www.cnblogs.com/guoyaohua/p/8542554.html","source":"_posts/深度学习之优化算法.md","raw":"---\ntitle: 深度学习之优化算法\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-28 17:00:22\npassword:\nsummary:\ntags: \n- DL\n- python\ncategories: 深度学习\n---\n\n\n\n# 一、前言\n\n​\t神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化(optimization).\n\n​\t使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法(stochastic gradient descent)，简称SGD。 但SGD也有它的缺点，根据不同的问题，也存在比SGD更好的方法。\n\n# 二、SGD\n\n深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.\n\n数学公式\n$$\nW \\leftarrow W - \\eta \\frac {\\partial L}{\\partial W}\n$$\n\n\n**代码实现**\n\n```python\nclass SGD:\n\n    \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        \"\"\"\n        更新权重\n        :param params: 权重, 字典，params['W1'], ..\n        :param grads: 梯度, 字典, grads['w1']\n        :return:\n        \"\"\"\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n```\n\n\n\n**SGD的缺点**\n\n1. 选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.\n2. SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.\n\n​\t![](sgd.png)\n\n​\t为了改正SGD的缺点，下面我们将使用Momentum、 AdaGrad、Adam这3种方法来取代SGD。\n\n# 三、Momentum\n\n​\tMomentum是\"动量\"的意思。动量方法旨在加速学习，特别是在面对小而连续的梯度但是含有很多噪声的时候。动量模拟了物体运动的惯性，即在更新的时候在一定程度上会考虑之前更新的方向，同时利用当前batch的梯度微调最终的结果。这样则可以在一定程度上增加稳定性，从而更快的学习。\n\n**数学公式**\n$$\n\\upsilon \\leftarrow \\alpha \\upsilon - \\eta \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W + \\upsilon\n$$\n\n\n\n**代码实现**\n\n```python\nclass Momentum:\n\n    \"\"\"Momentum SGD\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None: # 初始化v\n            self.v = {}\n            for key, val in params.items():                                \n                self.v[key] = np.zeros_like(val)\n                \n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n            params[key] += self.v[key]\n```\n\n\n\n![](mom.png)\n\n​\t\t\t\t\t\t\t\t**基于Momentum的最优化的更新路径**\n\n​\t和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。\n\n\n\n**特点**\n\n1. 下降初期，使用上一次参数更新，当下降方向一致时能够加速学习。\n2. 下降中后期，在局部最小值附近来回震荡，gradient$\\rightarrow$0\n3. 在梯度改变方向时，能减少更新。\n4. 总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。\n\n\n\n# 四、AdaGrad\n\n​\t在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。\n\n​\t在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。\n\n​\t和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。\n\n**数学公式**\n$$\nh \\leftarrow h + \\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}\n$$\n\n在更新参数时，通过乘以$\\frac {1}{\\sqrt h}$ ，就可以调整学习的尺度\n\n\n\n**代码实现**\n\n```python\nclass AdaGrad:\n\n    \"\"\"AdaGrad\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n            \n # 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7\n```\n\n​\t![](adagrad.png)\n\n​\t\t\t\t\t\t\t\t**基于AdaGrad的最优化的更新路径**\n\n​\t由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。\n\n**特点**\n\n1. 前期放大梯度，加速学习，后期约束梯度\n2. 适合处理稀疏梯度\n\n**缺点**\n\n​\t中后期，分母上梯度的平方的积累将会越来越大，使gradient-->0, 使得训练提前结束。\n\n\n\n# 五、RMSProp\n\nRMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度。\n\n**数学公式**\n$$\nh \\leftarrow \\alpha h + (1-\\alpha)\\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}\n$$\n\n$$\nW \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}\n$$\n\n**代码实现**\n\n```python\nclass RMSprop:\n\n    \"\"\"RMSprop\"\"\"\n\n    def __init__(self, lr=0.01, decay_rate = 0.99):\n        self.lr = lr\n        self.decay_rate = decay_rate\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] *= self.decay_rate\n            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n```\n\n**优点**\n\n1. 由于采用了梯度平方的指数加权平均，改进了AdaGrad在深度学习中过早结束的问题\n2. 适用于处理非平稳过程，-对RNN效果较好\n\n\n\n\n\n# 六、Adam\n\n​\tAdam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正.\n\n​\t![](adam.png)\n\n​\tAdam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\\beta_1$和二次momentum系数$\\beta_2$。根据论文，标准的设定值是$\\beta_1$为0.9， $\\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。\n\n**特点**\n\n1. 结合了AdaGrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点\n2.  对内存需求较小（指数加权平均不需要存储大量的值就能平均）\n3. 为不同的参数计算不同的自适应学习率\n4. 适用于大多非凸优化 - 适用于大数据集和高维空间\n\n\n\n# 七、不同算法比较\n\n​\t![](v1.webp)\n\n\n\n![](v2.webp)\n\n\n\n1. 如果数据是稀疏的，就用自适应算法, 即AdaGrad, Adadelta, RMSProp, Adam\n2. RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。\n3. Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。\n4. SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。\n\n# 八、参考\n\n  《深度学习入门: 基于Python的理论与实现》\n\n​\thttps://blog.csdn.net/qq_28031525/article/details/79535942\n\n​\thttps://www.cnblogs.com/guoyaohua/p/8542554.html","slug":"深度学习之优化算法","published":1,"updated":"2019-12-02T07:25:24.184Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tso000szsv5npcie8vs","content":"<h1 id=\"一、前言\"><a href=\"#一、前言\" class=\"headerlink\" title=\"一、前言\"></a>一、前言</h1><p>​    神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化(optimization).</p>\n<p>​    使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法(stochastic gradient descent)，简称SGD。 但SGD也有它的缺点，根据不同的问题，也存在比SGD更好的方法。</p>\n<h1 id=\"二、SGD\"><a href=\"#二、SGD\" class=\"headerlink\" title=\"二、SGD\"></a>二、SGD</h1><p>深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.</p>\n<p>数学公式<br>$$<br>W \\leftarrow W - \\eta \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">SGD</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token triple-quoted-string string\">\"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">=</span> lr\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">update</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> params<span class=\"token punctuation\">,</span> grads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"\n        更新权重\n        :param params: 权重, 字典，params['W1'], ..\n        :param grads: 梯度, 字典, grads['w1']\n        :return:\n        \"\"\"</span>\n        <span class=\"token keyword\">for</span> key <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            params<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">-=</span> self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>SGD的缺点</strong></p>\n<ol>\n<li>选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.</li>\n<li>SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.</li>\n</ol>\n<p>​    <img src=\"sgd.png\" alt></p>\n<p>​    为了改正SGD的缺点，下面我们将使用Momentum、 AdaGrad、Adam这3种方法来取代SGD。</p>\n<h1 id=\"三、Momentum\"><a href=\"#三、Momentum\" class=\"headerlink\" title=\"三、Momentum\"></a>三、Momentum</h1><p>​    Momentum是”动量”的意思。动量方法旨在加速学习，特别是在面对小而连续的梯度但是含有很多噪声的时候。动量模拟了物体运动的惯性，即在更新的时候在一定程度上会考虑之前更新的方向，同时利用当前batch的梯度微调最终的结果。这样则可以在一定程度上增加稳定性，从而更快的学习。</p>\n<p><strong>数学公式</strong><br>$$<br>\\upsilon \\leftarrow \\alpha \\upsilon - \\eta \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W + \\upsilon<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Momentum</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token triple-quoted-string string\">\"\"\"Momentum SGD\"\"\"</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">,</span> momentum<span class=\"token operator\">=</span><span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">=</span> lr\n        self<span class=\"token punctuation\">.</span>momentum <span class=\"token operator\">=</span> momentum\n        self<span class=\"token punctuation\">.</span>v <span class=\"token operator\">=</span> None\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">update</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> params<span class=\"token punctuation\">,</span> grads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>v <span class=\"token keyword\">is</span> None<span class=\"token punctuation\">:</span> <span class=\"token comment\" spellcheck=\"true\"># 初始化v</span>\n            self<span class=\"token punctuation\">.</span>v <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n            <span class=\"token keyword\">for</span> key<span class=\"token punctuation\">,</span> val <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>                                \n                self<span class=\"token punctuation\">.</span>v<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros_like<span class=\"token punctuation\">(</span>val<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> key <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>v<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>momentum<span class=\"token operator\">*</span>self<span class=\"token punctuation\">.</span>v<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">-</span> self<span class=\"token punctuation\">.</span>lr<span class=\"token operator\">*</span>grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> \n            params<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> self<span class=\"token punctuation\">.</span>v<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><img src=\"mom.png\" alt></p>\n<p>​                                <strong>基于Momentum的最优化的更新路径</strong></p>\n<p>​    和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>下降初期，使用上一次参数更新，当下降方向一致时能够加速学习。</li>\n<li>下降中后期，在局部最小值附近来回震荡，gradient$\\rightarrow$0</li>\n<li>在梯度改变方向时，能减少更新。</li>\n<li>总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。</li>\n</ol>\n<h1 id=\"四、AdaGrad\"><a href=\"#四、AdaGrad\" class=\"headerlink\" title=\"四、AdaGrad\"></a>四、AdaGrad</h1><p>​    在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。</p>\n<p>​    在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。</p>\n<p>​    和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。</p>\n<p><strong>数学公式</strong><br>$$<br>h \\leftarrow h + \\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>在更新参数时，通过乘以$\\frac {1}{\\sqrt h}$ ，就可以调整学习的尺度</p>\n<p><strong>代码实现</strong></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">AdaGrad</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token triple-quoted-string string\">\"\"\"AdaGrad\"\"\"</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">=</span> lr\n        self<span class=\"token punctuation\">.</span>h <span class=\"token operator\">=</span> None\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">update</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> params<span class=\"token punctuation\">,</span> grads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>h <span class=\"token keyword\">is</span> None<span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>h <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n            <span class=\"token keyword\">for</span> key<span class=\"token punctuation\">,</span> val <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros_like<span class=\"token punctuation\">(</span>val<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> key <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span>\n            params<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">-=</span> self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span>\n\n <span class=\"token comment\" spellcheck=\"true\"># 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>​    <img src=\"adagrad.png\" alt></p>\n<p>​                                <strong>基于AdaGrad的最优化的更新路径</strong></p>\n<p>​    由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>前期放大梯度，加速学习，后期约束梯度</li>\n<li>适合处理稀疏梯度</li>\n</ol>\n<p><strong>缺点</strong></p>\n<p>​    中后期，分母上梯度的平方的积累将会越来越大，使gradient–&gt;0, 使得训练提前结束。</p>\n<h1 id=\"五、RMSProp\"><a href=\"#五、RMSProp\" class=\"headerlink\" title=\"五、RMSProp\"></a>五、RMSProp</h1><p>RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度。</p>\n<p><strong>数学公式</strong><br>$$<br>h \\leftarrow \\alpha h + (1-\\alpha)\\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">RMSprop</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token triple-quoted-string string\">\"\"\"RMSprop\"\"\"</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">,</span> decay_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.99</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">=</span> lr\n        self<span class=\"token punctuation\">.</span>decay_rate <span class=\"token operator\">=</span> decay_rate\n        self<span class=\"token punctuation\">.</span>h <span class=\"token operator\">=</span> None\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">update</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> params<span class=\"token punctuation\">,</span> grads<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>h <span class=\"token keyword\">is</span> None<span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>h <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span>\n            <span class=\"token keyword\">for</span> key<span class=\"token punctuation\">,</span> val <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>items<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros_like<span class=\"token punctuation\">(</span>val<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> key <span class=\"token keyword\">in</span> params<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">*=</span> self<span class=\"token punctuation\">.</span>decay_rate\n            self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> self<span class=\"token punctuation\">.</span>decay_rate<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span>\n            params<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">-=</span> self<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">*</span> grads<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span> <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>h<span class=\"token punctuation\">[</span>key<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>优点</strong></p>\n<ol>\n<li>由于采用了梯度平方的指数加权平均，改进了AdaGrad在深度学习中过早结束的问题</li>\n<li>适用于处理非平稳过程，-对RNN效果较好</li>\n</ol>\n<h1 id=\"六、Adam\"><a href=\"#六、Adam\" class=\"headerlink\" title=\"六、Adam\"></a>六、Adam</h1><p>​    Adam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正.</p>\n<p>​    <img src=\"adam.png\" alt></p>\n<p>​    Adam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\\beta_1$和二次momentum系数$\\beta_2$。根据论文，标准的设定值是$\\beta_1$为0.9， $\\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>结合了AdaGrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>\n<li>对内存需求较小（指数加权平均不需要存储大量的值就能平均）</li>\n<li>为不同的参数计算不同的自适应学习率</li>\n<li>适用于大多非凸优化 - 适用于大数据集和高维空间</li>\n</ol>\n<h1 id=\"七、不同算法比较\"><a href=\"#七、不同算法比较\" class=\"headerlink\" title=\"七、不同算法比较\"></a>七、不同算法比较</h1><p>​    <img src=\"v1.webp\" alt></p>\n<p><img src=\"v2.webp\" alt></p>\n<ol>\n<li>如果数据是稀疏的，就用自适应算法, 即AdaGrad, Adadelta, RMSProp, Adam</li>\n<li>RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。</li>\n<li>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</li>\n<li>SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</li>\n</ol>\n<h1 id=\"八、参考\"><a href=\"#八、参考\" class=\"headerlink\" title=\"八、参考\"></a>八、参考</h1><p>  《深度学习入门: 基于Python的理论与实现》</p>\n<p>​    <a href=\"https://blog.csdn.net/qq_28031525/article/details/79535942\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_28031525/article/details/79535942</a></p>\n<p>​    <a href=\"https://www.cnblogs.com/guoyaohua/p/8542554.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/guoyaohua/p/8542554.html</a></p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、前言\"><a href=\"#一、前言\" class=\"headerlink\" title=\"一、前言\"></a>一、前言</h1><p>​    神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化(optimization).</p>\n<p>​    使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法(stochastic gradient descent)，简称SGD。 但SGD也有它的缺点，根据不同的问题，也存在比SGD更好的方法。</p>\n<h1 id=\"二、SGD\"><a href=\"#二、SGD\" class=\"headerlink\" title=\"二、SGD\"></a>二、SGD</h1><p>深度学习中的SGD指mini-batch gradient descent。 在训练过程中，采用固定的学习率.</p>\n<p>数学公式<br>$$<br>W \\leftarrow W - \\eta \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre><code class=\"python\">class SGD:\n\n    &quot;&quot;&quot;随机梯度下降法（Stochastic Gradient Descent）&quot;&quot;&quot;\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n\n    def update(self, params, grads):\n        &quot;&quot;&quot;\n        更新权重\n        :param params: 权重, 字典，params[&#39;W1&#39;], ..\n        :param grads: 梯度, 字典, grads[&#39;w1&#39;]\n        :return:\n        &quot;&quot;&quot;\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] </code></pre>\n<p><strong>SGD的缺点</strong></p>\n<ol>\n<li>选择合适的learning rate 比较困难, 且对所有的参数更新使用同样的learning rate.</li>\n<li>SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点.</li>\n</ol>\n<p>​    <img src=\"sgd.png\" alt></p>\n<p>​    为了改正SGD的缺点，下面我们将使用Momentum、 AdaGrad、Adam这3种方法来取代SGD。</p>\n<h1 id=\"三、Momentum\"><a href=\"#三、Momentum\" class=\"headerlink\" title=\"三、Momentum\"></a>三、Momentum</h1><p>​    Momentum是”动量”的意思。动量方法旨在加速学习，特别是在面对小而连续的梯度但是含有很多噪声的时候。动量模拟了物体运动的惯性，即在更新的时候在一定程度上会考虑之前更新的方向，同时利用当前batch的梯度微调最终的结果。这样则可以在一定程度上增加稳定性，从而更快的学习。</p>\n<p><strong>数学公式</strong><br>$$<br>\\upsilon \\leftarrow \\alpha \\upsilon - \\eta \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W + \\upsilon<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre><code class=\"python\">class Momentum:\n\n    &quot;&quot;&quot;Momentum SGD&quot;&quot;&quot;\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None: # 初始化v\n            self.v = {}\n            for key, val in params.items():                                \n                self.v[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n            params[key] += self.v[key]</code></pre>\n<p><img src=\"mom.png\" alt></p>\n<p>​                                <strong>基于Momentum的最优化的更新路径</strong></p>\n<p>​    和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然x轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然y轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以y轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝x轴方向靠近，减弱“之”字形的变动程度。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>下降初期，使用上一次参数更新，当下降方向一致时能够加速学习。</li>\n<li>下降中后期，在局部最小值附近来回震荡，gradient$\\rightarrow$0</li>\n<li>在梯度改变方向时，能减少更新。</li>\n<li>总体而言，momentum能够在相关方向上加速学习，抑制震荡，从而加速收敛。</li>\n</ol>\n<h1 id=\"四、AdaGrad\"><a href=\"#四、AdaGrad\" class=\"headerlink\" title=\"四、AdaGrad\"></a>四、AdaGrad</h1><p>​    在神经网络的学习中，学习率（数学式中记为η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。</p>\n<p>​    在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。</p>\n<p>​    和Momentum直接把动量累加到梯度上不同，它是通过动量逐步减小学习率的值，使得最后的值在最小值附近，更加接近收敛点。</p>\n<p><strong>数学公式</strong><br>$$<br>h \\leftarrow h + \\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>在更新参数时，通过乘以$\\frac {1}{\\sqrt h}$ ，就可以调整学习的尺度</p>\n<p><strong>代码实现</strong></p>\n<pre><code class=\"python\">class AdaGrad:\n\n    &quot;&quot;&quot;AdaGrad&quot;&quot;&quot;\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n\n # 为了防止当self.h[key]中有0时，将0用作除数的情况。添加了1e-7</code></pre>\n<p>​    <img src=\"adagrad.png\" alt></p>\n<p>​                                <strong>基于AdaGrad的最优化的更新路径</strong></p>\n<p>​    由图可知，函数的取值高效地向着最小值移动。由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>前期放大梯度，加速学习，后期约束梯度</li>\n<li>适合处理稀疏梯度</li>\n</ol>\n<p><strong>缺点</strong></p>\n<p>​    中后期，分母上梯度的平方的积累将会越来越大，使gradient–&gt;0, 使得训练提前结束。</p>\n<h1 id=\"五、RMSProp\"><a href=\"#五、RMSProp\" class=\"headerlink\" title=\"五、RMSProp\"></a>五、RMSProp</h1><p>RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度。</p>\n<p><strong>数学公式</strong><br>$$<br>h \\leftarrow \\alpha h + (1-\\alpha)\\frac {\\partial L}{\\partial W} \\cdot \\frac {\\partial L}{\\partial W}<br>$$</p>\n<p>$$<br>W \\leftarrow W - \\eta \\frac {1}{\\sqrt h}\\frac {\\partial L}{\\partial W}<br>$$</p>\n<p><strong>代码实现</strong></p>\n<pre><code class=\"python\">class RMSprop:\n\n    &quot;&quot;&quot;RMSprop&quot;&quot;&quot;\n\n    def __init__(self, lr=0.01, decay_rate = 0.99):\n        self.lr = lr\n        self.decay_rate = decay_rate\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n\n        for key in params.keys():\n            self.h[key] *= self.decay_rate\n            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)</code></pre>\n<p><strong>优点</strong></p>\n<ol>\n<li>由于采用了梯度平方的指数加权平均，改进了AdaGrad在深度学习中过早结束的问题</li>\n<li>适用于处理非平稳过程，-对RNN效果较好</li>\n</ol>\n<h1 id=\"六、Adam\"><a href=\"#六、Adam\" class=\"headerlink\" title=\"六、Adam\"></a>六、Adam</h1><p>​    Adam (Adaptive Moment Estimation)本质上是带有动量项的RMSProp。Adam的优点主要在于参数偏置校正.</p>\n<p>​    <img src=\"adam.png\" alt></p>\n<p>​    Adam会设置3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数$\\beta_1$和二次momentum系数$\\beta_2$。根据论文，标准的设定值是$\\beta_1$为0.9， $\\beta_2$ 为0.999。设置了这些值后，大多数情况下都能顺利运行。</p>\n<p><strong>特点</strong></p>\n<ol>\n<li>结合了AdaGrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>\n<li>对内存需求较小（指数加权平均不需要存储大量的值就能平均）</li>\n<li>为不同的参数计算不同的自适应学习率</li>\n<li>适用于大多非凸优化 - 适用于大数据集和高维空间</li>\n</ol>\n<h1 id=\"七、不同算法比较\"><a href=\"#七、不同算法比较\" class=\"headerlink\" title=\"七、不同算法比较\"></a>七、不同算法比较</h1><p>​    <img src=\"v1.webp\" alt></p>\n<p><img src=\"v2.webp\" alt></p>\n<ol>\n<li>如果数据是稀疏的，就用自适应算法, 即AdaGrad, Adadelta, RMSProp, Adam</li>\n<li>RMSProp, Adadelta, Adam 在很多情况下的效果是相似的。</li>\n<li>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</li>\n<li>SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</li>\n</ol>\n<h1 id=\"八、参考\"><a href=\"#八、参考\" class=\"headerlink\" title=\"八、参考\"></a>八、参考</h1><p>  《深度学习入门: 基于Python的理论与实现》</p>\n<p>​    <a href=\"https://blog.csdn.net/qq_28031525/article/details/79535942\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_28031525/article/details/79535942</a></p>\n<p>​    <a href=\"https://www.cnblogs.com/guoyaohua/p/8542554.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/guoyaohua/p/8542554.html</a></p>\n"},{"title":"深度学习之过拟合","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-12-29T13:48:28.000Z","password":null,"summary":null,"_content":"\n\n\n\n\n# 一、过拟合\n\n机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。\n\n发生过拟合的原因，主要有以下两个：\n\n- 模型拥有大量参数、表现力强（模型太复杂）\n- 训练数据少\n\n\n\n# 二、如何减少过拟合\n\n## 1、获取更多数据\n\n​\t获取更多的数据，从源头上解决问题，比如数据增强\n\n\n\n## 2、正则化\n\n​\t该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。\n\n**L1正则化**\n$$\nL_1(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^n|w_i|\n$$\n**L2正则化(权值衰减)**\n$$\nL_2(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^nw_i^2\n$$\n\n- L1减少的是一个常量，L2减少的是权重的固定比例\n- L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。\n- 实践中L2正则化通常优于L1正则化\n\n\n\n## 3、权重初始化\n\n上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习\n的方法。通过减小权重参数的值来抑制过拟合的发生。 \n\n如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 \n\n为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始\n值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行\n相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 \n\n各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 \n\nXavier：如果前一层的节点数为n，则初始值使用标准差为$\\frac{1}{\\sqrt n}$的分布 \n\nXavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\\frac{2}{\\sqrt n}$的高斯分布。当Xavier初始值是$\\frac{1}{\\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 \n\n\n\n## 4、Batch Normalization\n\n如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 \n\n为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。\n\n- 可以使学习快速进行（可以增大学习率）。\n- 不那么依赖初始值（对于初始值不用那么神经质）。\n- 抑制过拟合（降低Dropout等的必要性）。 \n\n![](02.png)\n\nBatch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 \n\n接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用\n数学式可以如下表示。 \n$$\ny_i \\leftarrow \\gamma x_i + \\beta\n$$\n这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合\n适的值。 \n\n## 5、Dropout\n\n​\t如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。\n\n![](01.png)\n\n​\tDropout是一种在学习的过程中随机删除神经元的方法。训练时，随机\n选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递\n\n\n\n## 6、早期停止法\n\n我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合\n# 参考\n\n\n\nhttps://www.cnblogs.com/skyfsm/p/8456968.html","source":"_posts/深度学习之过拟合.md","raw":"---\ntitle: 深度学习之过拟合\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-12-29 21:48:28\npassword:\nsummary:\ntags: \n- DL\n- Python\ncategories: 深度学习\n---\n\n\n\n\n\n# 一、过拟合\n\n机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。\n\n发生过拟合的原因，主要有以下两个：\n\n- 模型拥有大量参数、表现力强（模型太复杂）\n- 训练数据少\n\n\n\n# 二、如何减少过拟合\n\n## 1、获取更多数据\n\n​\t获取更多的数据，从源头上解决问题，比如数据增强\n\n\n\n## 2、正则化\n\n​\t该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。\n\n**L1正则化**\n$$\nL_1(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^n|w_i|\n$$\n**L2正则化(权值衰减)**\n$$\nL_2(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^nw_i^2\n$$\n\n- L1减少的是一个常量，L2减少的是权重的固定比例\n- L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。\n- 实践中L2正则化通常优于L1正则化\n\n\n\n## 3、权重初始化\n\n上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习\n的方法。通过减小权重参数的值来抑制过拟合的发生。 \n\n如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 \n\n为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始\n值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行\n相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 \n\n各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 \n\nXavier：如果前一层的节点数为n，则初始值使用标准差为$\\frac{1}{\\sqrt n}$的分布 \n\nXavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\\frac{2}{\\sqrt n}$的高斯分布。当Xavier初始值是$\\frac{1}{\\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 \n\n\n\n## 4、Batch Normalization\n\n如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 \n\n为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。\n\n- 可以使学习快速进行（可以增大学习率）。\n- 不那么依赖初始值（对于初始值不用那么神经质）。\n- 抑制过拟合（降低Dropout等的必要性）。 \n\n![](02.png)\n\nBatch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 \n\n接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用\n数学式可以如下表示。 \n$$\ny_i \\leftarrow \\gamma x_i + \\beta\n$$\n这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合\n适的值。 \n\n## 5、Dropout\n\n​\t如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。\n\n![](01.png)\n\n​\tDropout是一种在学习的过程中随机删除神经元的方法。训练时，随机\n选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递\n\n\n\n## 6、早期停止法\n\n我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合\n# 参考\n\n\n\nhttps://www.cnblogs.com/skyfsm/p/8456968.html","slug":"深度学习之过拟合","published":1,"updated":"2019-12-29T16:36:41.571Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tss000xzsv58rjg2wct","content":"<h1 id=\"一、过拟合\"><a href=\"#一、过拟合\" class=\"headerlink\" title=\"一、过拟合\"></a>一、过拟合</h1><p>机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。</p>\n<p>发生过拟合的原因，主要有以下两个：</p>\n<ul>\n<li>模型拥有大量参数、表现力强（模型太复杂）</li>\n<li>训练数据少</li>\n</ul>\n<h1 id=\"二、如何减少过拟合\"><a href=\"#二、如何减少过拟合\" class=\"headerlink\" title=\"二、如何减少过拟合\"></a>二、如何减少过拟合</h1><h2 id=\"1、获取更多数据\"><a href=\"#1、获取更多数据\" class=\"headerlink\" title=\"1、获取更多数据\"></a>1、获取更多数据</h2><p>​    获取更多的数据，从源头上解决问题，比如数据增强</p>\n<h2 id=\"2、正则化\"><a href=\"#2、正则化\" class=\"headerlink\" title=\"2、正则化\"></a>2、正则化</h2><p>​    该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p>\n<p><strong>L1正则化</strong><br>$$<br>L_1(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^n|w_i|<br>$$<br><strong>L2正则化(权值衰减)</strong><br>$$<br>L_2(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^nw_i^2<br>$$</p>\n<ul>\n<li>L1减少的是一个常量，L2减少的是权重的固定比例</li>\n<li>L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。</li>\n<li>实践中L2正则化通常优于L1正则化</li>\n</ul>\n<h2 id=\"3、权重初始化\"><a href=\"#3、权重初始化\" class=\"headerlink\" title=\"3、权重初始化\"></a>3、权重初始化</h2><p>上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习<br>的方法。通过减小权重参数的值来抑制过拟合的发生。 </p>\n<p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 </p>\n<p>为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始<br>值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行<br>相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 </p>\n<p>各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 </p>\n<p>Xavier：如果前一层的节点数为n，则初始值使用标准差为$\\frac{1}{\\sqrt n}$的分布 </p>\n<p>Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\\frac{2}{\\sqrt n}$的高斯分布。当Xavier初始值是$\\frac{1}{\\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 </p>\n<h2 id=\"4、Batch-Normalization\"><a href=\"#4、Batch-Normalization\" class=\"headerlink\" title=\"4、Batch Normalization\"></a>4、Batch Normalization</h2><p>如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 </p>\n<p>为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。</p>\n<ul>\n<li>可以使学习快速进行（可以增大学习率）。</li>\n<li>不那么依赖初始值（对于初始值不用那么神经质）。</li>\n<li>抑制过拟合（降低Dropout等的必要性）。 </li>\n</ul>\n<p><img src=\"02.png\" alt></p>\n<p>Batch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 </p>\n<p>接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用<br>数学式可以如下表示。<br>$$<br>y_i \\leftarrow \\gamma x_i + \\beta<br>$$<br>这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合<br>适的值。 </p>\n<h2 id=\"5、Dropout\"><a href=\"#5、Dropout\" class=\"headerlink\" title=\"5、Dropout\"></a>5、Dropout</h2><p>​    如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。</p>\n<p><img src=\"01.png\" alt></p>\n<p>​    Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机<br>选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递</p>\n<h2 id=\"6、早期停止法\"><a href=\"#6、早期停止法\" class=\"headerlink\" title=\"6、早期停止法\"></a>6、早期停止法</h2><p>我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://www.cnblogs.com/skyfsm/p/8456968.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/skyfsm/p/8456968.html</a></p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、过拟合\"><a href=\"#一、过拟合\" class=\"headerlink\" title=\"一、过拟合\"></a>一、过拟合</h1><p>机器学习的问题中，过拟合是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。</p>\n<p>发生过拟合的原因，主要有以下两个：</p>\n<ul>\n<li>模型拥有大量参数、表现力强（模型太复杂）</li>\n<li>训练数据少</li>\n</ul>\n<h1 id=\"二、如何减少过拟合\"><a href=\"#二、如何减少过拟合\" class=\"headerlink\" title=\"二、如何减少过拟合\"></a>二、如何减少过拟合</h1><h2 id=\"1、获取更多数据\"><a href=\"#1、获取更多数据\" class=\"headerlink\" title=\"1、获取更多数据\"></a>1、获取更多数据</h2><p>​    获取更多的数据，从源头上解决问题，比如数据增强</p>\n<h2 id=\"2、正则化\"><a href=\"#2、正则化\" class=\"headerlink\" title=\"2、正则化\"></a>2、正则化</h2><p>​    该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p>\n<p><strong>L1正则化</strong><br>$$<br>L_1(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^n|w_i|<br>$$<br><strong>L2正则化(权值衰减)</strong><br>$$<br>L_2(\\theta) = loss(\\theta) + \\lambda\\sum_{i=1}^nw_i^2<br>$$</p>\n<ul>\n<li>L1减少的是一个常量，L2减少的是权重的固定比例</li>\n<li>L1使权重稀疏，L2使权重平滑，一句话总结: L1会趋向于产生少量的特征，而其他特征都是0, 而L2则会选择更多的特征，这些特征都会接近于0。</li>\n<li>实践中L2正则化通常优于L1正则化</li>\n</ul>\n<h2 id=\"3、权重初始化\"><a href=\"#3、权重初始化\" class=\"headerlink\" title=\"3、权重初始化\"></a>3、权重初始化</h2><p>上面介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习<br>的方法。通过减小权重参数的值来抑制过拟合的发生。 </p>\n<p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。 </p>\n<p>为什么不能将权重初始值设为0呢？严格地说，为什么不能将权重初始<br>值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行<br>相同的更新。 为了防止“权重均一化”，必须随机生成初始值。 </p>\n<p>各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。 </p>\n<p>Xavier：如果前一层的节点数为n，则初始值使用标准差为$\\frac{1}{\\sqrt n}$的分布 </p>\n<p>Xavier初始值是以激活函数是线性函数为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数使用ReLU时，一般推荐使用ReLU专用的初始值，也称为“He初始值”。当前一层的节点数为n时， He初始值使用标准差为 $\\frac{2}{\\sqrt n}$的高斯分布。当Xavier初始值是$\\frac{1}{\\sqrt n}$ 时，（直观上）可以解释为，因为ReLU的负值区域的值为0，为了使它更有广度，所以需要2倍的系数 </p>\n<h2 id=\"4、Batch-Normalization\"><a href=\"#4、Batch-Normalization\" class=\"headerlink\" title=\"4、Batch Normalization\"></a>4、Batch Normalization</h2><p>如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。 </p>\n<p>为什么Batch Norm这么惹人注目呢？因为Batch Norm有以下优点。</p>\n<ul>\n<li>可以使学习快速进行（可以增大学习率）。</li>\n<li>不那么依赖初始值（对于初始值不用那么神经质）。</li>\n<li>抑制过拟合（降低Dropout等的必要性）。 </li>\n</ul>\n<p><img src=\"02.png\" alt></p>\n<p>Batch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的正规化。 </p>\n<p>接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，用<br>数学式可以如下表示。<br>$$<br>y_i \\leftarrow \\gamma x_i + \\beta<br>$$<br>这里， γ和β是参数。一开始γ = 1， β = 0，然后再通过学习调整到合<br>适的值。 </p>\n<h2 id=\"5、Dropout\"><a href=\"#5、Dropout\" class=\"headerlink\" title=\"5、Dropout\"></a>5、Dropout</h2><p>​    如果网络模型变的很复杂，只使用权值衰减就很难应对了。在这种情下，我们经常使用Dropout方法。</p>\n<p><img src=\"01.png\" alt></p>\n<p>​    Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机<br>选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递</p>\n<h2 id=\"6、早期停止法\"><a href=\"#6、早期停止法\" class=\"headerlink\" title=\"6、早期停止法\"></a>6、早期停止法</h2><p>我们将训练集和测试集相对于每个epoch误差绘制成图表，对于第一epoch，因为模型是完全随机的，所以训练误差和测试误差都很大，随着epoch的增加，训练曲线一直在下降，因为模型越来越好的拟合数据，测试误差先下降后升高，最低点之前模型欠拟合，之后模型过拟合，因为模型开始记住数据。所以我们只要在测试曲线达到最低点时停止训练就可以避免过拟合</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"https://www.cnblogs.com/skyfsm/p/8456968.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/skyfsm/p/8456968.html</a></p>\n"},{"title":"深度学习之超参数","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-12-29T16:47:24.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、什么是超参数\n\n​\t超参数是我们在将学习算法应用于数据集之前需要设置的变量。超参数的一个挑战在于,它不存在适用于所有地方的万能数字,每个任务和数据集的最佳数字各不相同。\n​\t一般来讲, 我们可以将超参数分为两类, 第一类是优化器超参数,它们是与优化和训练过程相关的变量,而非模型本身。这些包括学习率、minibatch大小以及训练迭代或 epoch 次数。 第二类是模型超参数。\n\n# 二、学习率\n\n学习率是最重要的一个超参数, 即使你将他人构建的模型应用于 自己的数据集 你也会发现你可能需要尝试多个不同的学习率值才能使模型正确训练。\n\n如果你归一化模型的输入，一个好的起始点通常是 0.01。这些是学习率的通常假设0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001。\n\n学习率用于控制梯度下降的幅度，根据选取的学习率的不同，梯度下降的误差会呈现出不同的情况:\n\n- 如果我们选择的学习率小于理想的学习率,没关系,我们的模型将继续学习 直到找到权重的最佳值。但是,如果学习率太小,那么我们的训练误差就会降低的非常慢。很明显，在这种情况下我们需要做的是提高学习率 。\n  \n- 另一种情况是，如果我们选择一个大于理想学习率的学习率，更新的值将会越过理想的权重值。下一次更新时它会在反方向越过最佳值，也就是误差来回震荡,  但它会越来越靠近最佳值，可能最终会收敛到一个合理的值。\n- 但是如果我们选择的学习率比理想学习率大很多，比如两倍以上，这就会产生问题。在这种情况下 我们会看到权重采取较大的步长 它不仅越过理想权值 而且实际上离我们每步获得的最佳误差越来越远。所以如果我们的训练误差在增加,不妨试试降低学习率，看看会发生什么 \n\n\n\n我们实际上无法保证误差曲线会是整洁的 U 形。事实上，它们会成为更复杂的形状。而且学习算法可能会错误地将局部最小值当做最佳值进行收敛 \n\n下面我们来看一个在调整学习率时经常会遇到的一个具体情形，假设我们选择了一个合理的学习率，它可以降低误差但只能到某一个点，在那之后就无法下降了，尽管它还没到达底部，它会一直在两个值之间震荡，她们优于刚开始训练时的误差但却不是此模型的最佳值。在这种情况下，让我们的训练算法降低整个训练过程的学习率会比较有用，此技术叫作学习率衰减。这么做的直观方式是线性降低学习率，假设每5个epoch减半，也可以按指数方式降低学习率，例如 每8个epoch对学习率乘以0.1, 除了之间降低学习率外还有一些聪明的学习算法如自适应学习率，不仅在需要时降低学习率，还在学习率太低时升高它。\n\n\n\n# 三、mini-batch\n\n​\t 一直以来人们都在争论哪种做法更好, 一种是在线随机训练, 在数据集中随机选择一条样本,然后仅用这一个样本进行前向传递,计算误差,然后反向传播并设置所有参数的调整数值，然后重复执行这个过程 。另一种是将整个数据集作为输入，使用数据集中所有示例的误差来计算整个数据集的梯度，这叫做批量训练 。\n\n​\t如今普遍使用的抽象是设置一个 mini-batch 大小，那么在线训练的mini-batch 大小就为 1，而批量训练的 mini-batch大小与训练集中的示例数量相同 。我们可以将 mini-batch 大小设置为1到数据集数量之间的任意值，32通常是一个不错的选择 。\n\n![](01.png)\n\n​\t较大的mini-batch可以更好的代表数据集整体的方向，会提高矩阵乘法的计算速度，但这也会占用更多的内存。较小的 mini-batch大小会使误差计算中有更多的噪声,但是此噪声通常有助于防止误差陷入局部最小值。\n\n![](02.png)\n\n从上图中可以看出：\n\n随着mini-batch的增大，训练一个epoch的时间越来越少\n\n随着mini-batch的增大，达到同一准确率所花费的时间越来越多\n\n\n\n一篇名为 “Systematic evaluation of CNN advances on the ImageNet” 的文章显示，在学习率相同的情况下，mini-batch越大，模型的准确度越低。这不仅在于 minibatch 大小的影响，而当我们改变批量大小时还需要改变学习率 。如果我们在增加批量大小的同时调整学习率，可以看到准确度会随批量大小增加而下降，不过只是轻微的下降 。\n所以总结来说， 32 至 256是不错的初始值选择\n\n\n\n# 四、隐藏单元和层的数量\n\n隐藏单元的数据量越多越好，但如果过多往往会导致过拟合。所以如果你的模型无法训练就向它添加更多隐藏层并跟踪验证误差，直到验证误差开始变大。\n\n Andrej Karpathy 告诉我们在实践中三层神经网络的性能往往优于两层网络的性能，但继续增加层却作用不大 。不过，卷积神经网络除外，它们往往是越深性能越好。\n\n\n\n# 五、超参数的验证\n\n为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样话，可能就会得到不能拟合其他数据、泛化能力低的模型。 \n\n因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。 \n\n训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。\n\n\n\n**超参数的最优化**\n\n进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。  \n\n**最优化的步骤**\n\n步骤0, 设定超参数的范围。\n步骤1, 从设定的超参数范围中随机采样。\n步骤2, 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精\n度（但是要将epoch设置得很小）。\n步骤3, 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。 \n\n反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。 \n\n\n\n在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine Learning Algorithms” 等。 ","source":"_posts/深度学习之超参数.md","raw":"---\ntitle: 深度学习之超参数\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-12-30 00:47:24\npassword:\nsummary:\ntags:\n- DL\n- Python\ncategories: 深度学习\n---\n\n\n\n# 一、什么是超参数\n\n​\t超参数是我们在将学习算法应用于数据集之前需要设置的变量。超参数的一个挑战在于,它不存在适用于所有地方的万能数字,每个任务和数据集的最佳数字各不相同。\n​\t一般来讲, 我们可以将超参数分为两类, 第一类是优化器超参数,它们是与优化和训练过程相关的变量,而非模型本身。这些包括学习率、minibatch大小以及训练迭代或 epoch 次数。 第二类是模型超参数。\n\n# 二、学习率\n\n学习率是最重要的一个超参数, 即使你将他人构建的模型应用于 自己的数据集 你也会发现你可能需要尝试多个不同的学习率值才能使模型正确训练。\n\n如果你归一化模型的输入，一个好的起始点通常是 0.01。这些是学习率的通常假设0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001。\n\n学习率用于控制梯度下降的幅度，根据选取的学习率的不同，梯度下降的误差会呈现出不同的情况:\n\n- 如果我们选择的学习率小于理想的学习率,没关系,我们的模型将继续学习 直到找到权重的最佳值。但是,如果学习率太小,那么我们的训练误差就会降低的非常慢。很明显，在这种情况下我们需要做的是提高学习率 。\n  \n- 另一种情况是，如果我们选择一个大于理想学习率的学习率，更新的值将会越过理想的权重值。下一次更新时它会在反方向越过最佳值，也就是误差来回震荡,  但它会越来越靠近最佳值，可能最终会收敛到一个合理的值。\n- 但是如果我们选择的学习率比理想学习率大很多，比如两倍以上，这就会产生问题。在这种情况下 我们会看到权重采取较大的步长 它不仅越过理想权值 而且实际上离我们每步获得的最佳误差越来越远。所以如果我们的训练误差在增加,不妨试试降低学习率，看看会发生什么 \n\n\n\n我们实际上无法保证误差曲线会是整洁的 U 形。事实上，它们会成为更复杂的形状。而且学习算法可能会错误地将局部最小值当做最佳值进行收敛 \n\n下面我们来看一个在调整学习率时经常会遇到的一个具体情形，假设我们选择了一个合理的学习率，它可以降低误差但只能到某一个点，在那之后就无法下降了，尽管它还没到达底部，它会一直在两个值之间震荡，她们优于刚开始训练时的误差但却不是此模型的最佳值。在这种情况下，让我们的训练算法降低整个训练过程的学习率会比较有用，此技术叫作学习率衰减。这么做的直观方式是线性降低学习率，假设每5个epoch减半，也可以按指数方式降低学习率，例如 每8个epoch对学习率乘以0.1, 除了之间降低学习率外还有一些聪明的学习算法如自适应学习率，不仅在需要时降低学习率，还在学习率太低时升高它。\n\n\n\n# 三、mini-batch\n\n​\t 一直以来人们都在争论哪种做法更好, 一种是在线随机训练, 在数据集中随机选择一条样本,然后仅用这一个样本进行前向传递,计算误差,然后反向传播并设置所有参数的调整数值，然后重复执行这个过程 。另一种是将整个数据集作为输入，使用数据集中所有示例的误差来计算整个数据集的梯度，这叫做批量训练 。\n\n​\t如今普遍使用的抽象是设置一个 mini-batch 大小，那么在线训练的mini-batch 大小就为 1，而批量训练的 mini-batch大小与训练集中的示例数量相同 。我们可以将 mini-batch 大小设置为1到数据集数量之间的任意值，32通常是一个不错的选择 。\n\n![](01.png)\n\n​\t较大的mini-batch可以更好的代表数据集整体的方向，会提高矩阵乘法的计算速度，但这也会占用更多的内存。较小的 mini-batch大小会使误差计算中有更多的噪声,但是此噪声通常有助于防止误差陷入局部最小值。\n\n![](02.png)\n\n从上图中可以看出：\n\n随着mini-batch的增大，训练一个epoch的时间越来越少\n\n随着mini-batch的增大，达到同一准确率所花费的时间越来越多\n\n\n\n一篇名为 “Systematic evaluation of CNN advances on the ImageNet” 的文章显示，在学习率相同的情况下，mini-batch越大，模型的准确度越低。这不仅在于 minibatch 大小的影响，而当我们改变批量大小时还需要改变学习率 。如果我们在增加批量大小的同时调整学习率，可以看到准确度会随批量大小增加而下降，不过只是轻微的下降 。\n所以总结来说， 32 至 256是不错的初始值选择\n\n\n\n# 四、隐藏单元和层的数量\n\n隐藏单元的数据量越多越好，但如果过多往往会导致过拟合。所以如果你的模型无法训练就向它添加更多隐藏层并跟踪验证误差，直到验证误差开始变大。\n\n Andrej Karpathy 告诉我们在实践中三层神经网络的性能往往优于两层网络的性能，但继续增加层却作用不大 。不过，卷积神经网络除外，它们往往是越深性能越好。\n\n\n\n# 五、超参数的验证\n\n为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样话，可能就会得到不能拟合其他数据、泛化能力低的模型。 \n\n因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。 \n\n训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。\n\n\n\n**超参数的最优化**\n\n进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。  \n\n**最优化的步骤**\n\n步骤0, 设定超参数的范围。\n步骤1, 从设定的超参数范围中随机采样。\n步骤2, 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精\n度（但是要将epoch设置得很小）。\n步骤3, 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。 \n\n反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。 \n\n\n\n在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine Learning Algorithms” 等。 ","slug":"深度学习之超参数","published":1,"updated":"2019-12-30T15:48:40.922Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tsv000zzsv574n6a04y","content":"<h1 id=\"一、什么是超参数\"><a href=\"#一、什么是超参数\" class=\"headerlink\" title=\"一、什么是超参数\"></a>一、什么是超参数</h1><p>​    超参数是我们在将学习算法应用于数据集之前需要设置的变量。超参数的一个挑战在于,它不存在适用于所有地方的万能数字,每个任务和数据集的最佳数字各不相同。<br>​    一般来讲, 我们可以将超参数分为两类, 第一类是优化器超参数,它们是与优化和训练过程相关的变量,而非模型本身。这些包括学习率、minibatch大小以及训练迭代或 epoch 次数。 第二类是模型超参数。</p>\n<h1 id=\"二、学习率\"><a href=\"#二、学习率\" class=\"headerlink\" title=\"二、学习率\"></a>二、学习率</h1><p>学习率是最重要的一个超参数, 即使你将他人构建的模型应用于 自己的数据集 你也会发现你可能需要尝试多个不同的学习率值才能使模型正确训练。</p>\n<p>如果你归一化模型的输入，一个好的起始点通常是 0.01。这些是学习率的通常假设0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001。</p>\n<p>学习率用于控制梯度下降的幅度，根据选取的学习率的不同，梯度下降的误差会呈现出不同的情况:</p>\n<ul>\n<li><p>如果我们选择的学习率小于理想的学习率,没关系,我们的模型将继续学习 直到找到权重的最佳值。但是,如果学习率太小,那么我们的训练误差就会降低的非常慢。很明显，在这种情况下我们需要做的是提高学习率 。</p>\n</li>\n<li><p>另一种情况是，如果我们选择一个大于理想学习率的学习率，更新的值将会越过理想的权重值。下一次更新时它会在反方向越过最佳值，也就是误差来回震荡,  但它会越来越靠近最佳值，可能最终会收敛到一个合理的值。</p>\n</li>\n<li><p>但是如果我们选择的学习率比理想学习率大很多，比如两倍以上，这就会产生问题。在这种情况下 我们会看到权重采取较大的步长 它不仅越过理想权值 而且实际上离我们每步获得的最佳误差越来越远。所以如果我们的训练误差在增加,不妨试试降低学习率，看看会发生什么 </p>\n</li>\n</ul>\n<p>我们实际上无法保证误差曲线会是整洁的 U 形。事实上，它们会成为更复杂的形状。而且学习算法可能会错误地将局部最小值当做最佳值进行收敛 </p>\n<p>下面我们来看一个在调整学习率时经常会遇到的一个具体情形，假设我们选择了一个合理的学习率，它可以降低误差但只能到某一个点，在那之后就无法下降了，尽管它还没到达底部，它会一直在两个值之间震荡，她们优于刚开始训练时的误差但却不是此模型的最佳值。在这种情况下，让我们的训练算法降低整个训练过程的学习率会比较有用，此技术叫作学习率衰减。这么做的直观方式是线性降低学习率，假设每5个epoch减半，也可以按指数方式降低学习率，例如 每8个epoch对学习率乘以0.1, 除了之间降低学习率外还有一些聪明的学习算法如自适应学习率，不仅在需要时降低学习率，还在学习率太低时升高它。</p>\n<h1 id=\"三、mini-batch\"><a href=\"#三、mini-batch\" class=\"headerlink\" title=\"三、mini-batch\"></a>三、mini-batch</h1><p>​     一直以来人们都在争论哪种做法更好, 一种是在线随机训练, 在数据集中随机选择一条样本,然后仅用这一个样本进行前向传递,计算误差,然后反向传播并设置所有参数的调整数值，然后重复执行这个过程 。另一种是将整个数据集作为输入，使用数据集中所有示例的误差来计算整个数据集的梯度，这叫做批量训练 。</p>\n<p>​    如今普遍使用的抽象是设置一个 mini-batch 大小，那么在线训练的mini-batch 大小就为 1，而批量训练的 mini-batch大小与训练集中的示例数量相同 。我们可以将 mini-batch 大小设置为1到数据集数量之间的任意值，32通常是一个不错的选择 。</p>\n<p><img src=\"01.png\" alt></p>\n<p>​    较大的mini-batch可以更好的代表数据集整体的方向，会提高矩阵乘法的计算速度，但这也会占用更多的内存。较小的 mini-batch大小会使误差计算中有更多的噪声,但是此噪声通常有助于防止误差陷入局部最小值。</p>\n<p><img src=\"02.png\" alt></p>\n<p>从上图中可以看出：</p>\n<p>随着mini-batch的增大，训练一个epoch的时间越来越少</p>\n<p>随着mini-batch的增大，达到同一准确率所花费的时间越来越多</p>\n<p>一篇名为 “Systematic evaluation of CNN advances on the ImageNet” 的文章显示，在学习率相同的情况下，mini-batch越大，模型的准确度越低。这不仅在于 minibatch 大小的影响，而当我们改变批量大小时还需要改变学习率 。如果我们在增加批量大小的同时调整学习率，可以看到准确度会随批量大小增加而下降，不过只是轻微的下降 。<br>所以总结来说， 32 至 256是不错的初始值选择</p>\n<h1 id=\"四、隐藏单元和层的数量\"><a href=\"#四、隐藏单元和层的数量\" class=\"headerlink\" title=\"四、隐藏单元和层的数量\"></a>四、隐藏单元和层的数量</h1><p>隐藏单元的数据量越多越好，但如果过多往往会导致过拟合。所以如果你的模型无法训练就向它添加更多隐藏层并跟踪验证误差，直到验证误差开始变大。</p>\n<p> Andrej Karpathy 告诉我们在实践中三层神经网络的性能往往优于两层网络的性能，但继续增加层却作用不大 。不过，卷积神经网络除外，它们往往是越深性能越好。</p>\n<h1 id=\"五、超参数的验证\"><a href=\"#五、超参数的验证\" class=\"headerlink\" title=\"五、超参数的验证\"></a>五、超参数的验证</h1><p>为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样话，可能就会得到不能拟合其他数据、泛化能力低的模型。 </p>\n<p>因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。 </p>\n<p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。</p>\n<p><strong>超参数的最优化</strong></p>\n<p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。  </p>\n<p><strong>最优化的步骤</strong></p>\n<p>步骤0, 设定超参数的范围。<br>步骤1, 从设定的超参数范围中随机采样。<br>步骤2, 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精<br>度（但是要将epoch设置得很小）。<br>步骤3, 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。 </p>\n<p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。 </p>\n<p>在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine Learning Algorithms” 等。 </p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、什么是超参数\"><a href=\"#一、什么是超参数\" class=\"headerlink\" title=\"一、什么是超参数\"></a>一、什么是超参数</h1><p>​    超参数是我们在将学习算法应用于数据集之前需要设置的变量。超参数的一个挑战在于,它不存在适用于所有地方的万能数字,每个任务和数据集的最佳数字各不相同。<br>​    一般来讲, 我们可以将超参数分为两类, 第一类是优化器超参数,它们是与优化和训练过程相关的变量,而非模型本身。这些包括学习率、minibatch大小以及训练迭代或 epoch 次数。 第二类是模型超参数。</p>\n<h1 id=\"二、学习率\"><a href=\"#二、学习率\" class=\"headerlink\" title=\"二、学习率\"></a>二、学习率</h1><p>学习率是最重要的一个超参数, 即使你将他人构建的模型应用于 自己的数据集 你也会发现你可能需要尝试多个不同的学习率值才能使模型正确训练。</p>\n<p>如果你归一化模型的输入，一个好的起始点通常是 0.01。这些是学习率的通常假设0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001。</p>\n<p>学习率用于控制梯度下降的幅度，根据选取的学习率的不同，梯度下降的误差会呈现出不同的情况:</p>\n<ul>\n<li><p>如果我们选择的学习率小于理想的学习率,没关系,我们的模型将继续学习 直到找到权重的最佳值。但是,如果学习率太小,那么我们的训练误差就会降低的非常慢。很明显，在这种情况下我们需要做的是提高学习率 。</p>\n</li>\n<li><p>另一种情况是，如果我们选择一个大于理想学习率的学习率，更新的值将会越过理想的权重值。下一次更新时它会在反方向越过最佳值，也就是误差来回震荡,  但它会越来越靠近最佳值，可能最终会收敛到一个合理的值。</p>\n</li>\n<li><p>但是如果我们选择的学习率比理想学习率大很多，比如两倍以上，这就会产生问题。在这种情况下 我们会看到权重采取较大的步长 它不仅越过理想权值 而且实际上离我们每步获得的最佳误差越来越远。所以如果我们的训练误差在增加,不妨试试降低学习率，看看会发生什么 </p>\n</li>\n</ul>\n<p>我们实际上无法保证误差曲线会是整洁的 U 形。事实上，它们会成为更复杂的形状。而且学习算法可能会错误地将局部最小值当做最佳值进行收敛 </p>\n<p>下面我们来看一个在调整学习率时经常会遇到的一个具体情形，假设我们选择了一个合理的学习率，它可以降低误差但只能到某一个点，在那之后就无法下降了，尽管它还没到达底部，它会一直在两个值之间震荡，她们优于刚开始训练时的误差但却不是此模型的最佳值。在这种情况下，让我们的训练算法降低整个训练过程的学习率会比较有用，此技术叫作学习率衰减。这么做的直观方式是线性降低学习率，假设每5个epoch减半，也可以按指数方式降低学习率，例如 每8个epoch对学习率乘以0.1, 除了之间降低学习率外还有一些聪明的学习算法如自适应学习率，不仅在需要时降低学习率，还在学习率太低时升高它。</p>\n<h1 id=\"三、mini-batch\"><a href=\"#三、mini-batch\" class=\"headerlink\" title=\"三、mini-batch\"></a>三、mini-batch</h1><p>​     一直以来人们都在争论哪种做法更好, 一种是在线随机训练, 在数据集中随机选择一条样本,然后仅用这一个样本进行前向传递,计算误差,然后反向传播并设置所有参数的调整数值，然后重复执行这个过程 。另一种是将整个数据集作为输入，使用数据集中所有示例的误差来计算整个数据集的梯度，这叫做批量训练 。</p>\n<p>​    如今普遍使用的抽象是设置一个 mini-batch 大小，那么在线训练的mini-batch 大小就为 1，而批量训练的 mini-batch大小与训练集中的示例数量相同 。我们可以将 mini-batch 大小设置为1到数据集数量之间的任意值，32通常是一个不错的选择 。</p>\n<p><img src=\"01.png\" alt></p>\n<p>​    较大的mini-batch可以更好的代表数据集整体的方向，会提高矩阵乘法的计算速度，但这也会占用更多的内存。较小的 mini-batch大小会使误差计算中有更多的噪声,但是此噪声通常有助于防止误差陷入局部最小值。</p>\n<p><img src=\"02.png\" alt></p>\n<p>从上图中可以看出：</p>\n<p>随着mini-batch的增大，训练一个epoch的时间越来越少</p>\n<p>随着mini-batch的增大，达到同一准确率所花费的时间越来越多</p>\n<p>一篇名为 “Systematic evaluation of CNN advances on the ImageNet” 的文章显示，在学习率相同的情况下，mini-batch越大，模型的准确度越低。这不仅在于 minibatch 大小的影响，而当我们改变批量大小时还需要改变学习率 。如果我们在增加批量大小的同时调整学习率，可以看到准确度会随批量大小增加而下降，不过只是轻微的下降 。<br>所以总结来说， 32 至 256是不错的初始值选择</p>\n<h1 id=\"四、隐藏单元和层的数量\"><a href=\"#四、隐藏单元和层的数量\" class=\"headerlink\" title=\"四、隐藏单元和层的数量\"></a>四、隐藏单元和层的数量</h1><p>隐藏单元的数据量越多越好，但如果过多往往会导致过拟合。所以如果你的模型无法训练就向它添加更多隐藏层并跟踪验证误差，直到验证误差开始变大。</p>\n<p> Andrej Karpathy 告诉我们在实践中三层神经网络的性能往往优于两层网络的性能，但继续增加层却作用不大 。不过，卷积神经网络除外，它们往往是越深性能越好。</p>\n<h1 id=\"五、超参数的验证\"><a href=\"#五、超参数的验证\" class=\"headerlink\" title=\"五、超参数的验证\"></a>五、超参数的验证</h1><p>为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样话，可能就会得到不能拟合其他数据、泛化能力低的模型。 </p>\n<p>因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。 </p>\n<p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。</p>\n<p><strong>超参数的最优化</strong></p>\n<p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。  </p>\n<p><strong>最优化的步骤</strong></p>\n<p>步骤0, 设定超参数的范围。<br>步骤1, 从设定的超参数范围中随机采样。<br>步骤2, 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精<br>度（但是要将epoch设置得很小）。<br>步骤3, 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。 </p>\n<p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。 </p>\n<p>在超参数的最优化中，如果需要更精炼的方法，可以使用贝叶斯最优化（Bayesian optimization）。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。详细内容请参 考 论 文“Practical Bayesian Optimization of Machine Learning Algorithms” 等。 </p>\n"},{"title":"神经网络简介","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2019-11-25T01:08:19.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、感知器\n\n​\t感知器(perceptron)是由美国学者FrankRoseblatt在1957年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。\n\n## 1.什么是感知器\n\n​\t感知机接收多个输入，生成一个输出，输出只有两种1和0。\n\n​\t![](p1.png)\n\n​\t\t\t\t\t\t\t\t\t**图1.1 有两个输入的感知机**\n\n图1-1是一个接收两个输入的感知机. $x_1$、$x_2$是输入，$y$是输出，$w_1$、$w_2$是权重。图中的○称为神经元或者节点。输入被送往神经元时，会被分别乘以固定的权重$(w_1x_1,  w_2x_2)$。神经元会计算传送过来的输入的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。\n$$\ny =\\begin{cases}1, & (w_1x_1 + w_2x_2) > \\theta \\\\0, & (w_1x_1 + w_2x_2) \\leq \\theta\\end{cases}\n$$\n​\t\t**权重越大，对应该权重的信号的重要性就越高。**\n\n## 2.逻辑运算\n\n使用感知器可以解决简单的逻辑运算，与门（AND）, 与非门（NOT AND）, 或门（OR）.\n\n​\t![](p2.png)\n\n​\t\t\t\t\t\t\t\t\t\t**1-2 与门真值表**\n\n满足图2-2的条件的参数的选择方法有无数多个。比如，当$(w_1, w_2, θ)$ = $(0.5, 0.5, 0.7)$ ​时，可以满足图 2-1的条件。\n\n我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。\n\n## 3.偏置和权重\n\n$$\ny =\\begin{cases}1, & (b + w_1x_1 + w_2x_2) > 0 \\\\0, & (b + w_1x_1 + w_2x_2) \\leq 0\\end{cases}\n$$\n\n令$b = -\\theta$， $b$称为偏置，$w_1$和$w_2$称为权重, 但是请注意，偏置和权重$w_1$、$w_2$的作用是不一样的。具体地说， $w_1$和$w_2$是控制输入的重要性的参数，而偏置是调整神经元被激活的容易程度（输出为1的程度）的参数。有时也会将$b$、$w_1$、$w_2$这些参数统称为权重。\n\n## 4.单层感知机的局限性\n\n单层感知机的局限性就在于它只能表示由一条直线分割的空间,无法表示用曲线分割的空间。弯曲的曲线无法用感知机表示\n\n​\t![](p3.png)\n\n​\t\t**图1-3　○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？**\n\n\n\n​\t![](p4.png)\n\n​\t\t\t\t\t**图1-4　使用曲线可以分开○和△**\n\n## 5.多层感知机\n\n​\t单层感知机虽然不能表示异或，但多层感知机的叠加却可以。\n\n​\t![](p5.png)\n\n​\t\t\t\t\t\t**图1-5　通过组合与门、与非门、或门实现异或门**\n\n\n\n​\t![](p6.png)\n\n​\t\t\t\t\t**图2-6　用感知机表示异或门**\t\t\t\n\n叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。异或可以通过多层感知机实现。\n\n\n\n图2-6中的感知机总共由3层构成，但是因为拥有权重的层实质上只有2层（第0层和第1层之间，第1层和第2层之间），所以称为“2层感知机”。不过，有的文献认为图2-6的感知机是由3层构成的，因而将其称为“3层感知机”。\n\n\n\n## 6.感知机与计算机\n\n​\t多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？\n\n​\t**理论上**可以说2层感知机就能构建计算机。这是因为，已有研究证明，2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数。但是，使用2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。\n\n​\t实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU）, 然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。\n\n## 7.总结\n\n​\t感知机从算法的角度来说就是单位阶跃函数+线性回归算法。\n\n# 二、神经网路\n\n## 1.1 从感知器到神经网络\n\n​\t一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用`sigmoid` 函数等平滑的激活函数的多层网络。\n\n​\t![](p7.png)\n\n​\t如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层.\n\n​\t图中的网络一共由 3 层神经元构成, 但实质上只有 2层神经元有权重，因此将其称为“2层网络”。\n\n\n\n## 1.2 激活函数\n\n​\t神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比\"阶跃函数\"更佳平滑.\n\n​\t阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。\n\n​\t激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。\n\n​\t输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。\n\n​\t常见的激活函数:\n\n- 阶跃函数:  \n\n$$\nh(x) =\\begin{cases}1, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n$$\n\n​\t![](n2.png)\n\n\n\n-  sigmoid函数(S函数)\n  $$\n  h(x) = \\dfrac {1}{1+e^{-x}}\n  $$\n  \n\n  ![](n3.png)\n\n- Relu函数\n  $$\n  h(x) =\\begin{cases}x, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n  $$\n  \n\n  ![](n4.png)\n\n- softmax函数\n\n$$\n\\sigma(x) =  \\dfrac {e^{a_k}}{ \\sum_{i=1}^n e^{a^i}   }\n$$\n\n**注意**: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。\n\n**改进**: 先进行归一化，再求值\n\n```python\na = np.array([1010, 1000, 990])\nnp.exp(a) / np.sum(np.exp(a))\n#  array([nan, nan, nan])  没有计算正确的值\n\nmi = np.min(a)     # 990                                 \nma = np.max(a)                                 \nnor = (a-mi)/(ma-mi)    # 归一化 array([1. , 0.5, 0. ])   \nnp.exp(nor)/np.sum(np.exp(nor))                 \n# array([0.50648039, 0.30719589, 0.18632372])\n\n```\n\n​\t一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此**输出层的softmax函数一般会被省略**\n\n​\t求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系\n\n\n\n## 1.3 3层神经网络\n\n​\t![](n7.png)\n\n3层神经网络：输入层（第0层）有2个神经元，第1个隐藏层（第1层）有3个神经元，第2个隐藏层（第2层）有2个神经元，输出层（第3层）有2个神经元\n\n\n\n![](n6.png)\n\n​\t\t\t\t\t\t\t\t图２－６权重的符号\n\n请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个，索引表示的是后一层神经元的索引。\n\n数学公式表示$a_1^{(1)}$\n$$\na_1^{(1)} = a_{11}^{(1)}x1 + w_{12}^{(1)}x2 + b_1^{(1)}\n$$\n矩阵$W^{(1)}$表示第１层的权重：\n$$\nW^{(1)} = \\begin{pmatrix}w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)}\\\\\\\\w_{12}^{(1)} &w_{22}^{(1)} &w_{32}^{(1)}\\end{pmatrix}\n$$\n向量$B^{(1)}$表示第一层的偏置:\n$$\n\\begin{pmatrix} b_1^{(1)} & b_2^{(1)} & b_3^{(1)}\\\\ \\end{pmatrix} \\quad\n$$\n\n$$\nA^{(1)} = \\begin{pmatrix} a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\\\\ \\end{pmatrix} \\quad\n$$\n\n$$\nX^{(1)} = \\begin{pmatrix} x_1 & x_2\\\\ \\end{pmatrix} \\quad\n$$\n\n第１层的加权和表示:\n$$\nA^{(1)} = XW^{(1)} + B^{(1)}\n$$\n\n\n\n# 三、神经网络的学习\n\n​\t神经网络的学习就是从训练数据学习权重参数，然后使用刚才学习到的参数对输入数据进行预测。\n\n​\t神经网络学习的策略是首先对输入数据进行前向传播（forward propagation）过程得到输出，然后计算输出与真实值之间的差别，最后通过反向传播跟新权重参数，重复这一过程直到权重参数没有更新，此时损失函数达到最小。\n\n​\t计算输出与真实值之间的差别通过损失函数计算。反向传播需要用到梯度下降算法实现。\n\n\n\n## 1.损失函数\n\n​\t损失函数是表示神经网络性能的指标。神经网络通过减小损失函数，寻找最优权重参数。常用的误差函数有均方误差和交叉熵误差等。\n\n- 均方误差(mean squared error MSE)\n\n$$\nE = \\frac {1}{2} \\sum_{k}({y_k - t_k})^2\n$$\n\n​\t$y_k$表示神经网络的输出，$t_k$表示目标数据，k表示输出数据的维度，比如手写数字识别的输出数据的维度是10 (0-9)。\n\n- 交叉熵误差(cross entropy error)\n  $$\n  E = - \\sum_{k}t_k\\log y_k\n  $$\n  上式是一条数据的误差，如果批量计算多条数据则函数为:\n  $$\n  E = - \\frac{1}{N}\\sum_{N}\\sum_{k}t_{nk}\\log y_{nk}\n  $$\n  假设数据有N个， $t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神\n  经网络的输出， $t_{nk}$是目标数据）, 不过最后还要除以N, 通过这样的\n  平均化，可以获得和训练数据的数量无关的统一指标。\n\n## 2.梯度下降\n\n​\t梯度是导数对多元函数的推广，它是多元函数对各个变量偏导形成的向量。多元函数的梯度定义为:\n$$\n\\triangledown(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}, & ..., & \\frac{\\partial f}{\\partial x_n}\\\\  \\end{pmatrix}^{T} \\quad\n$$\n　其中$\\triangledown$称为梯度算子，它作用与一个多元函数，得到一个向量。梯度是一个向量，即有大小又有方向，大小为该点的变化率，方向是该点增加最快的方向，-$\\triangledown$(x)就为减小最快的方向。所以只要沿着梯度的反方向就可能达到函数的驻点，可能是局部极小值，全局极小值或鞍点。\n\n​\t在梯度下降法中，函数的取值从当前位置沿着梯度反方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度反方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度反方向前进，逐渐减小函数值的过程就是梯度下降法（gradient descent method）。\n\n​\t梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题。\n\n数学公式来表示梯度下降方法:\n$$\nw_0 = w_0 - \\eta \\frac {\\partial f}{\\partial w_0}\n$$\n\n$$\nw_1 = w_1 - \\eta \\frac {\\partial f}{\\partial w_1}\n$$\n\n$\\eta$是学习率，表示根据梯度值变化的幅度，w在神经网络中代表权重.\n\n​\t控制梯度下降的幅度\n\n​\t0.1\t0.01\t0.001\t....\n\n​\t如果学习率太大，误差会逐步增大\n\n​\t如果学习率较大，误差会来回震荡\n\n​\t如果学习率太小， 误差下降缓慢\n\n​\t![](n8.png)\n\n​\t\t\t\t\t图　$f(x_0, x_1) = x_0^2 + x_1^2$的梯度下降更新过程:虚线是函数的等高线\n\n神经网络的学习分成下面4个步骤:\n**步骤1（ mini-batch）**\n\t从训练数据中选出一部分数据，这部分数据称为mini-batch。我们的目标是减小损失函数的值。\n**步骤2（计算梯度）**\n\t为了减小损失函数的值，需要求出各个权重参数的梯度。\n**步骤3（更新参数）**\n\t将权重参数沿梯度方向进行微小更新。\t\t\n\n**步骤4（重复）**\n\t重复步骤1、步骤2、步骤3, 直到梯度为０或接近０．\n\n​\t\t\n\n## 3.误差反向传播\n\n1. **计算图**\n\n   小明在超市买了2个苹果、 3个橘子。其中，苹果每个100日元，\n   橘子每个150日元。消费税是10%，请计算支付金额。\n\n   ![](b1.png)\n\n   ​\t\t\t\t\t\t**计算图的求解过程**\n\n   ​\t在计算图上，从左向右进行计算是一种正方向的传播, 简称为**正向传播**（forward propagation）。同理，反方向的计算，就是**反向传播**（backward propagation）。\n\n   ​\t计算图的特征是可以通过传递“局部计算”获得最终结果。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。\n\n   \n\n   ![](b2.png)\n\n   ​\t计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值. 从上图可以看到，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生200倍大小的影响，苹果的价格将产生2.2倍大小的影响。不过，因为这个例子中消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的1是100%，苹果的价格的1是1元）。\n\n   \n\n   反向传播的计算顺序是，将信号E乘以节点的局部导数，然后将结果传递给下一个节点 \n\n2. **链式法则**\n\n   反向传播将局部导数向正方向的反方向（从右到左）传递，传递这个局部导数的原理，是基于**链式法则**的。\n\n   ![](b3.png)\n\n   **图b3 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数**\n\n   如图所示，反向传播的计算顺序是，将信号$E$乘以节点的局部导数$(\\frac {\\partial y}{\\partial x})$，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中y = f(x)的导数，也就是y关于x的导数$(\\frac {\\partial y}{\\partial x})$。比如，假设$y = f(x) = x^2$，则局部导数为 $(\\frac {\\partial y}{\\partial x})$= $2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。\n\n   \n\n   ![](b4.png)\n\n   根据链式法则，$\\frac {\\partial z}{\\partial z} \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial x}$成立，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的\n\n4. **激活函数的反向传播**\n\n   - Relu激活函数\n     $$\n     h^{'}(x) =\\begin{cases}1, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n     $$\n     如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。\n\n     Relu 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。\n\n   - Sigmoid激活函数\n     $$\n     h^{'}(y) = y(1-y)\n     $$\n     \n\n```python\nclass Sigmoid:\n    def __init__(self):\n        self.out = None     # 保存当前层的输出\n\n    def forward(self, x):\n        \"\"\"\n        前向传播\n        :param x: \n        :return: \n        \"\"\"\n        out = sigmoid(x)\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        \"\"\"\n        反向传播\n        :param dout: \n        :return: \n        \"\"\"\n        dx = dout * (1.0 - self.out) * self.out\n\n        return dx\n```\n\n\n\n4. 矩阵运算\n   $$\n   X \\cdot W = Y\n   $$\n\n   $$\n   \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T\n   $$\n\n   $$\n   \\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}\n   $$\n\n   \n\n   ![](b6.png)\n\n   需要注意各个变量的形状\n\n5. Softmax-with-Loss层\n\n   ![](b7.png)\n\n   Softmax层将输入（a1, a2, a3）正规化，输出（y1,y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和目标值（t1,t2, t3），从这些数据中输出损失L。\n\n   Softmax反向传播的梯度为（y1 - t1,  y2 - t2, y3 - t3）.\n\n## 4.总结\n\n通过使用计算图，可以直观地把握计算过程。\n• 计算图的节点是由局部计算构成的。局部计算构成全局计算。\n• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以\n计算各个节点的导数。\n• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传\n播法）。\n• 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传\n播法的实现是否正确（梯度确认）\n\n\n\n# 四、参考\n\n《深度学习入门: 基于Python的理论与实现》","source":"_posts/神经网络简介.md","raw":"---\ntitle: 神经网络简介\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2019-11-25 09:08:19\npassword:\nsummary:\ntags: \n- DL\n- ML\n- ANN\ncategories: 深度学习\n---\n\n\n\n# 一、感知器\n\n​\t感知器(perceptron)是由美国学者FrankRoseblatt在1957年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。\n\n## 1.什么是感知器\n\n​\t感知机接收多个输入，生成一个输出，输出只有两种1和0。\n\n​\t![](p1.png)\n\n​\t\t\t\t\t\t\t\t\t**图1.1 有两个输入的感知机**\n\n图1-1是一个接收两个输入的感知机. $x_1$、$x_2$是输入，$y$是输出，$w_1$、$w_2$是权重。图中的○称为神经元或者节点。输入被送往神经元时，会被分别乘以固定的权重$(w_1x_1,  w_2x_2)$。神经元会计算传送过来的输入的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。\n$$\ny =\\begin{cases}1, & (w_1x_1 + w_2x_2) > \\theta \\\\0, & (w_1x_1 + w_2x_2) \\leq \\theta\\end{cases}\n$$\n​\t\t**权重越大，对应该权重的信号的重要性就越高。**\n\n## 2.逻辑运算\n\n使用感知器可以解决简单的逻辑运算，与门（AND）, 与非门（NOT AND）, 或门（OR）.\n\n​\t![](p2.png)\n\n​\t\t\t\t\t\t\t\t\t\t**1-2 与门真值表**\n\n满足图2-2的条件的参数的选择方法有无数多个。比如，当$(w_1, w_2, θ)$ = $(0.5, 0.5, 0.7)$ ​时，可以满足图 2-1的条件。\n\n我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。\n\n## 3.偏置和权重\n\n$$\ny =\\begin{cases}1, & (b + w_1x_1 + w_2x_2) > 0 \\\\0, & (b + w_1x_1 + w_2x_2) \\leq 0\\end{cases}\n$$\n\n令$b = -\\theta$， $b$称为偏置，$w_1$和$w_2$称为权重, 但是请注意，偏置和权重$w_1$、$w_2$的作用是不一样的。具体地说， $w_1$和$w_2$是控制输入的重要性的参数，而偏置是调整神经元被激活的容易程度（输出为1的程度）的参数。有时也会将$b$、$w_1$、$w_2$这些参数统称为权重。\n\n## 4.单层感知机的局限性\n\n单层感知机的局限性就在于它只能表示由一条直线分割的空间,无法表示用曲线分割的空间。弯曲的曲线无法用感知机表示\n\n​\t![](p3.png)\n\n​\t\t**图1-3　○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？**\n\n\n\n​\t![](p4.png)\n\n​\t\t\t\t\t**图1-4　使用曲线可以分开○和△**\n\n## 5.多层感知机\n\n​\t单层感知机虽然不能表示异或，但多层感知机的叠加却可以。\n\n​\t![](p5.png)\n\n​\t\t\t\t\t\t**图1-5　通过组合与门、与非门、或门实现异或门**\n\n\n\n​\t![](p6.png)\n\n​\t\t\t\t\t**图2-6　用感知机表示异或门**\t\t\t\n\n叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。异或可以通过多层感知机实现。\n\n\n\n图2-6中的感知机总共由3层构成，但是因为拥有权重的层实质上只有2层（第0层和第1层之间，第1层和第2层之间），所以称为“2层感知机”。不过，有的文献认为图2-6的感知机是由3层构成的，因而将其称为“3层感知机”。\n\n\n\n## 6.感知机与计算机\n\n​\t多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？\n\n​\t**理论上**可以说2层感知机就能构建计算机。这是因为，已有研究证明，2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数。但是，使用2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。\n\n​\t实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU）, 然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。\n\n## 7.总结\n\n​\t感知机从算法的角度来说就是单位阶跃函数+线性回归算法。\n\n# 二、神经网路\n\n## 1.1 从感知器到神经网络\n\n​\t一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用`sigmoid` 函数等平滑的激活函数的多层网络。\n\n​\t![](p7.png)\n\n​\t如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层.\n\n​\t图中的网络一共由 3 层神经元构成, 但实质上只有 2层神经元有权重，因此将其称为“2层网络”。\n\n\n\n## 1.2 激活函数\n\n​\t神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比\"阶跃函数\"更佳平滑.\n\n​\t阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。\n\n​\t激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。\n\n​\t输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。\n\n​\t常见的激活函数:\n\n- 阶跃函数:  \n\n$$\nh(x) =\\begin{cases}1, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n$$\n\n​\t![](n2.png)\n\n\n\n-  sigmoid函数(S函数)\n  $$\n  h(x) = \\dfrac {1}{1+e^{-x}}\n  $$\n  \n\n  ![](n3.png)\n\n- Relu函数\n  $$\n  h(x) =\\begin{cases}x, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n  $$\n  \n\n  ![](n4.png)\n\n- softmax函数\n\n$$\n\\sigma(x) =  \\dfrac {e^{a_k}}{ \\sum_{i=1}^n e^{a^i}   }\n$$\n\n**注意**: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。\n\n**改进**: 先进行归一化，再求值\n\n```python\na = np.array([1010, 1000, 990])\nnp.exp(a) / np.sum(np.exp(a))\n#  array([nan, nan, nan])  没有计算正确的值\n\nmi = np.min(a)     # 990                                 \nma = np.max(a)                                 \nnor = (a-mi)/(ma-mi)    # 归一化 array([1. , 0.5, 0. ])   \nnp.exp(nor)/np.sum(np.exp(nor))                 \n# array([0.50648039, 0.30719589, 0.18632372])\n\n```\n\n​\t一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此**输出层的softmax函数一般会被省略**\n\n​\t求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系\n\n\n\n## 1.3 3层神经网络\n\n​\t![](n7.png)\n\n3层神经网络：输入层（第0层）有2个神经元，第1个隐藏层（第1层）有3个神经元，第2个隐藏层（第2层）有2个神经元，输出层（第3层）有2个神经元\n\n\n\n![](n6.png)\n\n​\t\t\t\t\t\t\t\t图２－６权重的符号\n\n请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个，索引表示的是后一层神经元的索引。\n\n数学公式表示$a_1^{(1)}$\n$$\na_1^{(1)} = a_{11}^{(1)}x1 + w_{12}^{(1)}x2 + b_1^{(1)}\n$$\n矩阵$W^{(1)}$表示第１层的权重：\n$$\nW^{(1)} = \\begin{pmatrix}w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)}\\\\\\\\w_{12}^{(1)} &w_{22}^{(1)} &w_{32}^{(1)}\\end{pmatrix}\n$$\n向量$B^{(1)}$表示第一层的偏置:\n$$\n\\begin{pmatrix} b_1^{(1)} & b_2^{(1)} & b_3^{(1)}\\\\ \\end{pmatrix} \\quad\n$$\n\n$$\nA^{(1)} = \\begin{pmatrix} a_1^{(1)} & a_2^{(1)} & a_3^{(1)}\\\\ \\end{pmatrix} \\quad\n$$\n\n$$\nX^{(1)} = \\begin{pmatrix} x_1 & x_2\\\\ \\end{pmatrix} \\quad\n$$\n\n第１层的加权和表示:\n$$\nA^{(1)} = XW^{(1)} + B^{(1)}\n$$\n\n\n\n# 三、神经网络的学习\n\n​\t神经网络的学习就是从训练数据学习权重参数，然后使用刚才学习到的参数对输入数据进行预测。\n\n​\t神经网络学习的策略是首先对输入数据进行前向传播（forward propagation）过程得到输出，然后计算输出与真实值之间的差别，最后通过反向传播跟新权重参数，重复这一过程直到权重参数没有更新，此时损失函数达到最小。\n\n​\t计算输出与真实值之间的差别通过损失函数计算。反向传播需要用到梯度下降算法实现。\n\n\n\n## 1.损失函数\n\n​\t损失函数是表示神经网络性能的指标。神经网络通过减小损失函数，寻找最优权重参数。常用的误差函数有均方误差和交叉熵误差等。\n\n- 均方误差(mean squared error MSE)\n\n$$\nE = \\frac {1}{2} \\sum_{k}({y_k - t_k})^2\n$$\n\n​\t$y_k$表示神经网络的输出，$t_k$表示目标数据，k表示输出数据的维度，比如手写数字识别的输出数据的维度是10 (0-9)。\n\n- 交叉熵误差(cross entropy error)\n  $$\n  E = - \\sum_{k}t_k\\log y_k\n  $$\n  上式是一条数据的误差，如果批量计算多条数据则函数为:\n  $$\n  E = - \\frac{1}{N}\\sum_{N}\\sum_{k}t_{nk}\\log y_{nk}\n  $$\n  假设数据有N个， $t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神\n  经网络的输出， $t_{nk}$是目标数据）, 不过最后还要除以N, 通过这样的\n  平均化，可以获得和训练数据的数量无关的统一指标。\n\n## 2.梯度下降\n\n​\t梯度是导数对多元函数的推广，它是多元函数对各个变量偏导形成的向量。多元函数的梯度定义为:\n$$\n\\triangledown(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}, & ..., & \\frac{\\partial f}{\\partial x_n}\\\\  \\end{pmatrix}^{T} \\quad\n$$\n　其中$\\triangledown$称为梯度算子，它作用与一个多元函数，得到一个向量。梯度是一个向量，即有大小又有方向，大小为该点的变化率，方向是该点增加最快的方向，-$\\triangledown$(x)就为减小最快的方向。所以只要沿着梯度的反方向就可能达到函数的驻点，可能是局部极小值，全局极小值或鞍点。\n\n​\t在梯度下降法中，函数的取值从当前位置沿着梯度反方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度反方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度反方向前进，逐渐减小函数值的过程就是梯度下降法（gradient descent method）。\n\n​\t梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题。\n\n数学公式来表示梯度下降方法:\n$$\nw_0 = w_0 - \\eta \\frac {\\partial f}{\\partial w_0}\n$$\n\n$$\nw_1 = w_1 - \\eta \\frac {\\partial f}{\\partial w_1}\n$$\n\n$\\eta$是学习率，表示根据梯度值变化的幅度，w在神经网络中代表权重.\n\n​\t控制梯度下降的幅度\n\n​\t0.1\t0.01\t0.001\t....\n\n​\t如果学习率太大，误差会逐步增大\n\n​\t如果学习率较大，误差会来回震荡\n\n​\t如果学习率太小， 误差下降缓慢\n\n​\t![](n8.png)\n\n​\t\t\t\t\t图　$f(x_0, x_1) = x_0^2 + x_1^2$的梯度下降更新过程:虚线是函数的等高线\n\n神经网络的学习分成下面4个步骤:\n**步骤1（ mini-batch）**\n\t从训练数据中选出一部分数据，这部分数据称为mini-batch。我们的目标是减小损失函数的值。\n**步骤2（计算梯度）**\n\t为了减小损失函数的值，需要求出各个权重参数的梯度。\n**步骤3（更新参数）**\n\t将权重参数沿梯度方向进行微小更新。\t\t\n\n**步骤4（重复）**\n\t重复步骤1、步骤2、步骤3, 直到梯度为０或接近０．\n\n​\t\t\n\n## 3.误差反向传播\n\n1. **计算图**\n\n   小明在超市买了2个苹果、 3个橘子。其中，苹果每个100日元，\n   橘子每个150日元。消费税是10%，请计算支付金额。\n\n   ![](b1.png)\n\n   ​\t\t\t\t\t\t**计算图的求解过程**\n\n   ​\t在计算图上，从左向右进行计算是一种正方向的传播, 简称为**正向传播**（forward propagation）。同理，反方向的计算，就是**反向传播**（backward propagation）。\n\n   ​\t计算图的特征是可以通过传递“局部计算”获得最终结果。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。\n\n   \n\n   ![](b2.png)\n\n   ​\t计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值. 从上图可以看到，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生200倍大小的影响，苹果的价格将产生2.2倍大小的影响。不过，因为这个例子中消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的1是100%，苹果的价格的1是1元）。\n\n   \n\n   反向传播的计算顺序是，将信号E乘以节点的局部导数，然后将结果传递给下一个节点 \n\n2. **链式法则**\n\n   反向传播将局部导数向正方向的反方向（从右到左）传递，传递这个局部导数的原理，是基于**链式法则**的。\n\n   ![](b3.png)\n\n   **图b3 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数**\n\n   如图所示，反向传播的计算顺序是，将信号$E$乘以节点的局部导数$(\\frac {\\partial y}{\\partial x})$，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中y = f(x)的导数，也就是y关于x的导数$(\\frac {\\partial y}{\\partial x})$。比如，假设$y = f(x) = x^2$，则局部导数为 $(\\frac {\\partial y}{\\partial x})$= $2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。\n\n   \n\n   ![](b4.png)\n\n   根据链式法则，$\\frac {\\partial z}{\\partial z} \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial x}$成立，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的\n\n4. **激活函数的反向传播**\n\n   - Relu激活函数\n     $$\n     h^{'}(x) =\\begin{cases}1, & x > 0 \\\\0, & x \\leq 0\\end{cases}\n     $$\n     如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。\n\n     Relu 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。\n\n   - Sigmoid激活函数\n     $$\n     h^{'}(y) = y(1-y)\n     $$\n     \n\n```python\nclass Sigmoid:\n    def __init__(self):\n        self.out = None     # 保存当前层的输出\n\n    def forward(self, x):\n        \"\"\"\n        前向传播\n        :param x: \n        :return: \n        \"\"\"\n        out = sigmoid(x)\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        \"\"\"\n        反向传播\n        :param dout: \n        :return: \n        \"\"\"\n        dx = dout * (1.0 - self.out) * self.out\n\n        return dx\n```\n\n\n\n4. 矩阵运算\n   $$\n   X \\cdot W = Y\n   $$\n\n   $$\n   \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T\n   $$\n\n   $$\n   \\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}\n   $$\n\n   \n\n   ![](b6.png)\n\n   需要注意各个变量的形状\n\n5. Softmax-with-Loss层\n\n   ![](b7.png)\n\n   Softmax层将输入（a1, a2, a3）正规化，输出（y1,y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和目标值（t1,t2, t3），从这些数据中输出损失L。\n\n   Softmax反向传播的梯度为（y1 - t1,  y2 - t2, y3 - t3）.\n\n## 4.总结\n\n通过使用计算图，可以直观地把握计算过程。\n• 计算图的节点是由局部计算构成的。局部计算构成全局计算。\n• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以\n计算各个节点的导数。\n• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传\n播法）。\n• 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传\n播法的实现是否正确（梯度确认）\n\n\n\n# 四、参考\n\n《深度学习入门: 基于Python的理论与实现》","slug":"神经网络简介","published":1,"updated":"2020-01-05T13:05:02.387Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tsz0012zsv5c1yzmnay","content":"<h1 id=\"一、感知器\"><a href=\"#一、感知器\" class=\"headerlink\" title=\"一、感知器\"></a>一、感知器</h1><p>​    感知器(perceptron)是由美国学者FrankRoseblatt在1957年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。</p>\n<h2 id=\"1-什么是感知器\"><a href=\"#1-什么是感知器\" class=\"headerlink\" title=\"1.什么是感知器\"></a>1.什么是感知器</h2><p>​    感知机接收多个输入，生成一个输出，输出只有两种1和0。</p>\n<p>​    <img src=\"p1.png\" alt></p>\n<p>​                                    <strong>图1.1 有两个输入的感知机</strong></p>\n<p>图1-1是一个接收两个输入的感知机. $x_1$、$x_2$是输入，$y$是输出，$w_1$、$w_2$是权重。图中的○称为神经元或者节点。输入被送往神经元时，会被分别乘以固定的权重$(w_1x_1,  w_2x_2)$。神经元会计算传送过来的输入的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。<br>$$<br>y =\\begin{cases}1, &amp; (w_1x_1 + w_2x_2) &gt; \\theta \\\\0, &amp; (w_1x_1 + w_2x_2) \\leq \\theta\\end{cases}<br>$$<br>​        <strong>权重越大，对应该权重的信号的重要性就越高。</strong></p>\n<h2 id=\"2-逻辑运算\"><a href=\"#2-逻辑运算\" class=\"headerlink\" title=\"2.逻辑运算\"></a>2.逻辑运算</h2><p>使用感知器可以解决简单的逻辑运算，与门（AND）, 与非门（NOT AND）, 或门（OR）.</p>\n<p>​    <img src=\"p2.png\" alt></p>\n<p>​                                        <strong>1-2 与门真值表</strong></p>\n<p>满足图2-2的条件的参数的选择方法有无数多个。比如，当$(w_1, w_2, θ)$ = $(0.5, 0.5, 0.7)$ ​时，可以满足图 2-1的条件。</p>\n<p>我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。</p>\n<h2 id=\"3-偏置和权重\"><a href=\"#3-偏置和权重\" class=\"headerlink\" title=\"3.偏置和权重\"></a>3.偏置和权重</h2><p>$$<br>y =\\begin{cases}1, &amp; (b + w_1x_1 + w_2x_2) &gt; 0 \\\\0, &amp; (b + w_1x_1 + w_2x_2) \\leq 0\\end{cases}<br>$$</p>\n<p>令$b = -\\theta$， $b$称为偏置，$w_1$和$w_2$称为权重, 但是请注意，偏置和权重$w_1$、$w_2$的作用是不一样的。具体地说， $w_1$和$w_2$是控制输入的重要性的参数，而偏置是调整神经元被激活的容易程度（输出为1的程度）的参数。有时也会将$b$、$w_1$、$w_2$这些参数统称为权重。</p>\n<h2 id=\"4-单层感知机的局限性\"><a href=\"#4-单层感知机的局限性\" class=\"headerlink\" title=\"4.单层感知机的局限性\"></a>4.单层感知机的局限性</h2><p>单层感知机的局限性就在于它只能表示由一条直线分割的空间,无法表示用曲线分割的空间。弯曲的曲线无法用感知机表示</p>\n<p>​    <img src=\"p3.png\" alt></p>\n<p>​        <strong>图1-3　○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？</strong></p>\n<p>​    <img src=\"p4.png\" alt></p>\n<p>​                    <strong>图1-4　使用曲线可以分开○和△</strong></p>\n<h2 id=\"5-多层感知机\"><a href=\"#5-多层感知机\" class=\"headerlink\" title=\"5.多层感知机\"></a>5.多层感知机</h2><p>​    单层感知机虽然不能表示异或，但多层感知机的叠加却可以。</p>\n<p>​    <img src=\"p5.png\" alt></p>\n<p>​                        <strong>图1-5　通过组合与门、与非门、或门实现异或门</strong></p>\n<p>​    <img src=\"p6.png\" alt></p>\n<p>​                    <strong>图2-6　用感知机表示异或门</strong>            </p>\n<p>叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。异或可以通过多层感知机实现。</p>\n<p>图2-6中的感知机总共由3层构成，但是因为拥有权重的层实质上只有2层（第0层和第1层之间，第1层和第2层之间），所以称为“2层感知机”。不过，有的文献认为图2-6的感知机是由3层构成的，因而将其称为“3层感知机”。</p>\n<h2 id=\"6-感知机与计算机\"><a href=\"#6-感知机与计算机\" class=\"headerlink\" title=\"6.感知机与计算机\"></a>6.感知机与计算机</h2><p>​    多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？</p>\n<p>​    <strong>理论上</strong>可以说2层感知机就能构建计算机。这是因为，已有研究证明，2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数。但是，使用2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。</p>\n<p>​    实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU）, 然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。</p>\n<h2 id=\"7-总结\"><a href=\"#7-总结\" class=\"headerlink\" title=\"7.总结\"></a>7.总结</h2><p>​    感知机从算法的角度来说就是单位阶跃函数+线性回归算法。</p>\n<h1 id=\"二、神经网路\"><a href=\"#二、神经网路\" class=\"headerlink\" title=\"二、神经网路\"></a>二、神经网路</h1><h2 id=\"1-1-从感知器到神经网络\"><a href=\"#1-1-从感知器到神经网络\" class=\"headerlink\" title=\"1.1 从感知器到神经网络\"></a>1.1 从感知器到神经网络</h2><p>​    一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用<code>sigmoid</code> 函数等平滑的激活函数的多层网络。</p>\n<p>​    <img src=\"p7.png\" alt></p>\n<p>​    如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层.</p>\n<p>​    图中的网络一共由 3 层神经元构成, 但实质上只有 2层神经元有权重，因此将其称为“2层网络”。</p>\n<h2 id=\"1-2-激活函数\"><a href=\"#1-2-激活函数\" class=\"headerlink\" title=\"1.2 激活函数\"></a>1.2 激活函数</h2><p>​    神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比”阶跃函数”更佳平滑.</p>\n<p>​    阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。</p>\n<p>​    激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。</p>\n<p>​    输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。</p>\n<p>​    常见的激活函数:</p>\n<ul>\n<li>阶跃函数:  </li>\n</ul>\n<p>$$<br>h(x) =\\begin{cases}1, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$</p>\n<p>​    <img src=\"n2.png\" alt></p>\n<ul>\n<li>sigmoid函数(S函数)<br>$$<br>h(x) = \\dfrac {1}{1+e^{-x}}<br>$$</li>\n</ul>\n<p>  <img src=\"n3.png\" alt></p>\n<ul>\n<li>Relu函数<br>$$<br>h(x) =\\begin{cases}x, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$</li>\n</ul>\n<p>  <img src=\"n4.png\" alt></p>\n<ul>\n<li>softmax函数</li>\n</ul>\n<p>$$<br>\\sigma(x) =  \\dfrac {e^{a_k}}{ \\sum_{i=1}^n e^{a^i}   }<br>$$</p>\n<p><strong>注意</strong>: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。</p>\n<p><strong>改进</strong>: 先进行归一化，再求值</p>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\">a <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1010</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> <span class=\"token number\">990</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nnp<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> np<span class=\"token punctuation\">.</span>sum<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\" spellcheck=\"true\">#  array([nan, nan, nan])  没有计算正确的值</span>\n\nmi <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>min<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>     <span class=\"token comment\" spellcheck=\"true\"># 990                                 </span>\nma <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>max<span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span>                                 \nnor <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>a<span class=\"token operator\">-</span>mi<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span><span class=\"token punctuation\">(</span>ma<span class=\"token operator\">-</span>mi<span class=\"token punctuation\">)</span>    <span class=\"token comment\" spellcheck=\"true\"># 归一化 array([1. , 0.5, 0. ])   </span>\nnp<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>nor<span class=\"token punctuation\">)</span><span class=\"token operator\">/</span>np<span class=\"token punctuation\">.</span>sum<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>nor<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>                 \n<span class=\"token comment\" spellcheck=\"true\"># array([0.50648039, 0.30719589, 0.18632372])</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>​    一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此<strong>输出层的softmax函数一般会被省略</strong></p>\n<p>​    求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系</p>\n<h2 id=\"1-3-3层神经网络\"><a href=\"#1-3-3层神经网络\" class=\"headerlink\" title=\"1.3 3层神经网络\"></a>1.3 3层神经网络</h2><p>​    <img src=\"n7.png\" alt></p>\n<p>3层神经网络：输入层（第0层）有2个神经元，第1个隐藏层（第1层）有3个神经元，第2个隐藏层（第2层）有2个神经元，输出层（第3层）有2个神经元</p>\n<p><img src=\"n6.png\" alt></p>\n<p>​                                图２－６权重的符号</p>\n<p>请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个，索引表示的是后一层神经元的索引。</p>\n<p>数学公式表示$a_1^{(1)}$<br>$$<br>a_1^{(1)} = a_{11}^{(1)}x1 + w_{12}^{(1)}x2 + b_1^{(1)}<br>$$<br>矩阵$W^{(1)}$表示第１层的权重：<br>$$<br>W^{(1)} = \\begin{pmatrix}w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)}\\\\\\\\w_{12}^{(1)} &amp;w_{22}^{(1)} &amp;w_{32}^{(1)}\\end{pmatrix}<br>$$<br>向量$B^{(1)}$表示第一层的偏置:<br>$$<br>\\begin{pmatrix} b_1^{(1)} &amp; b_2^{(1)} &amp; b_3^{(1)}\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>$$<br>A^{(1)} = \\begin{pmatrix} a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)}\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>$$<br>X^{(1)} = \\begin{pmatrix} x_1 &amp; x_2\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>第１层的加权和表示:<br>$$<br>A^{(1)} = XW^{(1)} + B^{(1)}<br>$$</p>\n<h1 id=\"三、神经网络的学习\"><a href=\"#三、神经网络的学习\" class=\"headerlink\" title=\"三、神经网络的学习\"></a>三、神经网络的学习</h1><p>​    神经网络的学习就是从训练数据学习权重参数，然后使用刚才学习到的参数对输入数据进行预测。</p>\n<p>​    神经网络学习的策略是首先对输入数据进行前向传播（forward propagation）过程得到输出，然后计算输出与真实值之间的差别，最后通过反向传播跟新权重参数，重复这一过程直到权重参数没有更新，此时损失函数达到最小。</p>\n<p>​    计算输出与真实值之间的差别通过损失函数计算。反向传播需要用到梯度下降算法实现。</p>\n<h2 id=\"1-损失函数\"><a href=\"#1-损失函数\" class=\"headerlink\" title=\"1.损失函数\"></a>1.损失函数</h2><p>​    损失函数是表示神经网络性能的指标。神经网络通过减小损失函数，寻找最优权重参数。常用的误差函数有均方误差和交叉熵误差等。</p>\n<ul>\n<li>均方误差(mean squared error MSE)</li>\n</ul>\n<p>$$<br>E = \\frac {1}{2} \\sum_{k}({y_k - t_k})^2<br>$$</p>\n<p>​    $y_k$表示神经网络的输出，$t_k$表示目标数据，k表示输出数据的维度，比如手写数字识别的输出数据的维度是10 (0-9)。</p>\n<ul>\n<li>交叉熵误差(cross entropy error)<br>$$<br>E = - \\sum_{k}t_k\\log y_k<br>$$<br>上式是一条数据的误差，如果批量计算多条数据则函数为:<br>$$<br>E = - \\frac{1}{N}\\sum_{N}\\sum_{k}t_{nk}\\log y_{nk}<br>$$<br>假设数据有N个， $t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神<br>经网络的输出， $t_{nk}$是目标数据）, 不过最后还要除以N, 通过这样的<br>平均化，可以获得和训练数据的数量无关的统一指标。</li>\n</ul>\n<h2 id=\"2-梯度下降\"><a href=\"#2-梯度下降\" class=\"headerlink\" title=\"2.梯度下降\"></a>2.梯度下降</h2><p>​    梯度是导数对多元函数的推广，它是多元函数对各个变量偏导形成的向量。多元函数的梯度定义为:<br>$$<br>\\triangledown(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}, &amp; …, &amp; \\frac{\\partial f}{\\partial x_n}\\\\  \\end{pmatrix}^{T} \\quad<br>$$<br>　其中$\\triangledown$称为梯度算子，它作用与一个多元函数，得到一个向量。梯度是一个向量，即有大小又有方向，大小为该点的变化率，方向是该点增加最快的方向，-$\\triangledown$(x)就为减小最快的方向。所以只要沿着梯度的反方向就可能达到函数的驻点，可能是局部极小值，全局极小值或鞍点。</p>\n<p>​    在梯度下降法中，函数的取值从当前位置沿着梯度反方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度反方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度反方向前进，逐渐减小函数值的过程就是梯度下降法（gradient descent method）。</p>\n<p>​    梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题。</p>\n<p>数学公式来表示梯度下降方法:<br>$$<br>w_0 = w_0 - \\eta \\frac {\\partial f}{\\partial w_0}<br>$$</p>\n<p>$$<br>w_1 = w_1 - \\eta \\frac {\\partial f}{\\partial w_1}<br>$$</p>\n<p>$\\eta$是学习率，表示根据梯度值变化的幅度，w在神经网络中代表权重.</p>\n<p>​    控制梯度下降的幅度</p>\n<p>​    0.1    0.01    0.001    ….</p>\n<p>​    如果学习率太大，误差会逐步增大</p>\n<p>​    如果学习率较大，误差会来回震荡</p>\n<p>​    如果学习率太小， 误差下降缓慢</p>\n<p>​    <img src=\"n8.png\" alt></p>\n<p>​                    图　$f(x_0, x_1) = x_0^2 + x_1^2$的梯度下降更新过程:虚线是函数的等高线</p>\n<p>神经网络的学习分成下面4个步骤:<br><strong>步骤1（ mini-batch）</strong><br>    从训练数据中选出一部分数据，这部分数据称为mini-batch。我们的目标是减小损失函数的值。<br><strong>步骤2（计算梯度）</strong><br>    为了减小损失函数的值，需要求出各个权重参数的梯度。<br><strong>步骤3（更新参数）</strong><br>    将权重参数沿梯度方向进行微小更新。        </p>\n<p><strong>步骤4（重复）</strong><br>    重复步骤1、步骤2、步骤3, 直到梯度为０或接近０．</p>\n<p>​        </p>\n<h2 id=\"3-误差反向传播\"><a href=\"#3-误差反向传播\" class=\"headerlink\" title=\"3.误差反向传播\"></a>3.误差反向传播</h2><ol>\n<li><p><strong>计算图</strong></p>\n<p>小明在超市买了2个苹果、 3个橘子。其中，苹果每个100日元，<br>橘子每个150日元。消费税是10%，请计算支付金额。</p>\n<p><img src=\"b1.png\" alt></p>\n<p>​                        <strong>计算图的求解过程</strong></p>\n<p>​    在计算图上，从左向右进行计算是一种正方向的传播, 简称为<strong>正向传播</strong>（forward propagation）。同理，反方向的计算，就是<strong>反向传播</strong>（backward propagation）。</p>\n<p>​    计算图的特征是可以通过传递“局部计算”获得最终结果。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。</p>\n</li>\n</ol>\n<p>   <img src=\"b2.png\" alt></p>\n<p>   ​    计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值. 从上图可以看到，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生200倍大小的影响，苹果的价格将产生2.2倍大小的影响。不过，因为这个例子中消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的1是100%，苹果的价格的1是1元）。</p>\n<p>   反向传播的计算顺序是，将信号E乘以节点的局部导数，然后将结果传递给下一个节点 </p>\n<ol start=\"2\">\n<li><p><strong>链式法则</strong></p>\n<p>反向传播将局部导数向正方向的反方向（从右到左）传递，传递这个局部导数的原理，是基于<strong>链式法则</strong>的。</p>\n<p><img src=\"b3.png\" alt></p>\n<p><strong>图b3 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数</strong></p>\n<p>如图所示，反向传播的计算顺序是，将信号$E$乘以节点的局部导数$(\\frac {\\partial y}{\\partial x})$，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中y = f(x)的导数，也就是y关于x的导数$(\\frac {\\partial y}{\\partial x})$。比如，假设$y = f(x) = x^2$，则局部导数为 $(\\frac {\\partial y}{\\partial x})$= $2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。</p>\n</li>\n</ol>\n<p>   <img src=\"b4.png\" alt></p>\n<p>   根据链式法则，$\\frac {\\partial z}{\\partial z} \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial x}$成立，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的</p>\n<ol start=\"4\">\n<li><p><strong>激活函数的反向传播</strong></p>\n<ul>\n<li><p>Relu激活函数<br>$$<br>h^{‘}(x) =\\begin{cases}1, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$<br>如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。</p>\n<p>Relu 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。</p>\n</li>\n<li><p>Sigmoid激活函数<br>$$<br>h^{‘}(y) = y(1-y)<br>$$</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre class=\"line-numbers language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Sigmoid</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>out <span class=\"token operator\">=</span> None     <span class=\"token comment\" spellcheck=\"true\"># 保存当前层的输出</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"\n        前向传播\n        :param x: \n        :return: \n        \"\"\"</span>\n        out <span class=\"token operator\">=</span> sigmoid<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>out <span class=\"token operator\">=</span> out\n        <span class=\"token keyword\">return</span> out\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">backward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dout<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"\n        反向传播\n        :param dout: \n        :return: \n        \"\"\"</span>\n        dx <span class=\"token operator\">=</span> dout <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.0</span> <span class=\"token operator\">-</span> self<span class=\"token punctuation\">.</span>out<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> self<span class=\"token punctuation\">.</span>out\n\n        <span class=\"token keyword\">return</span> dx<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<ol start=\"4\">\n<li><p>矩阵运算<br>$$<br>X \\cdot W = Y<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}<br>$$</p>\n</li>\n</ol>\n<p>   <img src=\"b6.png\" alt></p>\n<p>   需要注意各个变量的形状</p>\n<ol start=\"5\">\n<li><p>Softmax-with-Loss层</p>\n<p><img src=\"b7.png\" alt></p>\n<p>Softmax层将输入（a1, a2, a3）正规化，输出（y1,y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和目标值（t1,t2, t3），从这些数据中输出损失L。</p>\n<p>Softmax反向传播的梯度为（y1 - t1,  y2 - t2, y3 - t3）.</p>\n</li>\n</ol>\n<h2 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4.总结\"></a>4.总结</h2><p>通过使用计算图，可以直观地把握计算过程。<br>• 计算图的节点是由局部计算构成的。局部计算构成全局计算。<br>• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以<br>计算各个节点的导数。<br>• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传<br>播法）。<br>• 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传<br>播法的实现是否正确（梯度确认）</p>\n<h1 id=\"四、参考\"><a href=\"#四、参考\" class=\"headerlink\" title=\"四、参考\"></a>四、参考</h1><p>《深度学习入门: 基于Python的理论与实现》</p>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、感知器\"><a href=\"#一、感知器\" class=\"headerlink\" title=\"一、感知器\"></a>一、感知器</h1><p>​    感知器(perceptron)是由美国学者FrankRoseblatt在1957年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。</p>\n<h2 id=\"1-什么是感知器\"><a href=\"#1-什么是感知器\" class=\"headerlink\" title=\"1.什么是感知器\"></a>1.什么是感知器</h2><p>​    感知机接收多个输入，生成一个输出，输出只有两种1和0。</p>\n<p>​    <img src=\"p1.png\" alt></p>\n<p>​                                    <strong>图1.1 有两个输入的感知机</strong></p>\n<p>图1-1是一个接收两个输入的感知机. $x_1$、$x_2$是输入，$y$是输出，$w_1$、$w_2$是权重。图中的○称为神经元或者节点。输入被送往神经元时，会被分别乘以固定的权重$(w_1x_1,  w_2x_2)$。神经元会计算传送过来的输入的总和，只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号θ表示。<br>$$<br>y =\\begin{cases}1, &amp; (w_1x_1 + w_2x_2) &gt; \\theta \\\\0, &amp; (w_1x_1 + w_2x_2) \\leq \\theta\\end{cases}<br>$$<br>​        <strong>权重越大，对应该权重的信号的重要性就越高。</strong></p>\n<h2 id=\"2-逻辑运算\"><a href=\"#2-逻辑运算\" class=\"headerlink\" title=\"2.逻辑运算\"></a>2.逻辑运算</h2><p>使用感知器可以解决简单的逻辑运算，与门（AND）, 与非门（NOT AND）, 或门（OR）.</p>\n<p>​    <img src=\"p2.png\" alt></p>\n<p>​                                        <strong>1-2 与门真值表</strong></p>\n<p>满足图2-2的条件的参数的选择方法有无数多个。比如，当$(w_1, w_2, θ)$ = $(0.5, 0.5, 0.7)$ ​时，可以满足图 2-1的条件。</p>\n<p>我们看着真值表这种“训练数据”，人工考虑（想到）了参数的值。而机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。 学习是确定合适的参数的过程，而人要做的是思考感知机的构造（模型），并把训练数据交给计算机。</p>\n<h2 id=\"3-偏置和权重\"><a href=\"#3-偏置和权重\" class=\"headerlink\" title=\"3.偏置和权重\"></a>3.偏置和权重</h2><p>$$<br>y =\\begin{cases}1, &amp; (b + w_1x_1 + w_2x_2) &gt; 0 \\\\0, &amp; (b + w_1x_1 + w_2x_2) \\leq 0\\end{cases}<br>$$</p>\n<p>令$b = -\\theta$， $b$称为偏置，$w_1$和$w_2$称为权重, 但是请注意，偏置和权重$w_1$、$w_2$的作用是不一样的。具体地说， $w_1$和$w_2$是控制输入的重要性的参数，而偏置是调整神经元被激活的容易程度（输出为1的程度）的参数。有时也会将$b$、$w_1$、$w_2$这些参数统称为权重。</p>\n<h2 id=\"4-单层感知机的局限性\"><a href=\"#4-单层感知机的局限性\" class=\"headerlink\" title=\"4.单层感知机的局限性\"></a>4.单层感知机的局限性</h2><p>单层感知机的局限性就在于它只能表示由一条直线分割的空间,无法表示用曲线分割的空间。弯曲的曲线无法用感知机表示</p>\n<p>​    <img src=\"p3.png\" alt></p>\n<p>​        <strong>图1-3　○和△表示异或门的输出。可否通过一条直线作出分割○和△的空间呢？</strong></p>\n<p>​    <img src=\"p4.png\" alt></p>\n<p>​                    <strong>图1-4　使用曲线可以分开○和△</strong></p>\n<h2 id=\"5-多层感知机\"><a href=\"#5-多层感知机\" class=\"headerlink\" title=\"5.多层感知机\"></a>5.多层感知机</h2><p>​    单层感知机虽然不能表示异或，但多层感知机的叠加却可以。</p>\n<p>​    <img src=\"p5.png\" alt></p>\n<p>​                        <strong>图1-5　通过组合与门、与非门、或门实现异或门</strong></p>\n<p>​    <img src=\"p6.png\" alt></p>\n<p>​                    <strong>图2-6　用感知机表示异或门</strong>            </p>\n<p>叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。异或可以通过多层感知机实现。</p>\n<p>图2-6中的感知机总共由3层构成，但是因为拥有权重的层实质上只有2层（第0层和第1层之间，第1层和第2层之间），所以称为“2层感知机”。不过，有的文献认为图2-6的感知机是由3层构成的，因而将其称为“3层感知机”。</p>\n<h2 id=\"6-感知机与计算机\"><a href=\"#6-感知机与计算机\" class=\"headerlink\" title=\"6.感知机与计算机\"></a>6.感知机与计算机</h2><p>​    多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？</p>\n<p>​    <strong>理论上</strong>可以说2层感知机就能构建计算机。这是因为，已有研究证明，2层感知机（严格地说是激活函数使用了非线性的sigmoid函数的感知机）可以表示任意函数。但是，使用2层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。</p>\n<p>​    实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU）, 然后实现CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。</p>\n<h2 id=\"7-总结\"><a href=\"#7-总结\" class=\"headerlink\" title=\"7.总结\"></a>7.总结</h2><p>​    感知机从算法的角度来说就是单位阶跃函数+线性回归算法。</p>\n<h1 id=\"二、神经网路\"><a href=\"#二、神经网路\" class=\"headerlink\" title=\"二、神经网路\"></a>二、神经网路</h1><h2 id=\"1-1-从感知器到神经网络\"><a href=\"#1-1-从感知器到神经网络\" class=\"headerlink\" title=\"1.1 从感知器到神经网络\"></a>1.1 从感知器到神经网络</h2><p>​    一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 的模型。“多层感知机”是指神经网络，即使用<code>sigmoid</code> 函数等平滑的激活函数的多层网络。</p>\n<p>​    <img src=\"p7.png\" alt></p>\n<p>​    如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层.</p>\n<p>​    图中的网络一共由 3 层神经元构成, 但实质上只有 2层神经元有权重，因此将其称为“2层网络”。</p>\n<h2 id=\"1-2-激活函数\"><a href=\"#1-2-激活函数\" class=\"headerlink\" title=\"1.2 激活函数\"></a>1.2 激活函数</h2><p>​    神经网络与感知机的一个最大区别是它使用了“阶跃函数”之外的其他激活函数，比如sigmoid函数。sigmoid函数相比”阶跃函数”更佳平滑.</p>\n<p>​    阶跃函数和sigmoid函数均为非线性函数, 线性函数是一条笔直的直线，而非线性函数，顾名思义，指的是不像线性函数那样呈现出一条直线的函数。</p>\n<p>​    激活函数一定是非线性函数，它的主要作用就是增加神经网络的非线性，因为线性函数的线性组合还是线性函数，这样的话多层神经网络就没有意义。</p>\n<p>​    输出层的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。所谓恒等函数，就是按输入原样输出，对于输入的信息，不加任何改动地直接输出。</p>\n<p>​    常见的激活函数:</p>\n<ul>\n<li>阶跃函数:  </li>\n</ul>\n<p>$$<br>h(x) =\\begin{cases}1, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$</p>\n<p>​    <img src=\"n2.png\" alt></p>\n<ul>\n<li>sigmoid函数(S函数)<br>$$<br>h(x) = \\dfrac {1}{1+e^{-x}}<br>$$</li>\n</ul>\n<p>  <img src=\"n3.png\" alt></p>\n<ul>\n<li>Relu函数<br>$$<br>h(x) =\\begin{cases}x, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$</li>\n</ul>\n<p>  <img src=\"n4.png\" alt></p>\n<ul>\n<li>softmax函数</li>\n</ul>\n<p>$$<br>\\sigma(x) =  \\dfrac {e^{a_k}}{ \\sum_{i=1}^n e^{a^i}   }<br>$$</p>\n<p><strong>注意</strong>: softmax函数有一个缺陷就是溢出问题，softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。如，$e^{1000}$的结果会返回一个表示无穷大的inf。</p>\n<p><strong>改进</strong>: 先进行归一化，再求值</p>\n<pre><code class=\"python\">a = np.array([1010, 1000, 990])\nnp.exp(a) / np.sum(np.exp(a))\n#  array([nan, nan, nan])  没有计算正确的值\n\nmi = np.min(a)     # 990                                 \nma = np.max(a)                                 \nnor = (a-mi)/(ma-mi)    # 归一化 array([1. , 0.5, 0. ])   \nnp.exp(nor)/np.sum(np.exp(nor))                 \n# array([0.50648039, 0.30719589, 0.18632372])\n</code></pre>\n<p>​    一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此<strong>输出层的softmax函数一般会被省略</strong></p>\n<p>​    求解机器学习问题的步骤可以分为“学习” 和“推理”两个阶段。首先， 在学习阶段进行模型的学习 ，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系</p>\n<h2 id=\"1-3-3层神经网络\"><a href=\"#1-3-3层神经网络\" class=\"headerlink\" title=\"1.3 3层神经网络\"></a>1.3 3层神经网络</h2><p>​    <img src=\"n7.png\" alt></p>\n<p>3层神经网络：输入层（第0层）有2个神经元，第1个隐藏层（第1层）有3个神经元，第2个隐藏层（第2层）有2个神经元，输出层（第3层）有2个神经元</p>\n<p><img src=\"n6.png\" alt></p>\n<p>​                                图２－６权重的符号</p>\n<p>请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个，索引表示的是后一层神经元的索引。</p>\n<p>数学公式表示$a_1^{(1)}$<br>$$<br>a_1^{(1)} = a_{11}^{(1)}x1 + w_{12}^{(1)}x2 + b_1^{(1)}<br>$$<br>矩阵$W^{(1)}$表示第１层的权重：<br>$$<br>W^{(1)} = \\begin{pmatrix}w_{11}^{(1)} &amp; w_{12}^{(1)} &amp; w_{13}^{(1)}\\\\\\\\w_{12}^{(1)} &amp;w_{22}^{(1)} &amp;w_{32}^{(1)}\\end{pmatrix}<br>$$<br>向量$B^{(1)}$表示第一层的偏置:<br>$$<br>\\begin{pmatrix} b_1^{(1)} &amp; b_2^{(1)} &amp; b_3^{(1)}\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>$$<br>A^{(1)} = \\begin{pmatrix} a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)}\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>$$<br>X^{(1)} = \\begin{pmatrix} x_1 &amp; x_2\\\\ \\end{pmatrix} \\quad<br>$$</p>\n<p>第１层的加权和表示:<br>$$<br>A^{(1)} = XW^{(1)} + B^{(1)}<br>$$</p>\n<h1 id=\"三、神经网络的学习\"><a href=\"#三、神经网络的学习\" class=\"headerlink\" title=\"三、神经网络的学习\"></a>三、神经网络的学习</h1><p>​    神经网络的学习就是从训练数据学习权重参数，然后使用刚才学习到的参数对输入数据进行预测。</p>\n<p>​    神经网络学习的策略是首先对输入数据进行前向传播（forward propagation）过程得到输出，然后计算输出与真实值之间的差别，最后通过反向传播跟新权重参数，重复这一过程直到权重参数没有更新，此时损失函数达到最小。</p>\n<p>​    计算输出与真实值之间的差别通过损失函数计算。反向传播需要用到梯度下降算法实现。</p>\n<h2 id=\"1-损失函数\"><a href=\"#1-损失函数\" class=\"headerlink\" title=\"1.损失函数\"></a>1.损失函数</h2><p>​    损失函数是表示神经网络性能的指标。神经网络通过减小损失函数，寻找最优权重参数。常用的误差函数有均方误差和交叉熵误差等。</p>\n<ul>\n<li>均方误差(mean squared error MSE)</li>\n</ul>\n<p>$$<br>E = \\frac {1}{2} \\sum_{k}({y_k - t_k})^2<br>$$</p>\n<p>​    $y_k$表示神经网络的输出，$t_k$表示目标数据，k表示输出数据的维度，比如手写数字识别的输出数据的维度是10 (0-9)。</p>\n<ul>\n<li>交叉熵误差(cross entropy error)<br>$$<br>E = - \\sum_{k}t_k\\log y_k<br>$$<br>上式是一条数据的误差，如果批量计算多条数据则函数为:<br>$$<br>E = - \\frac{1}{N}\\sum_{N}\\sum_{k}t_{nk}\\log y_{nk}<br>$$<br>假设数据有N个， $t_{nk}$表示第n个数据的第k个元素的值（$y_{nk}$是神<br>经网络的输出， $t_{nk}$是目标数据）, 不过最后还要除以N, 通过这样的<br>平均化，可以获得和训练数据的数量无关的统一指标。</li>\n</ul>\n<h2 id=\"2-梯度下降\"><a href=\"#2-梯度下降\" class=\"headerlink\" title=\"2.梯度下降\"></a>2.梯度下降</h2><p>​    梯度是导数对多元函数的推广，它是多元函数对各个变量偏导形成的向量。多元函数的梯度定义为:<br>$$<br>\\triangledown(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}, &amp; …, &amp; \\frac{\\partial f}{\\partial x_n}\\\\  \\end{pmatrix}^{T} \\quad<br>$$<br>　其中$\\triangledown$称为梯度算子，它作用与一个多元函数，得到一个向量。梯度是一个向量，即有大小又有方向，大小为该点的变化率，方向是该点增加最快的方向，-$\\triangledown$(x)就为减小最快的方向。所以只要沿着梯度的反方向就可能达到函数的驻点，可能是局部极小值，全局极小值或鞍点。</p>\n<p>​    在梯度下降法中，函数的取值从当前位置沿着梯度反方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度反方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度反方向前进，逐渐减小函数值的过程就是梯度下降法（gradient descent method）。</p>\n<p>​    梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题。</p>\n<p>数学公式来表示梯度下降方法:<br>$$<br>w_0 = w_0 - \\eta \\frac {\\partial f}{\\partial w_0}<br>$$</p>\n<p>$$<br>w_1 = w_1 - \\eta \\frac {\\partial f}{\\partial w_1}<br>$$</p>\n<p>$\\eta$是学习率，表示根据梯度值变化的幅度，w在神经网络中代表权重.</p>\n<p>​    控制梯度下降的幅度</p>\n<p>​    0.1    0.01    0.001    ….</p>\n<p>​    如果学习率太大，误差会逐步增大</p>\n<p>​    如果学习率较大，误差会来回震荡</p>\n<p>​    如果学习率太小， 误差下降缓慢</p>\n<p>​    <img src=\"n8.png\" alt></p>\n<p>​                    图　$f(x_0, x_1) = x_0^2 + x_1^2$的梯度下降更新过程:虚线是函数的等高线</p>\n<p>神经网络的学习分成下面4个步骤:<br><strong>步骤1（ mini-batch）</strong><br>    从训练数据中选出一部分数据，这部分数据称为mini-batch。我们的目标是减小损失函数的值。<br><strong>步骤2（计算梯度）</strong><br>    为了减小损失函数的值，需要求出各个权重参数的梯度。<br><strong>步骤3（更新参数）</strong><br>    将权重参数沿梯度方向进行微小更新。        </p>\n<p><strong>步骤4（重复）</strong><br>    重复步骤1、步骤2、步骤3, 直到梯度为０或接近０．</p>\n<p>​        </p>\n<h2 id=\"3-误差反向传播\"><a href=\"#3-误差反向传播\" class=\"headerlink\" title=\"3.误差反向传播\"></a>3.误差反向传播</h2><ol>\n<li><p><strong>计算图</strong></p>\n<p>小明在超市买了2个苹果、 3个橘子。其中，苹果每个100日元，<br>橘子每个150日元。消费税是10%，请计算支付金额。</p>\n<p><img src=\"b1.png\" alt></p>\n<p>​                        <strong>计算图的求解过程</strong></p>\n<p>​    在计算图上，从左向右进行计算是一种正方向的传播, 简称为<strong>正向传播</strong>（forward propagation）。同理，反方向的计算，就是<strong>反向传播</strong>（backward propagation）。</p>\n<p>​    计算图的特征是可以通过传递“局部计算”获得最终结果。局部计算是指，无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。</p>\n</li>\n</ol>\n<p>   <img src=\"b2.png\" alt></p>\n<p>   ​    计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值. 从上图可以看到，如果消费税和苹果的价格增加相同的值，则消费税将对最终价格产生200倍大小的影响，苹果的价格将产生2.2倍大小的影响。不过，因为这个例子中消费税和苹果的价格的量纲不同，所以才形成了这样的结果（消费税的1是100%，苹果的价格的1是1元）。</p>\n<p>   反向传播的计算顺序是，将信号E乘以节点的局部导数，然后将结果传递给下一个节点 </p>\n<ol start=\"2\">\n<li><p><strong>链式法则</strong></p>\n<p>反向传播将局部导数向正方向的反方向（从右到左）传递，传递这个局部导数的原理，是基于<strong>链式法则</strong>的。</p>\n<p><img src=\"b3.png\" alt></p>\n<p><strong>图b3 计算图的反向传播：沿着与正方向相反的方向，乘上局部导数</strong></p>\n<p>如图所示，反向传播的计算顺序是，将信号$E$乘以节点的局部导数$(\\frac {\\partial y}{\\partial x})$，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中y = f(x)的导数，也就是y关于x的导数$(\\frac {\\partial y}{\\partial x})$。比如，假设$y = f(x) = x^2$，则局部导数为 $(\\frac {\\partial y}{\\partial x})$= $2x$。把这个局部导数乘以上游传过来的值（本例中为E），然后传递给前面的节点。</p>\n</li>\n</ol>\n<p>   <img src=\"b4.png\" alt></p>\n<p>   根据链式法则，$\\frac {\\partial z}{\\partial z} \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial t} \\frac {\\partial t}{\\partial x} = \\frac {\\partial z}{\\partial x}$成立，对应“z关于x的导数”。也就是说，反向传播是基于链式法则的</p>\n<ol start=\"4\">\n<li><p><strong>激活函数的反向传播</strong></p>\n<ul>\n<li><p>Relu激活函数<br>$$<br>h^{‘}(x) =\\begin{cases}1, &amp; x &gt; 0 \\\\0, &amp; x \\leq 0\\end{cases}<br>$$<br>如果正向传播时的输入x大于0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处。</p>\n<p>Relu 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为ON；没有电流通过的话，就将开关设为OFF。反向传播时，开关为ON的话，电流会直接通过；开关为OFF的话，则不会有电流通过。</p>\n</li>\n<li><p>Sigmoid激活函数<br>$$<br>h^{‘}(y) = y(1-y)<br>$$</p>\n</li>\n</ul>\n</li>\n</ol>\n<pre><code class=\"python\">class Sigmoid:\n    def __init__(self):\n        self.out = None     # 保存当前层的输出\n\n    def forward(self, x):\n        &quot;&quot;&quot;\n        前向传播\n        :param x: \n        :return: \n        &quot;&quot;&quot;\n        out = sigmoid(x)\n        self.out = out\n        return out\n\n    def backward(self, dout):\n        &quot;&quot;&quot;\n        反向传播\n        :param dout: \n        :return: \n        &quot;&quot;&quot;\n        dx = dout * (1.0 - self.out) * self.out\n\n        return dx</code></pre>\n<ol start=\"4\">\n<li><p>矩阵运算<br>$$<br>X \\cdot W = Y<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T<br>$$</p>\n<p>$$<br>\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}<br>$$</p>\n</li>\n</ol>\n<p>   <img src=\"b6.png\" alt></p>\n<p>   需要注意各个变量的形状</p>\n<ol start=\"5\">\n<li><p>Softmax-with-Loss层</p>\n<p><img src=\"b7.png\" alt></p>\n<p>Softmax层将输入（a1, a2, a3）正规化，输出（y1,y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和目标值（t1,t2, t3），从这些数据中输出损失L。</p>\n<p>Softmax反向传播的梯度为（y1 - t1,  y2 - t2, y3 - t3）.</p>\n</li>\n</ol>\n<h2 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4.总结\"></a>4.总结</h2><p>通过使用计算图，可以直观地把握计算过程。<br>• 计算图的节点是由局部计算构成的。局部计算构成全局计算。<br>• 计算图的正向传播进行一般的计算。通过计算图的反向传播，可以<br>计算各个节点的导数。<br>• 通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传<br>播法）。<br>• 通过比较数值微分和误差反向传播法的结果，可以确认误差反向传<br>播法的实现是否正确（梯度确认）</p>\n<h1 id=\"四、参考\"><a href=\"#四、参考\" class=\"headerlink\" title=\"四、参考\"></a>四、参考</h1><p>《深度学习入门: 基于Python的理论与实现》</p>\n"},{"title":"C和C++语法总结","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2020-01-03T11:07:13.000Z","password":null,"summary":null,"_content":"\n\n\n# 一、C语言篇\n\n## 1、gcc/g++\n\n**1.1、为什么需要gcc/g++**\n\n编辑器(如vi、记事本)是指我用它来写程序的（编辑代码），而我们写的代码语句，电脑是不懂的，我们需要把它转成电脑能懂的语句，编译器就是这样的转化工具。就是说，**我们用编辑器编写程序，由编译器编译后才可以运行！**\n\n**1.2、gcc编译器介绍**\n\n编译器是将易于编写、阅读和维护的高级计算机语言翻译为计算机能解读、运行的低级机器语言的程序。\n\ngcc（GNU Compiler Collection，GNU 编译器套件），是由 GNU 开发的编程语言编译器。gcc原本作为GNU操作系统的官方编译器，现已被大多数类Unix操作系统（如Linux、BSD、Mac OS X等）采纳为标准的编译器，gcc同样适用于微软的Windows。\n\ngcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。\n\n \n\ngcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。\n\n**1.3、编译命令**\n\n编译命令格式：\n\ngcc [-option1] ... <filename>\n\ng++ [-option1] ... <filename>\n\n- 命令、选项和源文件之间使用空格分隔\n\n- 一行命令中可以有零个、一个或多个选项\n\n- 文件名可以包含文件的绝对路径，也可以使用相对路径\n\n- 如果命令中不包含输出可执行文件的文件名，可执行文件的文件名会自动生成一个默认名，Linux平台为**a.out**，Windows平台为**a.exe**\n\n \n\ngcc、g++编译常用选项说明：\n\n| **选项** | **含义**                   |\n| -------- | -------------------------- |\n| -o file  | 指定生成的输出文件名为file |\n| -E       | 只进行预处理               |\n| -S(大写) | 只进行预处理和编译         |\n| -c(小写) | 只进行预处理、编译和汇编   |\n\n \n\n**1.4、gcc/g++的区别**\n\n1. gcc与g++都可以编译c代码和c++代码, 但是: 后缀为.c的，gcc会把它当做C程序, 而g++当做是C++程序; 后缀为.cpp的，两者都会认为是C++程序.\n\n2. 编译阶段，可以使用gcc/g++, g++会自动调用gcc。而链接阶段,可以用g++或者gcc -lstdc++\n\n   ,因为gcc命令不能自动和c++程序使用的库链接，通常用g++来完成。\n\n\n\n## 2、C/C++编译过程\n\n**2.1、编译步骤**\n\nC代码编译成可执行程序经过4步：\n\n1）预处理：宏定义展开、头文件展开、条件编译等，同时将代码中的注释删除，这里并不会检查语法\n\n2)   编译：检查语法，将预处理后文件编译生成汇编文件\n\n3）汇编：将汇编文件生成目标文件(二进制文件)\n\n4）链接：C语言写的程序是需要依赖各种库的，所以编译之后还需要把库链接到最终的可执行程序中去\n\n![img](file:////tmp/wps-yang-pc/ksohtml/wpsOWVxwY.jpg)\n\n**2.2、gcc编译过程**\n\n1) 分步编译\n\n预处理：`gcc -E hello.c -o hello.i`\n\n编  译：`gcc -S hello.i -o hello.s`\n\n汇  编：`gcc -c hello.s -o hello.o`\n\n链  接：`gcc   hello.o -o hello_elf`\n\n \n\n| **选项** | **含义**                    |\n| -------- | --------------------------- |\n| -E       | 只进行预处理                |\n| -S(大写) | 只进行预处理和编译          |\n| -c(小写) | 只进行预处理、编译和汇编    |\n| -o file  | 指定生成的输出文件名为 file |\n\n \n\n| ***\\*文件后缀\\**** | ***\\*含义\\****        |\n| ------------------ | --------------------- |\n| .c                 | C 语言文件            |\n| .i                 | 预处理后的 C 语言文件 |\n| .s                 | 编译后的汇编文件      |\n| .o                 | 编译后的目标文件      |\n\n2) 一步编译\n\n`gcc hello.c -o demo `\n\n\n\n**2.3、查找程序所依赖的动态库**\n\n１）Linux平台下，`ldd`(“l”为字母) 可执行程序\n\n２）Windows平台下，需要相应软件(`Depends.exe`)\n\n\n\n## 3、CPU内部结构与寄存器\n\n### 3.1、CPU总线\n\n**数据总线**\n\n（1） 是CPU与内存或其他器件之间的数据传送的通道。\n\n（2）数据总线的宽度决定了CPU和外界的数据传送速度。\n\n（3）每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据(即一个字节)。\n\n（4）数据总线是数据线数量之和。\n\n数据总线数据总线是CPU与存储器、CPU与I/O接口设备之间传送数据信息(各种指令数据信息)的总线，这些信号通过数据总线往返于CPU与存储器、CPU与I/O接口设备之间，因此，数据总线上的信息是双向传输的。\n\n**地址总线**\n\n（1）CPU是通过地址总线来指定存储单元的。\n\n（2）地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据（1024个内存单元）(1B)\n\n（3）地址总线是地址线数量之和。\n\n地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。它是单向的，只能从CPU传向外部存储器或I/O端口\n\n有个说法：64位系统装了64位操作系统，最大物理内存理论上=2的64次方；然而实际上地址总线只用到了35位，所以最大物理内存是32G大小\n\n**控制总线**\n\n（1）CPU通过控制总线对外部器件进行控制。\n\n（2）控制总线的宽度决定了CPU对外部器件的控制能力。\n\n（3）控制总线是控制线数量之和。\n\n控制总线，英文名称：ControlBus，简称：CB。控制总线主要用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和输入输出设备接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的\n\n### 3.2、64位和32位系统区别\n\n- 寄存器是CPU内部最基本的存储单元\n- CPU的主要组成包括了运算器和控制器。运算器是由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。\n- CPU位数=CPU中寄存器的位数=CPU能够一次并行处理的数据宽度（位数）=数据总线宽度\n- **CPU的位宽(位数)一般是以 min{ALU位宽、通用寄存器位宽、数据总线位宽}决定的**\n\n- CPU对外是通过总线(地址、控制、数据)来和外部设备交互的，总线的宽度是8位，同时CPU的寄存器也是8位，那么这个CPU就叫8位CPU\n\n- 如果总线是32位，寄存器也是32位的，那么这个CPU就是32位CPU\n\n- 有一种CPU内部的寄存器是32位的，但总线是16位，准32位CPU\n\n- 所有的64位CPU兼容32位的指令，32位要兼容16位的指令，所以在64位的CPU上是可以识别32位的指令\n\n- 在64位的CPU构架上运行了64位的软件操作系统，那么这个系统是64位\n\n- 在64位的CPU构架上，运行了32位的软件操作系统，那么这个系统就是32位\n\n- 64位的软件不能运行在32位的CPU之上\n\n### 3.3、寄存器、缓存、内存三者关系\n\n1. 寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。\n\n2. 内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。\n\n3. 寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。\n4. Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache(L1 Cache)和二级Cache(L2 Cache)，L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。\n\n总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。\n\n\n\n## 4、内存、地址和指针\n\n### 4.1、内存\n\n内存含义：\n\n- 存储器：计算机的组成中，用来存储程序和数据，辅助CPU进行运算处理的重要部分。\n\n- 内存：内部存贮器，暂存程序/数据——掉电丢失 SRAM、DRAM、DDR、DDR2、DDR3。\n\n- 外存：外部存储器，长时间保存程序/数据—掉电不丢ROM、ERRROM、FLASH（NAND、NOR）、硬盘、光盘。\n\n \n\n内存是沟通CPU与硬盘的桥梁：\n\n- 暂存放CPU中的运算数据\n\n- 暂存与硬盘等外部存储器交换的数据\n\n\n\n### 4.2、物理存储器和存储地址空间\n\n有关内存的两个概念：物理存储器和存储地址空间。\n\n \n\n物理存储器：实际存在的具体存储器芯片。\n\n- 主板上装插的内存条\n\n- 显示卡上的显示RAM芯片\n\n- 各种适配卡上的RAM芯片和ROM芯片\n\n \n\n存储地址空间：对存储器编码的范围。我们在软件上常说的内存是指这一层含义。\n\n- 编码：对每个物理存储单元（一个字节）分配一个号码\n\n- 寻址：可以根据分配的号码找到相应的存储单元，完成数据的读写\n\n\n\n### 4.3、内存地址\n\n- 将内存抽象成一个很大的一维字符数组。\n\n- 编码就是对内存的每一个字节分配一个32位或64位的编号（与32位或者64位处理器相关）。\n\n- 这个内存编号我们称之为内存地址。\n\n内存中的每一个数据都会分配相应的地址：\n\n- char:占一个字节分配一个地址\n\n- int: 占四个字节分配四个地址\n\n- float、struct、函数、数组等\n\n\n\n### 4.4、指针和指针变量\n\n- 内存区的每一个字节都有一个编号，这就是“地址”。\n\n- 如果在程序中定义了一个变量，在对程序进行编译或运行时，系统就会给这个变量分配内存单元，并确定它的内存地址(编号)\n\n- 指针的实质就是内存“地址”。指针就是地址，地址就是指针。\n\n- 指针是内存单元的编号，指针变量是存放地址的变量。\n\n- **通常我们叙述时会把指针变量简称为指针，实际他们含义并不一样**\n\n\n\n## 5、存储类型\n\n### 5.1、auto\n\n1. 普通局部变量，自动存储，该对象会自动创建和销毁，调用函数时分配内存，函数结束时释放内存。只在{}内有效，存放在堆栈中一般省略auto,  不会被默认初始化，初值不随机\n2. 全局变量，**不允许声明为auto变量**， register不适用于全局变量，生命周期由定义到程序运行结束，没有初始化会**自动赋值0或空字符**。全局变量属于整个程序，不同文件中不能有同名的全局变量，通过**extern**在其他文件中引用使用\n\n### 5.2、static\n\n1. 静态局部变量，生命周期由定义到程序运行结束，在编译时赋初值，**只初始化一次，没有初始化会自动赋值0或空字符**。只在当前{}内有效\n2. 静态全局变量，生命周期由定义到程序运行结束，在编译时赋初值，只初始化一次，没有初始化会自动赋值0或空字符。从定义到文件结尾起作用，在一个程序中的其他文件中可以定义同名的静态全局变量，因为作用于不冲突。\n\n### 5.3、extern\n\n1. 外部变量声明，是指这是一个已在别的地方定义过的对象，这里只是对变量的一次重复引用，不会产生新的变量。\n\n2. 使用extern时，注意不能重复定义，否则编译报错\n\n   ```c\n   //    程序文件一：\n       extern int a = 10; //编译警告，extern的变量最好不要初始化\n   //    程序文件二：\n       extern int a = 20; //重复定义，应改为extern int a;\n   ```\n\n3. 如果我们希望该外部变量只能在本文件内使用，而不能被其他文件引用可以在外部变量定义时加static声明。防止别人写的模块误用。\n\n4. 在函数外部定义的全局变量，作用域开始于变量定义，结束于程序文件的结束。我们可以extern来声明外部变量来扩展它的作用域。同一个文件内，extern声明之后就可以作用域扩大到声明处到文件结束。比如在一个函数之后定义外部变量a，之后的函数可以使用该变量，但是之前的函数不能使用，加extern可以解决。\n\n   ```c\n   #include <stdio.h>\n   \n   extern int g1;\n   int main(void)\n   {\n       extern int g2;\n       printf(\"%d,%d\\n\", g1,g2);\n       return 0;\n   }\n   int g1 = 77;\n   int g2 = 88;\n   ```\n\n   \n\n5. 多个文件时，可以在未定义该外部变量的文件内做extern声明即可以使用。但是需要注意可能执行一个文件时改变了该全局变量的值，影响其他文件的调用。编译时遇到extern，会先在文件内找是否定义了该外部变量。如果未找到则在链接时在其他文件中找。\n\n### 5.4、register\n\n1. 寄存器变量，请求编译器将这个变量保存在CPU的寄存器中，从而加快程序的运行.只是建议CPU这样做，非强制,声明变量为register,编译器并不一定会将它处理为寄存器变量\n\n2. 动态和静态变量都是存放在内存中，程序中遇到该值时用控制器发指令将变量的值送到运算器中，需要存数再保存到内存中。如果频繁使用一个变量，比如一个函数体内的多次循环每次都引用该局部变量，我们则可以把局部变量的值放到CPU的寄存器中，叫寄存器变量。不需要多次到内存中存取提高效率。\n\n3. 但是只能局部自动变量和形参可以做寄存器变量。在函数调用时占用一些寄存器，函数结束时释放。不同系统对register要求也不一样，比如对定义register变量个数，数据类型等限制，有的默认为自动变量处理。所以在程序一般也不用。\n\n4. register是不能取址的。比如 `int i`；(自动为auto)`int *p=&i;`是对的， 但`register int j; int *p = &j;`是错的，因为无法对寄存器的定址。\n\n   ```c\n   #include <stdio.h>\n   #include <time.h>\n   \n   #define TIME 1000000000\n   \n   int m, n = TIME; /* 全局变量 */\n   int main(void)\n   {\n       time_t start, stop;\n       register int a, b = TIME; /* 寄存器变量 */\n       int x, y = TIME;          /* 一般变量   */\n   \n       time(&start);\n       for (a = 0; a < b; a++);\n       time(&stop);\n       printf(\"寄存器变量用时: %d 秒\\n\", stop - start);\n       time(&start);\n       for (x = 0; x < y; x++);\n       time(&stop);\n       printf(\"一般变量用时: %d 秒\\n\", stop - start);\n       time(&start);\n       for (m = 0; m < n; m++);\n       time(&stop);\n       printf(\"全局变量用时: %d 秒\\n\", stop - start);\n   \n       return 0;\n   }\n   ```\n\n   \n\n### 5.5、volatile\n\n```c\n\t程序在使用变量时, 特别是连续多次使用变量时, 一般是载入寄存器, 直接从寄存器存取, 之后再还回内存;但如果此变量在返回内存时, 假如内存中的值已经改变了(从外部修改了)怎么办?\n为了避免这种情况的发生, 可以用 volatile 说明此变量, 以保证变量的每次使用都是直接从内存存取.\n但这样肯定会影响效率, 幸好它并不常用.\n\n另外: 如果 const volatile 同时使用, 这表示此变量只接受外部的修改.\n    \n#include <stdio.h>\n\nvolatile int num = 123;\nint main(void)\n{    \n    printf(\"%d\\n\", num);\n    getchar();\n    return 0;\n}\n```\n\n\n### 5.6、总结\n\n\n\n| 关键字       | 类型         | 生命周期           | 作用域           | 修饰对象   | 所属区                          |\n| ------------ | ------------ | ------------------ | ---------------- | ---------- | ------------------------------- |\n| auto[可省略] | 普通局部变量 | 定义到{}运行结束   | ｛｝             | 变量       | 栈区                            |\n| static       | 静态局部变量 | 定义到程序运行结束 | ｛｝             | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n|              | 全局变量     | 定义到程序运行结束 | 定义到文件结尾   |            | 初始化在data段，未初始化在BSS段 |\n| extern       | 全局变量     | 定义到程序运行结束 | 声明处到文件结尾 | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n| static       | 全局变量     | 整个程序运行期     | 声明处到文件结尾 | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n| register     | 寄存器变量   | 定义到{}运行结束   | ｛｝             | 变量       | 运行时存储在CPU寄存器           |\n| extern       | 函数         | 整个程序运行期     | 声明处到文件结尾 |            | 代码区                          |\n| static       | 函数         | 整个程序运行期     | 声明处到文件结尾 |            | 代码区                          |\n\n\n\n## 6、内存分区\n\nC代码经过预处理、编译、汇编、链接4步后生成一个可执行程序。在 Linux 下，程序是一个普通的可执行文件，以下列出一个二进制可执行文件的基本情况：\n\n![](01.png)\n\n通过上图可以得知，在没有运行程序前，也就是说程序没有加载到内存前，可执行程序内部已经分好3段信息，分别为代码区（text）、数据区（data）和未初始化数据区（bss）3 个部分（有些人直接把data和bss合起来叫做静态区或全局区）。\n\n- **代码区**\n\n  存放 CPU 执行的机器指令。通常代码区是**可共享**的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区通常是**只读的**，使其只读的原因是防止程序意外地修改了它的指令。另外，代码区还规划了局部变量的相关信息。\n\n- **全局初始化数据区/静态数据区（data段）**\n\n  该区包含了在程序中明确被初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。\n\n- **未初始化数据区（又叫 bss 区）**\n\n  存入的是全局未初始化变量和未初始化静态变量。未初始化数据区的数据在程序开始执行之前被内核初始化为 0 或者空（NULL）。\n\n程序在加载到内存前，代码区和全局区(data和bss)的大小就是固定的，程序运行期间不能改变。然后，运行可执行程序，系统把程序加载到内存，除了根据可执行程序的信息分出代码区（text）、数据区（data）和未初始化数据区（bss）之外，还额外增加了栈区、堆区。\n\n![](02.png)\n\n- 代码区（text segment）\n\n  加载的是可执行文件代码段，所有的可执行代码都加载到代码区，这块内存是不可以在运行期间修改的。\n  \n- 只读数据区（文字常量区 RO data）\n\n  只读数区是程序使用的一些不会被更改的数据。一般是const修饰的变量以及程序中使用的文字常量。\n\n- 已初始化数据区 （RW data）\n\n  加载的是可执行文件数据段，存储于数据段（全局初始化，静态初始化数据）的数据的生存周期为整个程序运行过程。\n\n- 未初始化数据区（BSS）\n\n  加载的是可执行文件BSS段，位置可以分开亦可以紧靠数据段，存储于数据段的数据（全局未初始化，静态未初始化数据）的生存周期为整个程序运行过程。\n  \n- 堆区（heap）\n\n  堆是一个大容器，它的容量要远远大于栈，但没有栈那样先进后出的顺序。用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收。\n\n- 栈区（stack）\n\n  栈是一种先进后出的内存结构，由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。\n\n \n\n## 7、结构体字节对齐\n\n\n\n**7.1、内存对齐原因**\n\n```c\nstruct data\n{\n\tchar c;\n\tint i;\n};\n\nint main()\n{\n\tprintf(\"%d\\n\", sizeof(struct data)); // 5还是 8?\n\n\treturn 0;\n}\n```\n\n\n\n在Linux 32位架构下，假设变量stu存放在内存中的起始地址为0x00，那么c的起始地址为0x00、i的起始地址为0x01，变量stu共占用了5个字节：\n\n- 对变量c访问：CPU只需要一个读周期\n\n- 变量i访问：\n\n1. 首先CPU用一个读周期，从0x00处读取了4个字节(32位架构)，然后将0x01-0x03的3个字节暂存。\n\n2. 再花一个读周期读取了从0x04-0x07的4字节数据，将0x04这个字节与刚刚暂存的3个字节进行拼接从而读取到成员变量i的值。\n\n3. 读取一个成员变量i，CPU却花费了2个读周期。\n\n \n\n如果数据成员i的起始地址被放在了0x04处\n\n- 读取c成员，花费周期为1  \n\n- 读取i所花费的周期也变成了1\n\n- 引入字节对齐可以避免读取效率的下降，同时也浪费了3个字节的空间(0x01-0x03)\n\n **结构体内部成员对齐是为了实现用空间换取时间。**\n\n**7.2、内存对齐原则**\n\n- 原则1：数据成员的对齐规则\n\n  1) 最大对齐单位以CPU架构对齐，如Linux 32位最大以4字节对齐，Linux 64位最大以8字节对齐，vs(32位、64位)最大对齐单位为8字节\n\n  2) 需要和**结构体的最大成员**和CPU架构(32位或64位)对比，取小的作为对齐单位\n\n  3) 字节对齐也可以通过程序控制，采用指令：\n\n  ```c\n  #pragma pack(xx)   \n  #pragma pack(1)     //1字节对齐\n  #pragma pack(2)     //2字节对齐\n  #pragma pack(4)     //4字节对齐\n  #pragma pack(8)     //8字节对齐\n  #pragma pack(16)    //16字节对齐\n  \n  \n  #include <stdio.h>\n  #include <stdlib.h>\n  \n  #pragma pack(2)\n  \n  typedef struct\n  {\n    int aa1; //4个字节对齐 1111\n    char bb1;//1个字节对齐 1\n    short cc1;//2个字节对齐 011\n    char dd1; //1个字节对齐 1\n    } testlength;\n  \n  int length = sizeof(testlength); //2个字节对齐，length = 10\n  \n  int main(){\n  \tprintf(\"length=%d\\n\", length); // length=10\n  }\n  \n  ```\n\n  尽管通过pragma pack(xx)可以指定字节对齐单位，但需要和结构体的最大成员、CPU架构(32位或64位)对比，取最小的作为对齐单位。\n\n- 原则2：数据成员的偏移起点\n\n  结构体（struct）的数据成员，第一个数据成员放在偏移量为0的地方，以后每个数据成员存放在偏移量为该数据成员类型大小的整数倍的地方（比如int在32位机器为４字节，则要从4的整数倍地址开始存储）\n\n- 原则3：收尾工作 \n\n  结构体的总大小，也就是sizeof的结果，必须是对齐单位的整数倍，不足的要补齐。\n\n \n\n## 8、typedef与define的区别\n\n- typedef为C语言的关键字，作用是为一种数据类型(基本类型或自定义数据类型)定义一个新名字，不能创建新类型。\n\n- 与#define不同，typedef仅限于数据类型，而不是能是表达式或具体的值\n\n- define发生在预处理，typedef发生在编译阶段\n\n\n\n## 9、C语言文本操作的区别\n\n### 9.1 二进制文件和文本文件\n\n\n\n- b是二进制模式的意思，b只是在Windows有效，在Linux用r和rb的结果是一样的\n\n- Unix和Linux下所有的文本文件行都是\\n结尾，而Windows所有的文本文件行都是\\r\\n结尾\n\n- 在Windows平台下，以“文本”方式打开文件，不加b：\n\n  - 当读取文件的时候，系统会将所有的 \"\\r\\n\" 转换成 \"\\n\"\n\n  - 当写入文件的时候，系统会将 \"\\n\" 转换成 \"\\r\\n\" 写入 \n\n  - 以\"二进制\"方式打开文件，则读\\写都不会进行这样的转换\n\n- 在Unix/Linux平台下，“文本”与“二进制”模式没有区别，\"\\r\\n\" 作为两个字符原样输入输出\n\n### 9.2 文本结尾\n\n在C语言中，EOF表示文件结束符(end of file)。在while循环中以EOF作为文件结束标志，这种以EOF作为文件结束标志的文件，必须是文本文件。在文本文件中，数据都是以字符的ASCII代码值的形式存放。我们知道，ASCII代码值的范围是0~127，不可能出现-1，因此可以用EOF作为文件结束标志。\n\n`#define EOF (-1)`\n\n当把数据以二进制形式存放到文件中时，就会有-1值的出现，因此不能采用EOF作为二进制文件的结束标志。为解决这一个问题，ANSI C提供一个feof函数，用来判断文件是否结束。feof函数既可用以判断二进制文件又可用以判断文本文件。\n\n```c\n#include <stdio.h>\nint feof(FILE * stream);\n功能：检测是否读取到了文件结尾。判断的是最后一次“读操作的内容”，不是当前位置内容(上一个内容)。\n参数：\n\tstream：文件指针\n返回值：\n\t非0值：已经到文件结尾\n\t0：没有到文件结尾\n```\n\n\n\n## 10、void\n\n**void的作用**\n\n- 对函数参数的限定：当不需要传入参数时，即 `function (void);`\n- 对函数返回值的限定：当函数没有返回值时，即 `void function(void);`\n\n**void指针的作用**\n\n（1）void指针可以指向任意的数据类型，即任意类型的指针可以赋值给void指针\n\n```c\nint *a;\nvoid *p;\np=a;\n```\n\n如果void指针赋值给其他类型，则需要强制转换；`a=（int *）p;`\n\n（2）在ANSI C标准中不允许对void指针进行算术运算，因为没有特定的数据类型，即在内存中不知道移动多少个字节；而在GNU标准中，认为void指针和char指针等同。\n\n**应用**\n\n（1）void指针一般用于应用的底层，比如malloc函数的返回类型是void指针，需要再强制转换； \n\n（2）文件句柄HANDLE也是void指针类型，这也是句柄和指针的区别； \n\n（3）内存操作函数的原型也需要void指针限定传入参数：\n\n```c\nvoid * memcpy (void *dest, const void *src, size_t len);\nvoid * memset (void *buffer, int c, size_t num );\n```\n\n（4）面向对象函数中底层对基类的抽象。\n\n## 11、数据类型的本质\n\n- 数据类型可理解为创建变量的模具：是固定内存大小的别名。\n\n- 数据类型的作用：编译器预算对象（变量）分配的内存空间大小。\n\n\n\n## 12、变量的本质\n\n变量的本质：一段连续内存空间的别名。\n\n\t1）程序通过变量来申请和命名内存空间 int a = 0\n\t\n\t2）通过变量名访问内存空间\n\t\n\t3）不是向变量读写数据，而是向变量所代表的内存空间中读写数据\n\n\n\n## 13、数组与指针的关系\n\n数组不是指针，数组名也只有在表达式中才会被当成一个指针常量。数组名在表达式中使用的时候，编译器才会产生一个指针常量。\n\n- `p[i]`这种写法只不过是`*(p + i)`的简便写法。实际上，至少对于编译器来说，[]这样的运算符完全可以不存在。[]运算符是为了方便人们读写而引入的，是一种语法糖。\n\n- 当数组名作为sizeof操作符的操作数的时候，此时sizeof返回的是整个数组的长度，而不是指针数组指针的长度。\n- 当数组名作为 & 操作符的操作数的时候，此时返回的是一个指向数组的指针，而不是指向某个数组元素的指针常量。\n\n- 二级指针是指向指针的指针，而指针数组则是元素类型为指针的数组。虽然它们是不一样的，但是在表达式中，它们是等效的。\n\n## 14、字节序(大端、小端)\n\n**14.1 大端和小端**\n\n\n\n计算机的内存最小单位是字节。字节序是指多字节(大于1字节)数据的存储顺序，在设计计算机系统的时候，有两种处理内存中数据的方法：大端格式、小端格式。\n\n- 小端格式(Little-Endian)：将低位字节数据存储在低地址。X86和ARM都是小端对齐。\n\n- 大端格式(Big-Endian)：将高位字节数据存储在低地址。很多Unix服务器的CPU是大端对齐的、网络上数据是以大端对齐。\n\n![](03.png)\n\n对于整形 0x12345678，它在大端格式和小端格式的系统中，分别如下图所示的方式存放：\n\n![](04.png)\n\n**14.2 网络字节序和主机字节序**\n\n网络字节顺序NBO(Network Byte Order)\n\n\t在网络上使用统一的大端模式，低字节存储在高地址，高字节存储在低地址。\n\n\n\n主机字节序顺序HBO(Host Byte Order)\n\n\t不同的机器HBO不相同，与CPU设计相关，数据的顺序是由CPU决定的，而与操作系统无关。\n\n\n\n处理器 |操作系统  |字节排序|\n\nAlpha    全部    Little endian\nHP-PA    NT    Little endian\nHP-PA    UNIX    Big endian\nIntelx86    全部    Little endian <-----x86系统是小端字节序系统\nMotorola680x()    全部    Big endian\nMIPS    NT    Little endian\nMIPS    UNIX    Big endian\nPowerPC    NT    Little endian\nPowerPC    非NT    Big endian  <-----PPC系统是大端字节序系统\nRS/6000    UNIX    Big endian\nSPARC    UNIX    Big endian\nIXP1200 ARM核心    全部    Little endian \n\n```c\n相关函数：\nhtons 把unsigned short类型从主机序转换到网络序\nhtonl 把unsigned long类型从主机序转换到网络序\nntohs 把unsigned short类型从网络序转换到主机序\nntohl 把unsigned long类型从网络序转换到主机序\n\n头文件：#include <netinet/in.h>\n定义函数：unsigned short ntohs(unsigned short netshort);\n函数说明：ntohs()用来将参数指定的16 位netshort 转换成主机字符顺序.\n返回值：返回对应的主机顺序.\n范例：参考getservent().\n```\n\n```c\n#include <stdio.h>\n#include <winsock.h> // windows使用winsock.h\n\nint main()\n{\n    //左边是高位，右边是低位，高位放高地址，低位放低地址\n    int a = 0x11223344;\n    unsigned char *p = (unsigned char *)&a;\n\n    int i;\n    for (i = 0; i < 4; i++)\n    {\n        printf(\"%x\\n\", p[i]);\n    }\n\n    u_long b = htonl(a);\n    unsigned char *q = (unsigned char *)&b;\n    for (i = 0; i<4; i++){\n        printf(\"%x\\n\", q[i]);\n    }\n\n    return 0;\n}\n\n//  gcc hello.c -lwsock32 -o hello\n// 编译时添加-lwsock32，不然会报错undefined reference to `htonl@4'\n// 在编译socket程序的时候，一定要加上-l wsock32选项，因为mingw默认没有包含windows库\n```\n\n## 15、数组指针\n\n```c\n// 1) 先定义数组类型，再根据类型定义指针变量\ntypedef int A[10];\nA *p = NULL;\n\n// 2) 先定义数组指针类型，根据类型定义指针变量\ntypedef int(*P)[10]; //第一个()代表指针，第二个[]代表数组\nP q; //数据组指针变量\n\n// 3) 直接定义数组指针变量\nint (*q)[10];\n\n```\n\n\n\n## 16、深拷贝和浅拷贝\n\n**结构体**:\n\n浅拷贝  不同结构体成员指针变量指向同一块内存\n\n深拷贝  不同结构体成员指针变量指向不同的内存\n\n\n\n**类**:\n\n浅拷贝 类中有动态分配的空间的指针指向相同\n\n深拷贝 类中有动态分配的空间的指针指向不同的内存空间\n\n\n\n## 17、\\#include< > 与 #include \"\"的区别\n\n- \"\" 表示系统先在file1.c所在的当前目录找file1.h，如果找不到，再按系统指定的目录检索。\n\n- < > 表示系统直接按系统指定的目录检索。\n\n \n\n注意：\n\n- #include <>常用于包含库函数的头文件\n- #include \"\"常用于包含自定义的头文件\n- 理论上#include可以包含任意格式的文件(.c .h等) ，但我们一般用于头文件的包含。\n\n\n\n## 18、静态库和动态库\n\n**18.1、静态库优缺点**\n\n- 静态库在程序的链接阶段被复制到了程序中，和程序运行的时候没有关系；\n\n- 程序在运行时与函数库再无瓜葛，移植方便；\n\n- 浪费空间和资源，所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。\n\n**18.2、动态库**\n\n要解决空间浪费和更新困难这两个问题，最简单的办法就是把程序的模块相互分割开来，形成独立的文件，而不是将他们静态的链接在一起。\n\n简单地讲，就是不对哪些组成程序的目标程序进行链接，等程序运行的时候才进行链接。也就是说，把整个链接过程推迟到了运行时再进行，这就是动态链接的基本思想。\n\n**18.3、动态库的lib文件和静态库的lib文件的区别**\n\n在使用动态库的时候，往往提供两个文件：一个引入库（.lib）文件（也称“导入库文件”）和一个DLL（.dll）文件。 \n\n虽然引入库的后缀名也是“lib”，但是，动态库的引入库文件和静态库文件有着本质的区别，对一个DLL文件来说，其引入库文件（.lib）包含该DLL导出的函数和变量的符号名，而.dll文件包含该DLL实际的函数和数据。\n\n在使用动态库的情况下，在编译链接可执行文件时，只需要链接该DLL的引入库文件，该DLL中的函数代码和数据并不复制到可执行文件，直到可执行程序运行时，才去加载所需的DLL，将该DLL映射到进程的地址空间中，然后访问DLL中导出的函数。\n\n\n\n# 二、C++篇\n\n## 1、c语言和c++语言的关系\n\n“c++”中的++来自于c语言中的递增运算符++，该运算符将变量加1。c++起初也叫”c with clsss”。通过名称表明，c++是对C的扩展，因此c++是c语言的超集，这意味着任何有效的c程序都是有效的c++程序。c++程序可以使用已有的c程序库。\n\nc++语言在c语言的基础上添加了**面向对象编程**和**泛型编程**的支持。c++继承了c语言高效，简洁，快速和可移植的传统。\n\nc++融合了3种不同的编程方式:\n\n- c语言代表的过程性语言.\n\n- c++在c语言基础上添加的类代表的面向对象语言.\n\n- c++模板支持的泛型编程。\n\n## 2、左值和右值\n\n判断是否是左值，有一个简单的方法，就是看看能否取它的地址，能取地址就是左值，否则就是右值。\n\n当一个对象成为右值时，使用的是它的值(内容), 而成为左值时，使用的是它的身份（在内存中的位置）。\n\n平常所说的引用，实际上指的就是左值引用`lvalue reference`, 常用单个&来表示。左值引用只能接收左值，不能接收右值。**const关键字会让左值引用变得不同，它可以接收右值。**\n\n\n\n**为了支持移动操作，在c++11版本，增加了右值引用。**右值引用一般用于绑定到一个即将销毁的对象，所以右值引用又通常出现在移动构造函数中。\n\n看完下面的例子，左值和右值基本就清楚了，左值具有持久的状态，有独立的内存空间，右值要么是字面常量，要么就是表达式求值过程中创建的临时对象\n\n\n\n```c++\nint i = 66;\nint &r = i ; //r 是一个左引用，绑定左值 i\n\nint &&rr = i ; //rr是一个右引用，绑定到左值i , 错误！\nint &r2 = i*42 ; //  r2 是一个左引用， 而i*42是一个表达式，计算出来的结果是一个右值。 错误！\n\nconst int &r3 = i*42; // const修饰的左值引用 正确\nint &&rr2 = i*42 ; // 右引用，绑定右值 正确\n```\n\n## 3、c++对c的扩展\n\n**3.1 三目运算符**\n\nc语言三目运算表达式返回值为数据值，为右值，不能赋值。\n\nc++语言三目运算表达式返回值为变量本身(引用)，为左值，可以赋值。\n\n```c++\nint a = 10;\nint b = 20;\nprintf(\"ret:%d\\n\", a > b ? a : b);\n//思考一个问题，(a > b ? a : b) 三目运算表达式返回的是什么？\n\ncout << \"b:\" << b << endl;\n//返回的是左值，变量的引用\n(a > b ? a : b) = 100;//返回的是左值，变量的引用\ncout << \"b:\" << b << endl;\n```\n\n**3.2 bool**\n\nc++中新增bool类型关键字\n\n- bool类型只有两个值，true(1)， false（0）\n- bool类型占1个字节\n- 给bool类型赋值时, 非0值会自动转换为true(1), 0值会自动转换为false（0）\n\nC语言中也有bool类型，在c99标准之前是没有bool关键字，c99标准已经有bool类型，包含头文件`stdbool.h`,就可以使用和c++一样的bool类型。\n\n**3.3 struct类型增强**\n\n- c中定义结构体变量需要加上struct关键字，c++不需要。\n\n- c中的结构体只能定义成员变量，不能定义成员函数。c++即可以定义成员变量，也可以定义成员函数。\n\n\n\n**3.4 更严格的类型转换**\n\n在C++中，不同类型的变量一般是不能直接赋值的，需要相应的强转。\n\n在C++中，所有的变量和函数都必须有类型\n\n**3.4 全局变量检测增强**\n\n```c++\nint a = 10; //赋值，当做定义\nint a; //没有赋值，当做声明\n\nint main(){\n\tprintf(\"a:%d\\n\",a);\n\treturn EXIT_SUCCESS;\n}\n\n// 上面的代码在c++下编译失败，在c下编译通过。\n```\n\n\n\n## 4、内部连接和外部连接\n\n内部连接：如果一个名称对编译单元(.cpp)来说是局部的，在链接的时候其他的编译单元无法链接到它且不会与其它编译单元(.cpp)中的同样的名称相冲突。例如static函数，inline函数等（注 : 用static修饰的函数，本限定在本源码文件中，不能被本源码文件以外的代码文件调用。而普通的函数，默认是extern的，也就是说，可以被其它代码文件调用该函数。）\n\n外部连接：如果一个名称对编译单元(.cpp)来说不是局部的，而在链接的时候其他的编译单元可以访问它，也就是说它可以和别的编译单元交互。 例如全局变量就是外部链接 。\n\n\n\n## 5、C/C++中const的区别\n\n1、const全局变量\n\nc语言全局const会被存储到只读数据区。c++中全局const当声明extern或者对变量取地址时，编译器会分配存储地址，变量存储在只读数据段。两个都受到了只读数据区的保护，不可修改。\n\n```c++\nconst int constA = 10;\nint main(){\n    int* p = (int*)&constA;\n    *p = 200;\n}\n\n// 以上代码在c/c++中编译通过，在运行期，修改constA的值时，发生写入错误。原因是修改只读数据段的数据。\n```\n\n2、const 局部变量\n\nc语言中局部const存储在堆栈区，只是不能通过变量直接修改const只读变量的值，但是可以跳过编译器的检查，通过指针间接修改const值。\n\n```c++\nconst int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\nprintf(\"constA:%d\\n\",constA);\nprintf(\"*p:%d\\n\", *p);\n\n// constA:300\n// *p:300\n```\n\nc++中对于局部的const变量要区别对待：\n\n- 对于基础数据类型，也就是const int a = 10这种，编译器会把它放到符号表中，不分配内存，当对其取地址时，会分配内存。\n\n```c++\nconst int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:10\n// *p:300\n// constA在符号表中，当我们对constA取地址，这个时候为constA分配了新的空间，*p操作的是分配的空间，而constA是从符号表获得的值。\n```\n\n​\t\n\n- 对于基础数据类型，如果用一个变量初始化const变量，如果const int a = b,那么也是会给a分配内存。\n\n```c++\nint b = 10;\nconst int constA = b;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:300\n// *p:300 \n```\n\n- 对于自定数据类型，比如类对象，那么也会分配内存。\n\n```c++\nconst Person person; //未初始化age\n//person.age = 50; //不可修改\nPerson* pPerson = (Person*)&person;\n//指针间接修改\npPerson->age = 100;\ncout << \"pPerson->age:\" << pPerson->age << endl;\npPerson->age = 200;\ncout << \"pPerson->age:\" << pPerson->age << endl;\n\n// pPerson->age:100\n// pPerson->age:200\n//为person分配了内存，所以我们可以通过指针的间接赋值修改person对象。\n```\n\n3、链接方式\n\nc中const默认为外部连接，c++中const默认为内部连接.当c语言两个文件中都有const int a的时候，编译器会报重定义的错误。而在c++中，则不会，因为c++中的const默认是内部连接的。如果想让c++中的const具有外部连接，必须显示声明为: extern const int a = 10;\n\n\n\n## 6、const与#define的区别\n\n- const有数据类型，可进行编译器类型安全检查。#define无类型，不可以进行类型检查\n\n- const有作用域，而#define不重视作用域，默认定义处到文件结尾。如果想定义在指定作用域下有效的常量，那么#define就不能用。\n\n  \n\n## 7、引用\n\n1. &在此不是求地址运算，而是起标识作用。\n\n2. 类型标识符是指目标变量的类型\n\n3. 必须在声明引用变量时进行初始化。\n\n4. 引用初始化之后不能改变。\n\n5. 不能有NULL引用。必须确保引用是和一块合法的存储单元关联。\n\n6. **可以建立对数组的引用。**\n7. 函数不能返回局部变量的引用\n8. 函数当左值，必须返回引用\n\n\n\n**引用的本质**\n\n引用的本质是在c++内部实现一个指针常量\n\n`Type& ref = val;  // Type* const ref = val;`\n\n\n\nc++编译器在编译过程中使用常指针作为引用的内部实现，因此引用所占用的空间大小与指针相同，只是这个过程是编译器内部实现，用户不可见。\n\n```c++\n//发现是引用，转换为 int* const ref = &a;\nvoid testFunc(int& ref){\n\tref = 100; // ref是引用，转换为*ref = 100\n}\nint main(){\n\tint a = 10;\n\tint& aRef = a; //自动转换为 int* const aRef = &a;这也能说明引用为什么必须初始化\n\taRef = 20; //内部发现aRef是引用，自动帮我们转换为: *aRef = 20;\n\tcout << \"a:\" << a << endl;\n\tcout << \"aRef:\" << aRef << endl;\n\ttestFunc(a);\n\treturn EXIT_SUCCESS;\n}\n\n```\n\n\n\n## 8、面向对象的三大特性\n\n**封装**\n\n1. 把变量（属性）和函数（操作）合成一个整体，封装在一个类中\n2. 对变量和函数进行访问控制\n\n**继承**\n\n\tc++最重要的特征是代码重用，通过继承机制可以利用已有的数据类型来定义新的数据类型，新的类不仅拥有旧类的成员，还拥有新定义的成员。\n\t\n\t派生类继承基类，派生类拥有基类中全部成员变量和成员方法（除了构造和析构之外的成员方法），但是在派生类中，继承的成员并不一定能直接访问，不同的继承方式会导致不同的访问权限。\n\t\n\t任何时候重新定义基类中的一个重载函数，在新类中所有的其他版本将被自动隐藏.\n\t\n\toperator=也不能被继承，因为它完成类似构造函数的行为。\n\n**多态**\n\n\tc++支持编译时多态(静态多态)和运行时多态(动态多态)，运算符重载和函数重载就是编译时多态，而派生类和虚函数实现运行时多态。\n\t\n\t静态多态和动态多态的区别就是函数地址是早绑定(静态联编)还是晚绑定(动态联编)。如果函数的调用，在编译阶段就可以确定函数的调用地址，并产生代码，就是静态多态(编译时多态)，就是说地址是早绑定的。而如果函数的调用地址不能编译不能在编译期间确定，而需要在运行时才能决定，这这就属于晚绑定(动态多态,运行时多态)。\n\t\n\t多态性改善了代码的可读性和组织性，同时也使创建的程序具有可扩展性。\n\n## 9、C++编译器优化技术：RVO/NRVO和复制省略\n\n现代编译器缺省会使用RVO（return value optimization，返回值优化）、NRVO（named return value optimization、命名返回值优化）和复制省略（Copy elision）技术，来减少拷贝次数来提升代码的运行效率\n\n注1：vc6、vs没有提供编译选项来关闭该优化，无论是debug还是release都会进行RVO和复制省略优化\n\n注2：vc6、vs2005以下及vs2005+ Debug上不支持NRVO优化，vs2005+ Release支持NRVO优化\n\n注3：g++支持这三种优化，并且可通过编译选项：-fno-elide-constructors来关闭优化\n\n**RVO**\n\n```c++\n#include <stdio.h>\nclass A\n{\npublic:\n    A()\n    {\n        printf(\"%p construct\\n\", this);\n    }\n    A(const A& cp)\n    {\n        printf(\"%p copy construct\\n\", this);\n    }\n    ~A() \n    {\n        printf(\"%p destruct\\n\", this);\n    }\n};\n\nA GetA()\n{\n    return A();\n}\n\nint main()\n{\n    {\n        A a = GetA();\n    }\n\n    return 0;\n}\n```\n\n在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct\n```\n\n在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下：\n\n```\n0x7ffc46947d4f construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量temp\n0x7ffc46947d7f copy construct // 函数GetA return语句处，把临时变量temp做为参数传入并调用拷贝构造函数A(const A& cp)将返回值ret构造出来\n0x7ffc46947d4f destruct // 函数GetA执行完return语句后，临时变量temp生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e copy construct // 函数GetA调用结束，返回上层main函数后，把返回值变量ret做为参数传入并调用拷贝构造函数A(const A& cp)将变量A a构造出来\n0x7ffc46947d7f destruct // A a = GetA()语句结束后，返回值ret生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n注：临时变量temp、返回值ret均为匿名变量\n\n**NRVO**\n\ng++编译器、vs2005+ Release（开启/O2及以上优化开关）\n\n修改上述代码，将GetA的实现修改成：\n\n```c++\nA GetA()\n{\n    A o;\n    return o;\n}\n```\n\n在g++、vs2005+ Release中，上述代码也仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct\n```\n\ng++加上-fno-elide-constructors选项关闭优化后，和上述结果一样\n\n```\n0x7ffc46947d4f construct\n0x7ffc46947d7f copy construct\n0x7ffc46947d4f destruct\n0x7ffc46947d7e copy construct\n0x7ffc46947d7f destruct\n0x7ffc46947d7e destruct\n```\n\n但在vc6、vs2005以下、vs2005+ Debug中，没有进行NRVO优化，输出结果为：\n\n```\n18fec4 construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量o\n18ff44 copy construct  // 函数GetA return语句处，把临时变量o做为参数传入并调用拷贝构造函数A(const A& cp)将返回值ret构造出来\n18fec4 destruct  // 函数GetA执行完return语句后，临时变量o生命周期结束，调用其析构函数~A()\n18ff44 destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n注：**与g++、vs2005+ Release相比，vc6、vs2005以下、vs2005+ Debug只优化掉了返回值到变量a的拷贝，命名局部变量o没有被优化掉，所以最后一共有2次构造和析构的调用**\n\n**复制省略**\n\n典型情况是：调用构造函数进行值类型传参\n\n```\nvoid Func(A a) \n{\n}\n\nint main()\n{\n    {\n        Func(A());\n    }\n\n    return 0;\n}\n```\n\n在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffeb5148d0f construct\n0x7ffeb5148d0f destruct\n```\n\n在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下： \n\n```\n0x7ffc53c141ef construct   // 在main函数中，调用无参构造函数构造实参变量o\n0x7ffc53c141ee copy construct // 调用Func函数后，将实参变量o做为参数传入并调用拷贝构造函数A(const A& cp)将形参变量a构造出来\n0x7ffc53c141ee destruct // 函数Func执行完后，形参变量a生命周期结束，调用其析构函数~A()\n0x7ffc53c141ef destruct // 返回main函数后，实参变量o要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n\n\n**优化失效的情况**\n\n1. 根据不同的条件分支，返回不同变量\n2. 返回参数变量\n3. 返回全局变量\n4. 返回复合函数类型中的成员变量\n5. 返回值赋值给已构造好的变量（此时会调用operator==赋值运算符）\n\nhttps://www.cnblogs.com/kekec/p/11303391.html\n\n## 10、explicit关键字\n\n- explicit用于修饰构造函数,防止隐式转化。\n\n- 是针对单参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造)而言。\n\n```c++\nclass MyString{\npublic:\n\texplicit MyString(int n){\n\t\tcout << \"MyString(int n)!\" << endl;\n\t}\n\tMyString(const char* str){\n\t\tcout << \"MyString(const char* str)\" << endl;\n\t}\n};\n\nint main(){\n\n\t//给字符串赋值？还是初始化？\n\t//MyString str1 = 1; \n\tMyString str2(10);\n\n\t//寓意非常明确，给字符串赋值\n\tMyString str3 = \"abcd\";\n\tMyString str4(\"abcd\");\n\n\treturn EXIT_SUCCESS;\n}\n\n```\n\n\n\n## 11、new/delete/malloc/free\n\n**11.1、new**\n\n1. 内存申请成功后，会返回一个指向该内存的地址。\n2. 若内存申请失败，则抛出异常，\n3. 申请成功后，如果是程序员定义的类型，会执行相应的构造函数\n\n**11.2、delete**\n\n1. 如果指针的值是0 ，delete不会执行任何操作，有检测机制\n2. delete只是释放内存，不会修改指针，指针仍然会指向原来的地址\n3. 重复delete，有可能出现异常\n4. 如果是自定义类型，会执行析构函数\n\n**11.3、malloc**\n\n1. malloc 申请成功之后，返回的是void类型的指针。需要将void*指针转换成我们需要的类型。1.\n2. malloc 要求制定申请的内存大小 ， 而new由编译器自行计算。\n3. 申请失败，返回的是NULL ， 比如： 内存不足。\n4. 不会执行自定义类型的构造函数\n\n\n\n**11.4、free**\n\n1. 如果是空指针，多次释放没有问题，非空指针，重复释放有问题\n2. 不会执行对应的析构\n3. delete的底层执行的是free\n\n \n\n## 12、const\n\n1. const修饰静态成员变量时可以在类内部初始化\n2. const 修饰成员函数时，修饰的是this指针，所以成员函数内不可以修改任何普通成员变量，当成员变量类型符前用mutable修饰时例外\n3. 常对象(cons修饰的对象)只能调用const修饰的成员函数\n4. 常对象可以访问成员属性，但是不能修改\n5. **const关键字会让左值引用变得不同，它可以接收右值。**\n\n\n\n## 13、虚继承\n\n虚继承是解决C++多重继承问题的一种手段，从不同途径继承来的同一基类，会在子类中存在多份拷贝。这将存在两个问题：其一，浪费存储空间；第二，存在二义性问题，多重继承可能存在一个基类的多份拷贝，这就出现了二义性。\n\n```c++\nclass BigBase{\npublic:\n\tBigBase(){ mParam = 0; }\n\tvoid func(){ cout << \"BigBase::func\" << endl; }\npublic:\n\tint mParam;\n};\n\nclass Base1 : public BigBase{};\nclass Base2 : public BigBase{};\nclass Derived : public Base1, public Base2{};\n\nint main(){\n\n\tDerived derived;\n\t//1. 对“func”的访问不明确\n\t//derived.func();\n\t//cout << derived.mParam << endl;\n\tcout << \"derived.Base1::mParam:\" << derived.Base1::mParam << endl;\n\tcout << \"derived.Base2::mParam:\" << derived.Base2::mParam << endl;\n\n\t//2. 重复继承\n\tcout << \"Derived size:\" << sizeof(Derived) << endl; //8\n\n\treturn EXIT_SUCCESS;\n}\n```\n\n\n\n虚继承可以解决多种继承前面提到的两个问题：\n\n虚继承底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，64位8字节/windows 4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类指针依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，**虚基类指针也会被继承。**\n\n实际上，`vbptr`指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。\n\n![](05.png)\n\n```c++\n#include <iostream>\n\nusing namespace std;\n\n\nclass A  //大小为4\n{\npublic:\n    int a;\n};\n\nclass B:virtual public A{ // vbptr 8, int b 4, int a 4 = 12\npublic:\n    int b;\n};\n\nclass C:virtual public A{// vbptr 8, int c 4, int a 4 = 12\npublic:\n    int c;\n};\n\nclass D:public B, public C{\n    // int a, b, c, d=16\n    // class B  vbptr 4\n    // class C  vbptr 4  = 24\npublic:\n    int d;\n};\n\nint main()\n{\n    A a;\n    B b;\n    C c;\n    D d;\n    cout << sizeof(a) << endl; // 4 \n    cout << sizeof(b) << endl; // 16\n    cout << sizeof(c) << endl; // 16\n    cout << sizeof(d) << endl; // 24\n    return 0;\n}\n```\n\n链接:https://blog.csdn.net/bxw1992/article/details/77726390\n\n## 14、虚函数\n\n**问题**\n\n\t父类引用或指针可以指向子类对象，通过父类指针或引用来操作子类对象。但是由于编译阶段编译器根据对象的指针或者引用选择函数调用，所以会调用父类的函数。\n\t\n\t解决问题的方法是迟绑定(动态绑定)，在运行时根据对象的实际类型决定。\n\n**解决**\n\n\tC++动态多态性是通过虚函数来实现的，虚函数允许子类（派生类）重新定义父类（基类）成员函数，而子类（派生类）重新定义父类（基类）虚函数的做法称为覆盖(override)，或者称为重写。\n\t\n\t对于特定的函数进行动态绑定，c++要求在基类中声明这个函数的时候使用virtual关键字,动态绑定也就对virtual函数起作用.\n\n- 为创建一个需要动态绑定的虚成员函数，可以简单在这个函数声明前面加上virtual关键字，定义时候不需要.\n\n- 如果一个函数在基类中被声明为virtual，那么在所有派生类中它都是virtual的.\n\n- 在派生类中virtual函数的重定义称为重写(override).\n\n- Virtual关键字只能修饰成员函数.\n\n- 构造函数不能为虚函数\n\n**虚函数原理**\n\n首先，我们看看编译器如何处理虚函数。当编译器发现我们的类中有虚函数的时候，编译器会创建一张虚函数表（virtual function table ），表中存储着类对象的虚函数地址，并且给类增加一个指针，这个指针就是`vpointer`(缩写`vptr`)，这个指针是指向虚函数表。\n\n父类对象包含的指针，指向父类的虚函数表地址，子类对象包含的指针，指向子类的虚函数表地址。\n\n如果子类重新定义了父类的函数，那么函数表中存放的是新的地址，如果子类没有重新定义，那么表中存放的是父类的函数地址。如果子类有自己的虚函数，则只需要添加到表中即可。\n\n```c++\nclass A{\npublic:\n    virtual void func1(){}\n    virtual void func2(){}\n};\n\n//B类为空，那么大小应该是1字节，实际情况是这样吗？\nclass B : public A{};\n\nvoid test(){\n    cout << \"A size:\" << sizeof(A) << endl; // win指针字节为4, linux 64 字节为8\n    cout << \"B size:\" << sizeof(B) << endl; // 4\n}\n```\n\n**多态的成立条件：**\n\n- 有继承\n\n- 子类重写父类虚函数函数\n\n      a) 返回值，函数名字，函数参数，必须和父类完全一致(析构函数除外) \n        \n      b) 子类中virtual关键字可写可不写，建议写\n\n- 类型兼容，父类指针，父类引用指向子类对象\n\n\n\n**抽象类和纯虚函数**\n\n当基类中有至少一个纯虚函数则为抽象类\n\n- 纯虚函数使用关键字virtual，并在其后面加上=0。如果试图去实例化一个抽象类，编译器则会阻止这种操作。\n\n- 当继承一个抽象类的时候，必须实现所有的纯虚函数，否则由抽象类派生的类也是一个抽象类。\n\n- Virtual void fun() = 0;告诉编译器在vtable中为函数保留一个位置，但在这个特定位置不放地址。\n\n**接口类**\n\n接口类中只有函数原型定义，没有任何数据定义。多重继承接口不会带来二义性和复杂性问题。接口类只是一个功能声明，并不是功能实现，子类需要根据功能说明定义功能实现。\n\n注意:除了析构函数外，其他声明都是纯虚函数。\n\n**虚析构函数**\n\n虚析构函数是为了解决[基类]的[指针]指向派生类对象，并用基类的指针删除派生类对象。\n\n**虚函数和虚继承的异同**\n\n在这里我们可以对比虚函数的实现原理：他们有相似之处，都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间）。\n\n虚基类指针依旧存在继承类中，只占用存储空间；基类虚函数指针不存在于子类中，不占用存储空间。\n\n虚基类表存储的是虚基类相对直接继承类的偏移；而虚函数表存储的是虚函数地址。\n\n## 15、函数模板的机制\n\n**函数模板机制结论：**\n\n- 编译器并不是把函数模板处理成能够处理任何类型的函数\n\n- 函数模板通过具体类型产生不同的函数\n\n- 编译器会对函数模板进行两次编译，在声明的地方对模板代码本身进行编译，在调用的地方对参数替换后的代码进行编译。\n\n**局限性**:\n\n编写的模板函数很可能无法处理某些类型，另一方面，有时候通用化是有意义的，但C++语法不允许这样做。为了解决这种问题，可以提供**模板的重载**，为这些特定的类型提供具体化的模板。\n\n```c++\nclass Person\n{\npublic:\n    Person(string name, int age)\n    {\n        this->mName = name;\n        this->mAge = age;\n    }\n    string mName;\n    int mAge;\n};\n\n\n//普通交换函数\ntemplate <class T>\nvoid mySwap(T &a,T &b)\n{\n    T temp = a;\n    a = b;\n    b = temp;\n}\n\n\n//第三代具体化，显示具体化的原型和定意思以template<>开头，并通过名称来指出类型\n//具体化优先于常规模板\ntemplate<>void mySwap<Person>(Person &p1, Person &p2)\n{\n    string nameTemp;\n    int ageTemp;\n\n    nameTemp = p1.mName;\n    p1.mName = p2.mName;\n    p2.mName = nameTemp;\n\n    ageTemp = p1.mAge;\n    p1.mAge = p2.mAge;\n    p2.mAge = ageTemp;\n\n}\n\nvoid test()\n{\n    Person P1(\"Tom\", 10);\n    Person P2(\"Jerry\", 20);\n\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n    mySwap(P1, P2);\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n}\n```\n\n## 16、C++类型转换\n\n**静态类型转换static_cast**\n\n- 用于[类层次结构](http://baike.baidu.com/view/2405425.htm)中基类（父类）和[派生类](http://baike.baidu.com/view/535532.htm)（子类）之间指针或引用的转换。\n\n  - 进行上行转换（把派生类的指针或引用转换成基类表示）是安全的；\n\n  - 进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。\n\n- 用于基本数据类型之间的转换，如把int转换成char，把char转换成int。这种转换的安全性也要开发人员来保证。\n\n**动态类型转换(dynamic_cast)**\n\n- dynamic_cast主要用于类层次间的上行转换和下行转换；\n\n- 在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；\n\n- 在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全；\n\n**常量转换(const_cast)**\n\n该运算符用来修改类型的const属性。。\n\n- 常量指针被转化成非常量指针，并且仍然指向原来的对象；\n\n- 常量引用被转换成非常量引用，并且仍然指向原来的对象；\n\n \n\n\n\n**重新解释转换(reinterpret_cast)**\n\n这是最不安全的一种转换机制，最有可能出问题。\n\n主要用于将一种数据类型从一种类型转换为另一种类型。它可以将一个指针转换成一个整数，也可以将一个整数转换成一个指针.\n\n\n\n# 三、STL\n\n## 1、STL六大组件简介\n\nSTL提供了六大组件，彼此之间可以组合套用，这六大组件分别是:容器、算法、迭代器、仿函数、适配器（配接器）、空间配置器。\n\n**容器：**各种数据结构，如vector、list、deque、set、map等, 用来存放数据，从实现角度来看，STL容器是一种class template。\n\n**算法：**各种常用的算法，如sort、find、copy、for_each。从实现的角度来看，STL算法是一种function tempalte.\n\n**迭代器：**扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，**迭代器是一种将operator* , operator-> , operator++,operator--等指针相关操作予以重载的class template.** 所有STL容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。原生指针(native pointer)也是一种迭代器。\n\n**仿函数：**行为类似函数，可作为算法的某种策略。从实现角度来看，仿函数是一种重载了operator()的class 或者class template\n\n**适配器：**一种用来修饰容器或者仿函数或迭代器接口的东西。\n\n**空间配置器：**负责空间的配置与管理。从实现角度看，配置器是一个实现了动态空间配置、空间管理、空间释放的class tempalte.\n\n## 2、string容器\n\nC风格字符串(以空字符结尾的字符数组)太过复杂难于掌握，不适合大程序的开发，所以C++标准库定义了一种string类，定义在头文件<string>。\n\nString和c风格字符串对比：\n\n- char*是一个指针，String是一个类\n\n  string封装了char\\*，管理这个字符串，是一个char*型的容器。\n\n- String封装了很多实用的成员方法\n\n  查找find，拷贝copy，删除delete 替换replace，插入insert\n\n- 不用考虑内存释放和越界\n\n   string管理char*所分配的内存。每一次string的复制，取值都由string类负责维护，不用担心复制越界和取值越界等。\n\n## 3、vector容器\n\nvector的数据安排以及操作方式，与array非常相似，两者的唯一差别在于空间的运用的灵活性。Array是静态空间，一旦配置了就不能改变，要换大一点或者小一点的空间，可以，一切琐碎得由自己来，首先配置一块新的空间，然后将旧空间的数据搬往新空间，再释放原来的空间。Vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新元素。因此vector的运用对于内存的合理利用与运用的灵活性有很大的帮助，我们再也不必害怕空间不足而一开始就要求一个大块头的array了。\n\n所谓动态增加大小，并不是在原空间之后续接新空间(因为无法保证原空间之后尚有可配置的空间)，而是一块更大的内存空间，然后将原数据拷贝新空间，并释放原空间。因此，对vector的任何操作，一旦引起空间的重新配置，指向原vector的所有迭代器就都失效了。这是程序员容易犯的一个错误，务必小心。\n\n为了降低空间配置时的速度成本，vector实际配置的大小可能比客户端需求大一些，以备将来可能的扩充，这边是**容量**的概念。换句话说，**一个vector的容量永远大于或等于其大小，一旦容量等于大小，便是满载，下次再有新增元素，整个vector容器就得另觅居所。**\n\n## 4、deque容器\n\nVector容器是单向开口的连续内存空间，deque则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector容器也可以在头尾两端插入元素，但是在其头部操作效率奇差，无法被接受。\n\nDeque容器和vector容器最大的差异，一在于deque允许使用常数项时间对头端进行元素的插入和删除操作。二在于deque没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，换句话说，像vector那样，”旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque身上是不会发生的。也因此，deque没有必须要提供所谓的空间保留(reserve)功能.\n\n虽然deque容器也提供了Random Access Iterator,但是它的迭代器并不是普通的指针，其复杂度和vector不是一个量级，这当然影响各个运算的层面。因此，除非有必要，我们应该尽可能的使用vector，而不是deque。对deque进行的排序操作，为了最高效率，可将deque先完整的复制到一个vector中，对vector容器进行排序，再复制回deque.\n\n既然deque是分段连续内存空间，那么就必须有中央控制，维持整体连续的假象，数据结构的设计及迭代器的前进后退操作颇为繁琐。Deque代码的实现远比vector或list都多得多。\n\nDeque采取一块所谓的map(注意，不是STL的map容器)作为主控，这里所谓的map是一小块连续的内存空间，其中每一个元素(此处成为一个结点)都是一个指针，指向另一段连续性内存空间，称作缓冲区。缓冲区才是deque的存储空间的主体。\n\n![](06.png)\n\n\n\n## 5、stack容器\n\nstack是一种先进后出(First In Last Out,FILO)的数据结构，它只有一个出口，形式如图所示。stack容器允许新增元素，移除元素，取得栈顶元素，但是除了最顶端外，没有任何其他方法可以存取stack的其他元素。换言之，stack不允许有遍历行为。\n\n有元素推入栈的操作称为:push,将元素推出stack的操作称为pop.\n\nStack所有元素的进出都必须符合”先进后出”的条件，只有stack顶端的元素，才有机会被外界取用。Stack不提供遍历功能，也不提供迭代器。\n\n\n\n## 6、queue容器\n\nQueue是一种先进先出(First In First Out,FIFO)的数据结构，它有两个出口，queue容器允许从一端新增元素，从另一端移除元素。\n\nQueue所有元素的进出都必须符合”先进先出”的条件，只有queue的顶端元素，才有机会被外界取用。Queue不提供遍历功能，也不提供迭代器。\n\n## 7、list容器\n\n链表是一种物理[存储单元](http://baike.baidu.com/view/1223079.htm)上非连续、非顺序的[存储结构](http://baike.baidu.com/view/2820182.htm)，[数据元素](http://baike.baidu.com/view/38785.htm)的逻辑顺序是通过链表中的[指针](http://baike.baidu.com/view/159417.htm)链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储[数据元素](http://baike.baidu.com/view/38785.htm)的数据域，另一个是存储下一个结点地址的[指针](http://baike.baidu.com/view/159417.htm)域。\n\n相较于vector的连续线性空间，list就显得负责许多，它的好处是每次插入或者删除一个元素，就是配置或者释放一个元素的空间。因此，list对于空间的运用有绝对的精准，一点也不浪费。而且，对于任何位置的元素插入或元素的移除，list永远是常数时间。\n\nList和vector是两个最常被使用的容器。\n\nList容器是一个双向链表。\n\n- 采用动态存储分配，不会造成内存浪费和溢出\n\n- 链表执行插入和删除操作十分方便，修改指针即可，不需要移动大量元素\n\n- 链表灵活，但是空间和时间额外耗费较大\n\nList有一个重要的性质，插入操作和删除操作都不会造成原有list迭代器的失效。这在vector是不成立的，因为vector的插入操作可能造成记忆体重新配置，导致原有的迭代器全部失效，甚至List元素的删除，也只有被删除的那个元素的迭代器失效，其他迭代器不受任何影响。\n\n\n\n## 8、set/multiset容器\n\nSet的特性是。所有元素都会根据元素的键值自动被排序。Set的元素不像map那样可以同时拥有实值和键值，set的元素即是键值又是实值。Set不允许两个元素有相同的键值。\n\n我们可以通过set的迭代器改变set元素的值吗？不行，因为set元素值就是其键值，关系到set元素的排序规则。如果任意改变set元素值，会严重破坏set组织。换句话说，set的iterator是一种const_iterator.\n\nset拥有和list某些相同的性质，当对容器中的元素进行插入操作或者删除操作的时候，操作之前所有的迭代器，在操作完成之后依然有效，被删除的那个元素的迭代器必然是一个例外。\n\n\n\n## 9、map/multimap容器\n\nMap的特性是，所有元素都会根据元素的键值自动排序。Map所有的元素都是pair,同时拥有实值和键值，pair的第一元素被视为键值，第二元素被视为实值，map不允许两个元素有相同的键值。\n\n我们可以通过map的迭代器改变map的键值吗？答案是不行，因为map的键值关系到map元素的排列规则，任意改变map键值将会严重破坏map组织。如果想要修改元素的实值，那么是可以的。\n\nMap和list拥有相同的某些性质，当对它的容器元素进行新增操作或者删除操作时，操作之前的所有迭代器，在操作完成之后依然有效，当然被删除的那个元素的迭代器必然是个例外。\n\nMultimap和map的操作类似，唯一区别multimap键值可重复。\n\nMap和multimap都是以红黑树为底层实现机制。\n\n## 10、STL容器使用时机\n\n|              | vector   | deque    | list     | set    | multiset | map             | multimap      |\n| ------------ | -------- | -------- | -------- | ------ | -------- | --------------- | ------------- |\n| 典型内存结构 | 单端数组 | 双端数组 | 双向链表 | 二叉树 | 二叉树   | 二叉树          | 二叉树        |\n| 可随机存取   | 是       | 是       | 否       | 否     | 否       | 对key而言：不是 | 否            |\n| 元素搜寻速度 | 慢       | 慢       | 非常慢   | 快     | 快       | 对key而言：快   | 对key而言：快 |\n| 元素安插移除 | 尾端     | 头尾两端 | 任何位置 | -      | -        | -               | -             |\n\n- vector的使用场景：比如软件历史操作记录的存储，我们经常要查看历史记录，比如上一次的记录，上上次的记录，但却不会去删除记录，因为记录是事实的描述。\n\n- deque的使用场景：比如排队购票系统，对排队者的存储可以采用deque，支持头端的快速移除，尾端的快速添加。如果采用vector，则头端移除时，会移动大量的数据，速度慢。\n\n  ​     vector与deque的比较：\n\n  ​      一：vector.at()比deque.at()效率高，比如vector.at(0)是固定的，deque的开始位置却是不固定的。\n\n  ​    二：如果有大量释放操作的话，vector花的时间更少，这跟二者的内部实现有关。\n\n  ​    三：deque支持头部的快速插入与快速移除，这是deque的优点。\n\n- list的使用场景：比如公交车乘客的存储，随时可能有乘客下车，支持频繁的不确实位置元素的移除插入。\n\n- set的使用场景：比如对手机游戏的个人得分记录的存储，存储要求从高分到低分的顺序排列。 \n\n- map的使用场景：比如按ID号存储十万个用户，想要快速要通过ID查找对应的用户。二叉树的查找效率，这时就体现出来了。如果是vector容器，最坏的情况下可能要遍历完整个容器才能找到该用户。","source":"_posts/C和C++语法总结.md","raw":"---\ntitle: C和C++语法总结\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2020-01-03 19:07:13\npassword:\nsummary:\ntags: \n- C\n- C++\ncategories: C++\n---\n\n\n\n# 一、C语言篇\n\n## 1、gcc/g++\n\n**1.1、为什么需要gcc/g++**\n\n编辑器(如vi、记事本)是指我用它来写程序的（编辑代码），而我们写的代码语句，电脑是不懂的，我们需要把它转成电脑能懂的语句，编译器就是这样的转化工具。就是说，**我们用编辑器编写程序，由编译器编译后才可以运行！**\n\n**1.2、gcc编译器介绍**\n\n编译器是将易于编写、阅读和维护的高级计算机语言翻译为计算机能解读、运行的低级机器语言的程序。\n\ngcc（GNU Compiler Collection，GNU 编译器套件），是由 GNU 开发的编程语言编译器。gcc原本作为GNU操作系统的官方编译器，现已被大多数类Unix操作系统（如Linux、BSD、Mac OS X等）采纳为标准的编译器，gcc同样适用于微软的Windows。\n\ngcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。\n\n \n\ngcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。\n\n**1.3、编译命令**\n\n编译命令格式：\n\ngcc [-option1] ... <filename>\n\ng++ [-option1] ... <filename>\n\n- 命令、选项和源文件之间使用空格分隔\n\n- 一行命令中可以有零个、一个或多个选项\n\n- 文件名可以包含文件的绝对路径，也可以使用相对路径\n\n- 如果命令中不包含输出可执行文件的文件名，可执行文件的文件名会自动生成一个默认名，Linux平台为**a.out**，Windows平台为**a.exe**\n\n \n\ngcc、g++编译常用选项说明：\n\n| **选项** | **含义**                   |\n| -------- | -------------------------- |\n| -o file  | 指定生成的输出文件名为file |\n| -E       | 只进行预处理               |\n| -S(大写) | 只进行预处理和编译         |\n| -c(小写) | 只进行预处理、编译和汇编   |\n\n \n\n**1.4、gcc/g++的区别**\n\n1. gcc与g++都可以编译c代码和c++代码, 但是: 后缀为.c的，gcc会把它当做C程序, 而g++当做是C++程序; 后缀为.cpp的，两者都会认为是C++程序.\n\n2. 编译阶段，可以使用gcc/g++, g++会自动调用gcc。而链接阶段,可以用g++或者gcc -lstdc++\n\n   ,因为gcc命令不能自动和c++程序使用的库链接，通常用g++来完成。\n\n\n\n## 2、C/C++编译过程\n\n**2.1、编译步骤**\n\nC代码编译成可执行程序经过4步：\n\n1）预处理：宏定义展开、头文件展开、条件编译等，同时将代码中的注释删除，这里并不会检查语法\n\n2)   编译：检查语法，将预处理后文件编译生成汇编文件\n\n3）汇编：将汇编文件生成目标文件(二进制文件)\n\n4）链接：C语言写的程序是需要依赖各种库的，所以编译之后还需要把库链接到最终的可执行程序中去\n\n![img](file:////tmp/wps-yang-pc/ksohtml/wpsOWVxwY.jpg)\n\n**2.2、gcc编译过程**\n\n1) 分步编译\n\n预处理：`gcc -E hello.c -o hello.i`\n\n编  译：`gcc -S hello.i -o hello.s`\n\n汇  编：`gcc -c hello.s -o hello.o`\n\n链  接：`gcc   hello.o -o hello_elf`\n\n \n\n| **选项** | **含义**                    |\n| -------- | --------------------------- |\n| -E       | 只进行预处理                |\n| -S(大写) | 只进行预处理和编译          |\n| -c(小写) | 只进行预处理、编译和汇编    |\n| -o file  | 指定生成的输出文件名为 file |\n\n \n\n| ***\\*文件后缀\\**** | ***\\*含义\\****        |\n| ------------------ | --------------------- |\n| .c                 | C 语言文件            |\n| .i                 | 预处理后的 C 语言文件 |\n| .s                 | 编译后的汇编文件      |\n| .o                 | 编译后的目标文件      |\n\n2) 一步编译\n\n`gcc hello.c -o demo `\n\n\n\n**2.3、查找程序所依赖的动态库**\n\n１）Linux平台下，`ldd`(“l”为字母) 可执行程序\n\n２）Windows平台下，需要相应软件(`Depends.exe`)\n\n\n\n## 3、CPU内部结构与寄存器\n\n### 3.1、CPU总线\n\n**数据总线**\n\n（1） 是CPU与内存或其他器件之间的数据传送的通道。\n\n（2）数据总线的宽度决定了CPU和外界的数据传送速度。\n\n（3）每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据(即一个字节)。\n\n（4）数据总线是数据线数量之和。\n\n数据总线数据总线是CPU与存储器、CPU与I/O接口设备之间传送数据信息(各种指令数据信息)的总线，这些信号通过数据总线往返于CPU与存储器、CPU与I/O接口设备之间，因此，数据总线上的信息是双向传输的。\n\n**地址总线**\n\n（1）CPU是通过地址总线来指定存储单元的。\n\n（2）地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据（1024个内存单元）(1B)\n\n（3）地址总线是地址线数量之和。\n\n地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。它是单向的，只能从CPU传向外部存储器或I/O端口\n\n有个说法：64位系统装了64位操作系统，最大物理内存理论上=2的64次方；然而实际上地址总线只用到了35位，所以最大物理内存是32G大小\n\n**控制总线**\n\n（1）CPU通过控制总线对外部器件进行控制。\n\n（2）控制总线的宽度决定了CPU对外部器件的控制能力。\n\n（3）控制总线是控制线数量之和。\n\n控制总线，英文名称：ControlBus，简称：CB。控制总线主要用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和输入输出设备接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的\n\n### 3.2、64位和32位系统区别\n\n- 寄存器是CPU内部最基本的存储单元\n- CPU的主要组成包括了运算器和控制器。运算器是由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。\n- CPU位数=CPU中寄存器的位数=CPU能够一次并行处理的数据宽度（位数）=数据总线宽度\n- **CPU的位宽(位数)一般是以 min{ALU位宽、通用寄存器位宽、数据总线位宽}决定的**\n\n- CPU对外是通过总线(地址、控制、数据)来和外部设备交互的，总线的宽度是8位，同时CPU的寄存器也是8位，那么这个CPU就叫8位CPU\n\n- 如果总线是32位，寄存器也是32位的，那么这个CPU就是32位CPU\n\n- 有一种CPU内部的寄存器是32位的，但总线是16位，准32位CPU\n\n- 所有的64位CPU兼容32位的指令，32位要兼容16位的指令，所以在64位的CPU上是可以识别32位的指令\n\n- 在64位的CPU构架上运行了64位的软件操作系统，那么这个系统是64位\n\n- 在64位的CPU构架上，运行了32位的软件操作系统，那么这个系统就是32位\n\n- 64位的软件不能运行在32位的CPU之上\n\n### 3.3、寄存器、缓存、内存三者关系\n\n1. 寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。\n\n2. 内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。\n\n3. 寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。\n4. Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache(L1 Cache)和二级Cache(L2 Cache)，L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。\n\n总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。\n\n\n\n## 4、内存、地址和指针\n\n### 4.1、内存\n\n内存含义：\n\n- 存储器：计算机的组成中，用来存储程序和数据，辅助CPU进行运算处理的重要部分。\n\n- 内存：内部存贮器，暂存程序/数据——掉电丢失 SRAM、DRAM、DDR、DDR2、DDR3。\n\n- 外存：外部存储器，长时间保存程序/数据—掉电不丢ROM、ERRROM、FLASH（NAND、NOR）、硬盘、光盘。\n\n \n\n内存是沟通CPU与硬盘的桥梁：\n\n- 暂存放CPU中的运算数据\n\n- 暂存与硬盘等外部存储器交换的数据\n\n\n\n### 4.2、物理存储器和存储地址空间\n\n有关内存的两个概念：物理存储器和存储地址空间。\n\n \n\n物理存储器：实际存在的具体存储器芯片。\n\n- 主板上装插的内存条\n\n- 显示卡上的显示RAM芯片\n\n- 各种适配卡上的RAM芯片和ROM芯片\n\n \n\n存储地址空间：对存储器编码的范围。我们在软件上常说的内存是指这一层含义。\n\n- 编码：对每个物理存储单元（一个字节）分配一个号码\n\n- 寻址：可以根据分配的号码找到相应的存储单元，完成数据的读写\n\n\n\n### 4.3、内存地址\n\n- 将内存抽象成一个很大的一维字符数组。\n\n- 编码就是对内存的每一个字节分配一个32位或64位的编号（与32位或者64位处理器相关）。\n\n- 这个内存编号我们称之为内存地址。\n\n内存中的每一个数据都会分配相应的地址：\n\n- char:占一个字节分配一个地址\n\n- int: 占四个字节分配四个地址\n\n- float、struct、函数、数组等\n\n\n\n### 4.4、指针和指针变量\n\n- 内存区的每一个字节都有一个编号，这就是“地址”。\n\n- 如果在程序中定义了一个变量，在对程序进行编译或运行时，系统就会给这个变量分配内存单元，并确定它的内存地址(编号)\n\n- 指针的实质就是内存“地址”。指针就是地址，地址就是指针。\n\n- 指针是内存单元的编号，指针变量是存放地址的变量。\n\n- **通常我们叙述时会把指针变量简称为指针，实际他们含义并不一样**\n\n\n\n## 5、存储类型\n\n### 5.1、auto\n\n1. 普通局部变量，自动存储，该对象会自动创建和销毁，调用函数时分配内存，函数结束时释放内存。只在{}内有效，存放在堆栈中一般省略auto,  不会被默认初始化，初值不随机\n2. 全局变量，**不允许声明为auto变量**， register不适用于全局变量，生命周期由定义到程序运行结束，没有初始化会**自动赋值0或空字符**。全局变量属于整个程序，不同文件中不能有同名的全局变量，通过**extern**在其他文件中引用使用\n\n### 5.2、static\n\n1. 静态局部变量，生命周期由定义到程序运行结束，在编译时赋初值，**只初始化一次，没有初始化会自动赋值0或空字符**。只在当前{}内有效\n2. 静态全局变量，生命周期由定义到程序运行结束，在编译时赋初值，只初始化一次，没有初始化会自动赋值0或空字符。从定义到文件结尾起作用，在一个程序中的其他文件中可以定义同名的静态全局变量，因为作用于不冲突。\n\n### 5.3、extern\n\n1. 外部变量声明，是指这是一个已在别的地方定义过的对象，这里只是对变量的一次重复引用，不会产生新的变量。\n\n2. 使用extern时，注意不能重复定义，否则编译报错\n\n   ```c\n   //    程序文件一：\n       extern int a = 10; //编译警告，extern的变量最好不要初始化\n   //    程序文件二：\n       extern int a = 20; //重复定义，应改为extern int a;\n   ```\n\n3. 如果我们希望该外部变量只能在本文件内使用，而不能被其他文件引用可以在外部变量定义时加static声明。防止别人写的模块误用。\n\n4. 在函数外部定义的全局变量，作用域开始于变量定义，结束于程序文件的结束。我们可以extern来声明外部变量来扩展它的作用域。同一个文件内，extern声明之后就可以作用域扩大到声明处到文件结束。比如在一个函数之后定义外部变量a，之后的函数可以使用该变量，但是之前的函数不能使用，加extern可以解决。\n\n   ```c\n   #include <stdio.h>\n   \n   extern int g1;\n   int main(void)\n   {\n       extern int g2;\n       printf(\"%d,%d\\n\", g1,g2);\n       return 0;\n   }\n   int g1 = 77;\n   int g2 = 88;\n   ```\n\n   \n\n5. 多个文件时，可以在未定义该外部变量的文件内做extern声明即可以使用。但是需要注意可能执行一个文件时改变了该全局变量的值，影响其他文件的调用。编译时遇到extern，会先在文件内找是否定义了该外部变量。如果未找到则在链接时在其他文件中找。\n\n### 5.4、register\n\n1. 寄存器变量，请求编译器将这个变量保存在CPU的寄存器中，从而加快程序的运行.只是建议CPU这样做，非强制,声明变量为register,编译器并不一定会将它处理为寄存器变量\n\n2. 动态和静态变量都是存放在内存中，程序中遇到该值时用控制器发指令将变量的值送到运算器中，需要存数再保存到内存中。如果频繁使用一个变量，比如一个函数体内的多次循环每次都引用该局部变量，我们则可以把局部变量的值放到CPU的寄存器中，叫寄存器变量。不需要多次到内存中存取提高效率。\n\n3. 但是只能局部自动变量和形参可以做寄存器变量。在函数调用时占用一些寄存器，函数结束时释放。不同系统对register要求也不一样，比如对定义register变量个数，数据类型等限制，有的默认为自动变量处理。所以在程序一般也不用。\n\n4. register是不能取址的。比如 `int i`；(自动为auto)`int *p=&i;`是对的， 但`register int j; int *p = &j;`是错的，因为无法对寄存器的定址。\n\n   ```c\n   #include <stdio.h>\n   #include <time.h>\n   \n   #define TIME 1000000000\n   \n   int m, n = TIME; /* 全局变量 */\n   int main(void)\n   {\n       time_t start, stop;\n       register int a, b = TIME; /* 寄存器变量 */\n       int x, y = TIME;          /* 一般变量   */\n   \n       time(&start);\n       for (a = 0; a < b; a++);\n       time(&stop);\n       printf(\"寄存器变量用时: %d 秒\\n\", stop - start);\n       time(&start);\n       for (x = 0; x < y; x++);\n       time(&stop);\n       printf(\"一般变量用时: %d 秒\\n\", stop - start);\n       time(&start);\n       for (m = 0; m < n; m++);\n       time(&stop);\n       printf(\"全局变量用时: %d 秒\\n\", stop - start);\n   \n       return 0;\n   }\n   ```\n\n   \n\n### 5.5、volatile\n\n```c\n\t程序在使用变量时, 特别是连续多次使用变量时, 一般是载入寄存器, 直接从寄存器存取, 之后再还回内存;但如果此变量在返回内存时, 假如内存中的值已经改变了(从外部修改了)怎么办?\n为了避免这种情况的发生, 可以用 volatile 说明此变量, 以保证变量的每次使用都是直接从内存存取.\n但这样肯定会影响效率, 幸好它并不常用.\n\n另外: 如果 const volatile 同时使用, 这表示此变量只接受外部的修改.\n    \n#include <stdio.h>\n\nvolatile int num = 123;\nint main(void)\n{    \n    printf(\"%d\\n\", num);\n    getchar();\n    return 0;\n}\n```\n\n\n### 5.6、总结\n\n\n\n| 关键字       | 类型         | 生命周期           | 作用域           | 修饰对象   | 所属区                          |\n| ------------ | ------------ | ------------------ | ---------------- | ---------- | ------------------------------- |\n| auto[可省略] | 普通局部变量 | 定义到{}运行结束   | ｛｝             | 变量       | 栈区                            |\n| static       | 静态局部变量 | 定义到程序运行结束 | ｛｝             | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n|              | 全局变量     | 定义到程序运行结束 | 定义到文件结尾   |            | 初始化在data段，未初始化在BSS段 |\n| extern       | 全局变量     | 定义到程序运行结束 | 声明处到文件结尾 | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n| static       | 全局变量     | 整个程序运行期     | 声明处到文件结尾 | 变量和函数 | 初始化在data段，未初始化在BSS段 |\n| register     | 寄存器变量   | 定义到{}运行结束   | ｛｝             | 变量       | 运行时存储在CPU寄存器           |\n| extern       | 函数         | 整个程序运行期     | 声明处到文件结尾 |            | 代码区                          |\n| static       | 函数         | 整个程序运行期     | 声明处到文件结尾 |            | 代码区                          |\n\n\n\n## 6、内存分区\n\nC代码经过预处理、编译、汇编、链接4步后生成一个可执行程序。在 Linux 下，程序是一个普通的可执行文件，以下列出一个二进制可执行文件的基本情况：\n\n![](01.png)\n\n通过上图可以得知，在没有运行程序前，也就是说程序没有加载到内存前，可执行程序内部已经分好3段信息，分别为代码区（text）、数据区（data）和未初始化数据区（bss）3 个部分（有些人直接把data和bss合起来叫做静态区或全局区）。\n\n- **代码区**\n\n  存放 CPU 执行的机器指令。通常代码区是**可共享**的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区通常是**只读的**，使其只读的原因是防止程序意外地修改了它的指令。另外，代码区还规划了局部变量的相关信息。\n\n- **全局初始化数据区/静态数据区（data段）**\n\n  该区包含了在程序中明确被初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。\n\n- **未初始化数据区（又叫 bss 区）**\n\n  存入的是全局未初始化变量和未初始化静态变量。未初始化数据区的数据在程序开始执行之前被内核初始化为 0 或者空（NULL）。\n\n程序在加载到内存前，代码区和全局区(data和bss)的大小就是固定的，程序运行期间不能改变。然后，运行可执行程序，系统把程序加载到内存，除了根据可执行程序的信息分出代码区（text）、数据区（data）和未初始化数据区（bss）之外，还额外增加了栈区、堆区。\n\n![](02.png)\n\n- 代码区（text segment）\n\n  加载的是可执行文件代码段，所有的可执行代码都加载到代码区，这块内存是不可以在运行期间修改的。\n  \n- 只读数据区（文字常量区 RO data）\n\n  只读数区是程序使用的一些不会被更改的数据。一般是const修饰的变量以及程序中使用的文字常量。\n\n- 已初始化数据区 （RW data）\n\n  加载的是可执行文件数据段，存储于数据段（全局初始化，静态初始化数据）的数据的生存周期为整个程序运行过程。\n\n- 未初始化数据区（BSS）\n\n  加载的是可执行文件BSS段，位置可以分开亦可以紧靠数据段，存储于数据段的数据（全局未初始化，静态未初始化数据）的生存周期为整个程序运行过程。\n  \n- 堆区（heap）\n\n  堆是一个大容器，它的容量要远远大于栈，但没有栈那样先进后出的顺序。用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收。\n\n- 栈区（stack）\n\n  栈是一种先进后出的内存结构，由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。\n\n \n\n## 7、结构体字节对齐\n\n\n\n**7.1、内存对齐原因**\n\n```c\nstruct data\n{\n\tchar c;\n\tint i;\n};\n\nint main()\n{\n\tprintf(\"%d\\n\", sizeof(struct data)); // 5还是 8?\n\n\treturn 0;\n}\n```\n\n\n\n在Linux 32位架构下，假设变量stu存放在内存中的起始地址为0x00，那么c的起始地址为0x00、i的起始地址为0x01，变量stu共占用了5个字节：\n\n- 对变量c访问：CPU只需要一个读周期\n\n- 变量i访问：\n\n1. 首先CPU用一个读周期，从0x00处读取了4个字节(32位架构)，然后将0x01-0x03的3个字节暂存。\n\n2. 再花一个读周期读取了从0x04-0x07的4字节数据，将0x04这个字节与刚刚暂存的3个字节进行拼接从而读取到成员变量i的值。\n\n3. 读取一个成员变量i，CPU却花费了2个读周期。\n\n \n\n如果数据成员i的起始地址被放在了0x04处\n\n- 读取c成员，花费周期为1  \n\n- 读取i所花费的周期也变成了1\n\n- 引入字节对齐可以避免读取效率的下降，同时也浪费了3个字节的空间(0x01-0x03)\n\n **结构体内部成员对齐是为了实现用空间换取时间。**\n\n**7.2、内存对齐原则**\n\n- 原则1：数据成员的对齐规则\n\n  1) 最大对齐单位以CPU架构对齐，如Linux 32位最大以4字节对齐，Linux 64位最大以8字节对齐，vs(32位、64位)最大对齐单位为8字节\n\n  2) 需要和**结构体的最大成员**和CPU架构(32位或64位)对比，取小的作为对齐单位\n\n  3) 字节对齐也可以通过程序控制，采用指令：\n\n  ```c\n  #pragma pack(xx)   \n  #pragma pack(1)     //1字节对齐\n  #pragma pack(2)     //2字节对齐\n  #pragma pack(4)     //4字节对齐\n  #pragma pack(8)     //8字节对齐\n  #pragma pack(16)    //16字节对齐\n  \n  \n  #include <stdio.h>\n  #include <stdlib.h>\n  \n  #pragma pack(2)\n  \n  typedef struct\n  {\n    int aa1; //4个字节对齐 1111\n    char bb1;//1个字节对齐 1\n    short cc1;//2个字节对齐 011\n    char dd1; //1个字节对齐 1\n    } testlength;\n  \n  int length = sizeof(testlength); //2个字节对齐，length = 10\n  \n  int main(){\n  \tprintf(\"length=%d\\n\", length); // length=10\n  }\n  \n  ```\n\n  尽管通过pragma pack(xx)可以指定字节对齐单位，但需要和结构体的最大成员、CPU架构(32位或64位)对比，取最小的作为对齐单位。\n\n- 原则2：数据成员的偏移起点\n\n  结构体（struct）的数据成员，第一个数据成员放在偏移量为0的地方，以后每个数据成员存放在偏移量为该数据成员类型大小的整数倍的地方（比如int在32位机器为４字节，则要从4的整数倍地址开始存储）\n\n- 原则3：收尾工作 \n\n  结构体的总大小，也就是sizeof的结果，必须是对齐单位的整数倍，不足的要补齐。\n\n \n\n## 8、typedef与define的区别\n\n- typedef为C语言的关键字，作用是为一种数据类型(基本类型或自定义数据类型)定义一个新名字，不能创建新类型。\n\n- 与#define不同，typedef仅限于数据类型，而不是能是表达式或具体的值\n\n- define发生在预处理，typedef发生在编译阶段\n\n\n\n## 9、C语言文本操作的区别\n\n### 9.1 二进制文件和文本文件\n\n\n\n- b是二进制模式的意思，b只是在Windows有效，在Linux用r和rb的结果是一样的\n\n- Unix和Linux下所有的文本文件行都是\\n结尾，而Windows所有的文本文件行都是\\r\\n结尾\n\n- 在Windows平台下，以“文本”方式打开文件，不加b：\n\n  - 当读取文件的时候，系统会将所有的 \"\\r\\n\" 转换成 \"\\n\"\n\n  - 当写入文件的时候，系统会将 \"\\n\" 转换成 \"\\r\\n\" 写入 \n\n  - 以\"二进制\"方式打开文件，则读\\写都不会进行这样的转换\n\n- 在Unix/Linux平台下，“文本”与“二进制”模式没有区别，\"\\r\\n\" 作为两个字符原样输入输出\n\n### 9.2 文本结尾\n\n在C语言中，EOF表示文件结束符(end of file)。在while循环中以EOF作为文件结束标志，这种以EOF作为文件结束标志的文件，必须是文本文件。在文本文件中，数据都是以字符的ASCII代码值的形式存放。我们知道，ASCII代码值的范围是0~127，不可能出现-1，因此可以用EOF作为文件结束标志。\n\n`#define EOF (-1)`\n\n当把数据以二进制形式存放到文件中时，就会有-1值的出现，因此不能采用EOF作为二进制文件的结束标志。为解决这一个问题，ANSI C提供一个feof函数，用来判断文件是否结束。feof函数既可用以判断二进制文件又可用以判断文本文件。\n\n```c\n#include <stdio.h>\nint feof(FILE * stream);\n功能：检测是否读取到了文件结尾。判断的是最后一次“读操作的内容”，不是当前位置内容(上一个内容)。\n参数：\n\tstream：文件指针\n返回值：\n\t非0值：已经到文件结尾\n\t0：没有到文件结尾\n```\n\n\n\n## 10、void\n\n**void的作用**\n\n- 对函数参数的限定：当不需要传入参数时，即 `function (void);`\n- 对函数返回值的限定：当函数没有返回值时，即 `void function(void);`\n\n**void指针的作用**\n\n（1）void指针可以指向任意的数据类型，即任意类型的指针可以赋值给void指针\n\n```c\nint *a;\nvoid *p;\np=a;\n```\n\n如果void指针赋值给其他类型，则需要强制转换；`a=（int *）p;`\n\n（2）在ANSI C标准中不允许对void指针进行算术运算，因为没有特定的数据类型，即在内存中不知道移动多少个字节；而在GNU标准中，认为void指针和char指针等同。\n\n**应用**\n\n（1）void指针一般用于应用的底层，比如malloc函数的返回类型是void指针，需要再强制转换； \n\n（2）文件句柄HANDLE也是void指针类型，这也是句柄和指针的区别； \n\n（3）内存操作函数的原型也需要void指针限定传入参数：\n\n```c\nvoid * memcpy (void *dest, const void *src, size_t len);\nvoid * memset (void *buffer, int c, size_t num );\n```\n\n（4）面向对象函数中底层对基类的抽象。\n\n## 11、数据类型的本质\n\n- 数据类型可理解为创建变量的模具：是固定内存大小的别名。\n\n- 数据类型的作用：编译器预算对象（变量）分配的内存空间大小。\n\n\n\n## 12、变量的本质\n\n变量的本质：一段连续内存空间的别名。\n\n\t1）程序通过变量来申请和命名内存空间 int a = 0\n\t\n\t2）通过变量名访问内存空间\n\t\n\t3）不是向变量读写数据，而是向变量所代表的内存空间中读写数据\n\n\n\n## 13、数组与指针的关系\n\n数组不是指针，数组名也只有在表达式中才会被当成一个指针常量。数组名在表达式中使用的时候，编译器才会产生一个指针常量。\n\n- `p[i]`这种写法只不过是`*(p + i)`的简便写法。实际上，至少对于编译器来说，[]这样的运算符完全可以不存在。[]运算符是为了方便人们读写而引入的，是一种语法糖。\n\n- 当数组名作为sizeof操作符的操作数的时候，此时sizeof返回的是整个数组的长度，而不是指针数组指针的长度。\n- 当数组名作为 & 操作符的操作数的时候，此时返回的是一个指向数组的指针，而不是指向某个数组元素的指针常量。\n\n- 二级指针是指向指针的指针，而指针数组则是元素类型为指针的数组。虽然它们是不一样的，但是在表达式中，它们是等效的。\n\n## 14、字节序(大端、小端)\n\n**14.1 大端和小端**\n\n\n\n计算机的内存最小单位是字节。字节序是指多字节(大于1字节)数据的存储顺序，在设计计算机系统的时候，有两种处理内存中数据的方法：大端格式、小端格式。\n\n- 小端格式(Little-Endian)：将低位字节数据存储在低地址。X86和ARM都是小端对齐。\n\n- 大端格式(Big-Endian)：将高位字节数据存储在低地址。很多Unix服务器的CPU是大端对齐的、网络上数据是以大端对齐。\n\n![](03.png)\n\n对于整形 0x12345678，它在大端格式和小端格式的系统中，分别如下图所示的方式存放：\n\n![](04.png)\n\n**14.2 网络字节序和主机字节序**\n\n网络字节顺序NBO(Network Byte Order)\n\n\t在网络上使用统一的大端模式，低字节存储在高地址，高字节存储在低地址。\n\n\n\n主机字节序顺序HBO(Host Byte Order)\n\n\t不同的机器HBO不相同，与CPU设计相关，数据的顺序是由CPU决定的，而与操作系统无关。\n\n\n\n处理器 |操作系统  |字节排序|\n\nAlpha    全部    Little endian\nHP-PA    NT    Little endian\nHP-PA    UNIX    Big endian\nIntelx86    全部    Little endian <-----x86系统是小端字节序系统\nMotorola680x()    全部    Big endian\nMIPS    NT    Little endian\nMIPS    UNIX    Big endian\nPowerPC    NT    Little endian\nPowerPC    非NT    Big endian  <-----PPC系统是大端字节序系统\nRS/6000    UNIX    Big endian\nSPARC    UNIX    Big endian\nIXP1200 ARM核心    全部    Little endian \n\n```c\n相关函数：\nhtons 把unsigned short类型从主机序转换到网络序\nhtonl 把unsigned long类型从主机序转换到网络序\nntohs 把unsigned short类型从网络序转换到主机序\nntohl 把unsigned long类型从网络序转换到主机序\n\n头文件：#include <netinet/in.h>\n定义函数：unsigned short ntohs(unsigned short netshort);\n函数说明：ntohs()用来将参数指定的16 位netshort 转换成主机字符顺序.\n返回值：返回对应的主机顺序.\n范例：参考getservent().\n```\n\n```c\n#include <stdio.h>\n#include <winsock.h> // windows使用winsock.h\n\nint main()\n{\n    //左边是高位，右边是低位，高位放高地址，低位放低地址\n    int a = 0x11223344;\n    unsigned char *p = (unsigned char *)&a;\n\n    int i;\n    for (i = 0; i < 4; i++)\n    {\n        printf(\"%x\\n\", p[i]);\n    }\n\n    u_long b = htonl(a);\n    unsigned char *q = (unsigned char *)&b;\n    for (i = 0; i<4; i++){\n        printf(\"%x\\n\", q[i]);\n    }\n\n    return 0;\n}\n\n//  gcc hello.c -lwsock32 -o hello\n// 编译时添加-lwsock32，不然会报错undefined reference to `htonl@4'\n// 在编译socket程序的时候，一定要加上-l wsock32选项，因为mingw默认没有包含windows库\n```\n\n## 15、数组指针\n\n```c\n// 1) 先定义数组类型，再根据类型定义指针变量\ntypedef int A[10];\nA *p = NULL;\n\n// 2) 先定义数组指针类型，根据类型定义指针变量\ntypedef int(*P)[10]; //第一个()代表指针，第二个[]代表数组\nP q; //数据组指针变量\n\n// 3) 直接定义数组指针变量\nint (*q)[10];\n\n```\n\n\n\n## 16、深拷贝和浅拷贝\n\n**结构体**:\n\n浅拷贝  不同结构体成员指针变量指向同一块内存\n\n深拷贝  不同结构体成员指针变量指向不同的内存\n\n\n\n**类**:\n\n浅拷贝 类中有动态分配的空间的指针指向相同\n\n深拷贝 类中有动态分配的空间的指针指向不同的内存空间\n\n\n\n## 17、\\#include< > 与 #include \"\"的区别\n\n- \"\" 表示系统先在file1.c所在的当前目录找file1.h，如果找不到，再按系统指定的目录检索。\n\n- < > 表示系统直接按系统指定的目录检索。\n\n \n\n注意：\n\n- #include <>常用于包含库函数的头文件\n- #include \"\"常用于包含自定义的头文件\n- 理论上#include可以包含任意格式的文件(.c .h等) ，但我们一般用于头文件的包含。\n\n\n\n## 18、静态库和动态库\n\n**18.1、静态库优缺点**\n\n- 静态库在程序的链接阶段被复制到了程序中，和程序运行的时候没有关系；\n\n- 程序在运行时与函数库再无瓜葛，移植方便；\n\n- 浪费空间和资源，所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。\n\n**18.2、动态库**\n\n要解决空间浪费和更新困难这两个问题，最简单的办法就是把程序的模块相互分割开来，形成独立的文件，而不是将他们静态的链接在一起。\n\n简单地讲，就是不对哪些组成程序的目标程序进行链接，等程序运行的时候才进行链接。也就是说，把整个链接过程推迟到了运行时再进行，这就是动态链接的基本思想。\n\n**18.3、动态库的lib文件和静态库的lib文件的区别**\n\n在使用动态库的时候，往往提供两个文件：一个引入库（.lib）文件（也称“导入库文件”）和一个DLL（.dll）文件。 \n\n虽然引入库的后缀名也是“lib”，但是，动态库的引入库文件和静态库文件有着本质的区别，对一个DLL文件来说，其引入库文件（.lib）包含该DLL导出的函数和变量的符号名，而.dll文件包含该DLL实际的函数和数据。\n\n在使用动态库的情况下，在编译链接可执行文件时，只需要链接该DLL的引入库文件，该DLL中的函数代码和数据并不复制到可执行文件，直到可执行程序运行时，才去加载所需的DLL，将该DLL映射到进程的地址空间中，然后访问DLL中导出的函数。\n\n\n\n# 二、C++篇\n\n## 1、c语言和c++语言的关系\n\n“c++”中的++来自于c语言中的递增运算符++，该运算符将变量加1。c++起初也叫”c with clsss”。通过名称表明，c++是对C的扩展，因此c++是c语言的超集，这意味着任何有效的c程序都是有效的c++程序。c++程序可以使用已有的c程序库。\n\nc++语言在c语言的基础上添加了**面向对象编程**和**泛型编程**的支持。c++继承了c语言高效，简洁，快速和可移植的传统。\n\nc++融合了3种不同的编程方式:\n\n- c语言代表的过程性语言.\n\n- c++在c语言基础上添加的类代表的面向对象语言.\n\n- c++模板支持的泛型编程。\n\n## 2、左值和右值\n\n判断是否是左值，有一个简单的方法，就是看看能否取它的地址，能取地址就是左值，否则就是右值。\n\n当一个对象成为右值时，使用的是它的值(内容), 而成为左值时，使用的是它的身份（在内存中的位置）。\n\n平常所说的引用，实际上指的就是左值引用`lvalue reference`, 常用单个&来表示。左值引用只能接收左值，不能接收右值。**const关键字会让左值引用变得不同，它可以接收右值。**\n\n\n\n**为了支持移动操作，在c++11版本，增加了右值引用。**右值引用一般用于绑定到一个即将销毁的对象，所以右值引用又通常出现在移动构造函数中。\n\n看完下面的例子，左值和右值基本就清楚了，左值具有持久的状态，有独立的内存空间，右值要么是字面常量，要么就是表达式求值过程中创建的临时对象\n\n\n\n```c++\nint i = 66;\nint &r = i ; //r 是一个左引用，绑定左值 i\n\nint &&rr = i ; //rr是一个右引用，绑定到左值i , 错误！\nint &r2 = i*42 ; //  r2 是一个左引用， 而i*42是一个表达式，计算出来的结果是一个右值。 错误！\n\nconst int &r3 = i*42; // const修饰的左值引用 正确\nint &&rr2 = i*42 ; // 右引用，绑定右值 正确\n```\n\n## 3、c++对c的扩展\n\n**3.1 三目运算符**\n\nc语言三目运算表达式返回值为数据值，为右值，不能赋值。\n\nc++语言三目运算表达式返回值为变量本身(引用)，为左值，可以赋值。\n\n```c++\nint a = 10;\nint b = 20;\nprintf(\"ret:%d\\n\", a > b ? a : b);\n//思考一个问题，(a > b ? a : b) 三目运算表达式返回的是什么？\n\ncout << \"b:\" << b << endl;\n//返回的是左值，变量的引用\n(a > b ? a : b) = 100;//返回的是左值，变量的引用\ncout << \"b:\" << b << endl;\n```\n\n**3.2 bool**\n\nc++中新增bool类型关键字\n\n- bool类型只有两个值，true(1)， false（0）\n- bool类型占1个字节\n- 给bool类型赋值时, 非0值会自动转换为true(1), 0值会自动转换为false（0）\n\nC语言中也有bool类型，在c99标准之前是没有bool关键字，c99标准已经有bool类型，包含头文件`stdbool.h`,就可以使用和c++一样的bool类型。\n\n**3.3 struct类型增强**\n\n- c中定义结构体变量需要加上struct关键字，c++不需要。\n\n- c中的结构体只能定义成员变量，不能定义成员函数。c++即可以定义成员变量，也可以定义成员函数。\n\n\n\n**3.4 更严格的类型转换**\n\n在C++中，不同类型的变量一般是不能直接赋值的，需要相应的强转。\n\n在C++中，所有的变量和函数都必须有类型\n\n**3.4 全局变量检测增强**\n\n```c++\nint a = 10; //赋值，当做定义\nint a; //没有赋值，当做声明\n\nint main(){\n\tprintf(\"a:%d\\n\",a);\n\treturn EXIT_SUCCESS;\n}\n\n// 上面的代码在c++下编译失败，在c下编译通过。\n```\n\n\n\n## 4、内部连接和外部连接\n\n内部连接：如果一个名称对编译单元(.cpp)来说是局部的，在链接的时候其他的编译单元无法链接到它且不会与其它编译单元(.cpp)中的同样的名称相冲突。例如static函数，inline函数等（注 : 用static修饰的函数，本限定在本源码文件中，不能被本源码文件以外的代码文件调用。而普通的函数，默认是extern的，也就是说，可以被其它代码文件调用该函数。）\n\n外部连接：如果一个名称对编译单元(.cpp)来说不是局部的，而在链接的时候其他的编译单元可以访问它，也就是说它可以和别的编译单元交互。 例如全局变量就是外部链接 。\n\n\n\n## 5、C/C++中const的区别\n\n1、const全局变量\n\nc语言全局const会被存储到只读数据区。c++中全局const当声明extern或者对变量取地址时，编译器会分配存储地址，变量存储在只读数据段。两个都受到了只读数据区的保护，不可修改。\n\n```c++\nconst int constA = 10;\nint main(){\n    int* p = (int*)&constA;\n    *p = 200;\n}\n\n// 以上代码在c/c++中编译通过，在运行期，修改constA的值时，发生写入错误。原因是修改只读数据段的数据。\n```\n\n2、const 局部变量\n\nc语言中局部const存储在堆栈区，只是不能通过变量直接修改const只读变量的值，但是可以跳过编译器的检查，通过指针间接修改const值。\n\n```c++\nconst int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\nprintf(\"constA:%d\\n\",constA);\nprintf(\"*p:%d\\n\", *p);\n\n// constA:300\n// *p:300\n```\n\nc++中对于局部的const变量要区别对待：\n\n- 对于基础数据类型，也就是const int a = 10这种，编译器会把它放到符号表中，不分配内存，当对其取地址时，会分配内存。\n\n```c++\nconst int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:10\n// *p:300\n// constA在符号表中，当我们对constA取地址，这个时候为constA分配了新的空间，*p操作的是分配的空间，而constA是从符号表获得的值。\n```\n\n​\t\n\n- 对于基础数据类型，如果用一个变量初始化const变量，如果const int a = b,那么也是会给a分配内存。\n\n```c++\nint b = 10;\nconst int constA = b;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:300\n// *p:300 \n```\n\n- 对于自定数据类型，比如类对象，那么也会分配内存。\n\n```c++\nconst Person person; //未初始化age\n//person.age = 50; //不可修改\nPerson* pPerson = (Person*)&person;\n//指针间接修改\npPerson->age = 100;\ncout << \"pPerson->age:\" << pPerson->age << endl;\npPerson->age = 200;\ncout << \"pPerson->age:\" << pPerson->age << endl;\n\n// pPerson->age:100\n// pPerson->age:200\n//为person分配了内存，所以我们可以通过指针的间接赋值修改person对象。\n```\n\n3、链接方式\n\nc中const默认为外部连接，c++中const默认为内部连接.当c语言两个文件中都有const int a的时候，编译器会报重定义的错误。而在c++中，则不会，因为c++中的const默认是内部连接的。如果想让c++中的const具有外部连接，必须显示声明为: extern const int a = 10;\n\n\n\n## 6、const与#define的区别\n\n- const有数据类型，可进行编译器类型安全检查。#define无类型，不可以进行类型检查\n\n- const有作用域，而#define不重视作用域，默认定义处到文件结尾。如果想定义在指定作用域下有效的常量，那么#define就不能用。\n\n  \n\n## 7、引用\n\n1. &在此不是求地址运算，而是起标识作用。\n\n2. 类型标识符是指目标变量的类型\n\n3. 必须在声明引用变量时进行初始化。\n\n4. 引用初始化之后不能改变。\n\n5. 不能有NULL引用。必须确保引用是和一块合法的存储单元关联。\n\n6. **可以建立对数组的引用。**\n7. 函数不能返回局部变量的引用\n8. 函数当左值，必须返回引用\n\n\n\n**引用的本质**\n\n引用的本质是在c++内部实现一个指针常量\n\n`Type& ref = val;  // Type* const ref = val;`\n\n\n\nc++编译器在编译过程中使用常指针作为引用的内部实现，因此引用所占用的空间大小与指针相同，只是这个过程是编译器内部实现，用户不可见。\n\n```c++\n//发现是引用，转换为 int* const ref = &a;\nvoid testFunc(int& ref){\n\tref = 100; // ref是引用，转换为*ref = 100\n}\nint main(){\n\tint a = 10;\n\tint& aRef = a; //自动转换为 int* const aRef = &a;这也能说明引用为什么必须初始化\n\taRef = 20; //内部发现aRef是引用，自动帮我们转换为: *aRef = 20;\n\tcout << \"a:\" << a << endl;\n\tcout << \"aRef:\" << aRef << endl;\n\ttestFunc(a);\n\treturn EXIT_SUCCESS;\n}\n\n```\n\n\n\n## 8、面向对象的三大特性\n\n**封装**\n\n1. 把变量（属性）和函数（操作）合成一个整体，封装在一个类中\n2. 对变量和函数进行访问控制\n\n**继承**\n\n\tc++最重要的特征是代码重用，通过继承机制可以利用已有的数据类型来定义新的数据类型，新的类不仅拥有旧类的成员，还拥有新定义的成员。\n\t\n\t派生类继承基类，派生类拥有基类中全部成员变量和成员方法（除了构造和析构之外的成员方法），但是在派生类中，继承的成员并不一定能直接访问，不同的继承方式会导致不同的访问权限。\n\t\n\t任何时候重新定义基类中的一个重载函数，在新类中所有的其他版本将被自动隐藏.\n\t\n\toperator=也不能被继承，因为它完成类似构造函数的行为。\n\n**多态**\n\n\tc++支持编译时多态(静态多态)和运行时多态(动态多态)，运算符重载和函数重载就是编译时多态，而派生类和虚函数实现运行时多态。\n\t\n\t静态多态和动态多态的区别就是函数地址是早绑定(静态联编)还是晚绑定(动态联编)。如果函数的调用，在编译阶段就可以确定函数的调用地址，并产生代码，就是静态多态(编译时多态)，就是说地址是早绑定的。而如果函数的调用地址不能编译不能在编译期间确定，而需要在运行时才能决定，这这就属于晚绑定(动态多态,运行时多态)。\n\t\n\t多态性改善了代码的可读性和组织性，同时也使创建的程序具有可扩展性。\n\n## 9、C++编译器优化技术：RVO/NRVO和复制省略\n\n现代编译器缺省会使用RVO（return value optimization，返回值优化）、NRVO（named return value optimization、命名返回值优化）和复制省略（Copy elision）技术，来减少拷贝次数来提升代码的运行效率\n\n注1：vc6、vs没有提供编译选项来关闭该优化，无论是debug还是release都会进行RVO和复制省略优化\n\n注2：vc6、vs2005以下及vs2005+ Debug上不支持NRVO优化，vs2005+ Release支持NRVO优化\n\n注3：g++支持这三种优化，并且可通过编译选项：-fno-elide-constructors来关闭优化\n\n**RVO**\n\n```c++\n#include <stdio.h>\nclass A\n{\npublic:\n    A()\n    {\n        printf(\"%p construct\\n\", this);\n    }\n    A(const A& cp)\n    {\n        printf(\"%p copy construct\\n\", this);\n    }\n    ~A() \n    {\n        printf(\"%p destruct\\n\", this);\n    }\n};\n\nA GetA()\n{\n    return A();\n}\n\nint main()\n{\n    {\n        A a = GetA();\n    }\n\n    return 0;\n}\n```\n\n在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct\n```\n\n在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下：\n\n```\n0x7ffc46947d4f construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量temp\n0x7ffc46947d7f copy construct // 函数GetA return语句处，把临时变量temp做为参数传入并调用拷贝构造函数A(const A& cp)将返回值ret构造出来\n0x7ffc46947d4f destruct // 函数GetA执行完return语句后，临时变量temp生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e copy construct // 函数GetA调用结束，返回上层main函数后，把返回值变量ret做为参数传入并调用拷贝构造函数A(const A& cp)将变量A a构造出来\n0x7ffc46947d7f destruct // A a = GetA()语句结束后，返回值ret生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n注：临时变量temp、返回值ret均为匿名变量\n\n**NRVO**\n\ng++编译器、vs2005+ Release（开启/O2及以上优化开关）\n\n修改上述代码，将GetA的实现修改成：\n\n```c++\nA GetA()\n{\n    A o;\n    return o;\n}\n```\n\n在g++、vs2005+ Release中，上述代码也仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct\n```\n\ng++加上-fno-elide-constructors选项关闭优化后，和上述结果一样\n\n```\n0x7ffc46947d4f construct\n0x7ffc46947d7f copy construct\n0x7ffc46947d4f destruct\n0x7ffc46947d7e copy construct\n0x7ffc46947d7f destruct\n0x7ffc46947d7e destruct\n```\n\n但在vc6、vs2005以下、vs2005+ Debug中，没有进行NRVO优化，输出结果为：\n\n```\n18fec4 construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量o\n18ff44 copy construct  // 函数GetA return语句处，把临时变量o做为参数传入并调用拷贝构造函数A(const A& cp)将返回值ret构造出来\n18fec4 destruct  // 函数GetA执行完return语句后，临时变量o生命周期结束，调用其析构函数~A()\n18ff44 destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n注：**与g++、vs2005+ Release相比，vc6、vs2005以下、vs2005+ Debug只优化掉了返回值到变量a的拷贝，命名局部变量o没有被优化掉，所以最后一共有2次构造和析构的调用**\n\n**复制省略**\n\n典型情况是：调用构造函数进行值类型传参\n\n```\nvoid Func(A a) \n{\n}\n\nint main()\n{\n    {\n        Func(A());\n    }\n\n    return 0;\n}\n```\n\n在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：\n\n```\n0x7ffeb5148d0f construct\n0x7ffeb5148d0f destruct\n```\n\n在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下： \n\n```\n0x7ffc53c141ef construct   // 在main函数中，调用无参构造函数构造实参变量o\n0x7ffc53c141ee copy construct // 调用Func函数后，将实参变量o做为参数传入并调用拷贝构造函数A(const A& cp)将形参变量a构造出来\n0x7ffc53c141ee destruct // 函数Func执行完后，形参变量a生命周期结束，调用其析构函数~A()\n0x7ffc53c141ef destruct // 返回main函数后，实参变量o要离开作用域，生命周期结束，调用其析构函数~A()\n```\n\n\n\n**优化失效的情况**\n\n1. 根据不同的条件分支，返回不同变量\n2. 返回参数变量\n3. 返回全局变量\n4. 返回复合函数类型中的成员变量\n5. 返回值赋值给已构造好的变量（此时会调用operator==赋值运算符）\n\nhttps://www.cnblogs.com/kekec/p/11303391.html\n\n## 10、explicit关键字\n\n- explicit用于修饰构造函数,防止隐式转化。\n\n- 是针对单参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造)而言。\n\n```c++\nclass MyString{\npublic:\n\texplicit MyString(int n){\n\t\tcout << \"MyString(int n)!\" << endl;\n\t}\n\tMyString(const char* str){\n\t\tcout << \"MyString(const char* str)\" << endl;\n\t}\n};\n\nint main(){\n\n\t//给字符串赋值？还是初始化？\n\t//MyString str1 = 1; \n\tMyString str2(10);\n\n\t//寓意非常明确，给字符串赋值\n\tMyString str3 = \"abcd\";\n\tMyString str4(\"abcd\");\n\n\treturn EXIT_SUCCESS;\n}\n\n```\n\n\n\n## 11、new/delete/malloc/free\n\n**11.1、new**\n\n1. 内存申请成功后，会返回一个指向该内存的地址。\n2. 若内存申请失败，则抛出异常，\n3. 申请成功后，如果是程序员定义的类型，会执行相应的构造函数\n\n**11.2、delete**\n\n1. 如果指针的值是0 ，delete不会执行任何操作，有检测机制\n2. delete只是释放内存，不会修改指针，指针仍然会指向原来的地址\n3. 重复delete，有可能出现异常\n4. 如果是自定义类型，会执行析构函数\n\n**11.3、malloc**\n\n1. malloc 申请成功之后，返回的是void类型的指针。需要将void*指针转换成我们需要的类型。1.\n2. malloc 要求制定申请的内存大小 ， 而new由编译器自行计算。\n3. 申请失败，返回的是NULL ， 比如： 内存不足。\n4. 不会执行自定义类型的构造函数\n\n\n\n**11.4、free**\n\n1. 如果是空指针，多次释放没有问题，非空指针，重复释放有问题\n2. 不会执行对应的析构\n3. delete的底层执行的是free\n\n \n\n## 12、const\n\n1. const修饰静态成员变量时可以在类内部初始化\n2. const 修饰成员函数时，修饰的是this指针，所以成员函数内不可以修改任何普通成员变量，当成员变量类型符前用mutable修饰时例外\n3. 常对象(cons修饰的对象)只能调用const修饰的成员函数\n4. 常对象可以访问成员属性，但是不能修改\n5. **const关键字会让左值引用变得不同，它可以接收右值。**\n\n\n\n## 13、虚继承\n\n虚继承是解决C++多重继承问题的一种手段，从不同途径继承来的同一基类，会在子类中存在多份拷贝。这将存在两个问题：其一，浪费存储空间；第二，存在二义性问题，多重继承可能存在一个基类的多份拷贝，这就出现了二义性。\n\n```c++\nclass BigBase{\npublic:\n\tBigBase(){ mParam = 0; }\n\tvoid func(){ cout << \"BigBase::func\" << endl; }\npublic:\n\tint mParam;\n};\n\nclass Base1 : public BigBase{};\nclass Base2 : public BigBase{};\nclass Derived : public Base1, public Base2{};\n\nint main(){\n\n\tDerived derived;\n\t//1. 对“func”的访问不明确\n\t//derived.func();\n\t//cout << derived.mParam << endl;\n\tcout << \"derived.Base1::mParam:\" << derived.Base1::mParam << endl;\n\tcout << \"derived.Base2::mParam:\" << derived.Base2::mParam << endl;\n\n\t//2. 重复继承\n\tcout << \"Derived size:\" << sizeof(Derived) << endl; //8\n\n\treturn EXIT_SUCCESS;\n}\n```\n\n\n\n虚继承可以解决多种继承前面提到的两个问题：\n\n虚继承底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，64位8字节/windows 4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类指针依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，**虚基类指针也会被继承。**\n\n实际上，`vbptr`指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。\n\n![](05.png)\n\n```c++\n#include <iostream>\n\nusing namespace std;\n\n\nclass A  //大小为4\n{\npublic:\n    int a;\n};\n\nclass B:virtual public A{ // vbptr 8, int b 4, int a 4 = 12\npublic:\n    int b;\n};\n\nclass C:virtual public A{// vbptr 8, int c 4, int a 4 = 12\npublic:\n    int c;\n};\n\nclass D:public B, public C{\n    // int a, b, c, d=16\n    // class B  vbptr 4\n    // class C  vbptr 4  = 24\npublic:\n    int d;\n};\n\nint main()\n{\n    A a;\n    B b;\n    C c;\n    D d;\n    cout << sizeof(a) << endl; // 4 \n    cout << sizeof(b) << endl; // 16\n    cout << sizeof(c) << endl; // 16\n    cout << sizeof(d) << endl; // 24\n    return 0;\n}\n```\n\n链接:https://blog.csdn.net/bxw1992/article/details/77726390\n\n## 14、虚函数\n\n**问题**\n\n\t父类引用或指针可以指向子类对象，通过父类指针或引用来操作子类对象。但是由于编译阶段编译器根据对象的指针或者引用选择函数调用，所以会调用父类的函数。\n\t\n\t解决问题的方法是迟绑定(动态绑定)，在运行时根据对象的实际类型决定。\n\n**解决**\n\n\tC++动态多态性是通过虚函数来实现的，虚函数允许子类（派生类）重新定义父类（基类）成员函数，而子类（派生类）重新定义父类（基类）虚函数的做法称为覆盖(override)，或者称为重写。\n\t\n\t对于特定的函数进行动态绑定，c++要求在基类中声明这个函数的时候使用virtual关键字,动态绑定也就对virtual函数起作用.\n\n- 为创建一个需要动态绑定的虚成员函数，可以简单在这个函数声明前面加上virtual关键字，定义时候不需要.\n\n- 如果一个函数在基类中被声明为virtual，那么在所有派生类中它都是virtual的.\n\n- 在派生类中virtual函数的重定义称为重写(override).\n\n- Virtual关键字只能修饰成员函数.\n\n- 构造函数不能为虚函数\n\n**虚函数原理**\n\n首先，我们看看编译器如何处理虚函数。当编译器发现我们的类中有虚函数的时候，编译器会创建一张虚函数表（virtual function table ），表中存储着类对象的虚函数地址，并且给类增加一个指针，这个指针就是`vpointer`(缩写`vptr`)，这个指针是指向虚函数表。\n\n父类对象包含的指针，指向父类的虚函数表地址，子类对象包含的指针，指向子类的虚函数表地址。\n\n如果子类重新定义了父类的函数，那么函数表中存放的是新的地址，如果子类没有重新定义，那么表中存放的是父类的函数地址。如果子类有自己的虚函数，则只需要添加到表中即可。\n\n```c++\nclass A{\npublic:\n    virtual void func1(){}\n    virtual void func2(){}\n};\n\n//B类为空，那么大小应该是1字节，实际情况是这样吗？\nclass B : public A{};\n\nvoid test(){\n    cout << \"A size:\" << sizeof(A) << endl; // win指针字节为4, linux 64 字节为8\n    cout << \"B size:\" << sizeof(B) << endl; // 4\n}\n```\n\n**多态的成立条件：**\n\n- 有继承\n\n- 子类重写父类虚函数函数\n\n      a) 返回值，函数名字，函数参数，必须和父类完全一致(析构函数除外) \n        \n      b) 子类中virtual关键字可写可不写，建议写\n\n- 类型兼容，父类指针，父类引用指向子类对象\n\n\n\n**抽象类和纯虚函数**\n\n当基类中有至少一个纯虚函数则为抽象类\n\n- 纯虚函数使用关键字virtual，并在其后面加上=0。如果试图去实例化一个抽象类，编译器则会阻止这种操作。\n\n- 当继承一个抽象类的时候，必须实现所有的纯虚函数，否则由抽象类派生的类也是一个抽象类。\n\n- Virtual void fun() = 0;告诉编译器在vtable中为函数保留一个位置，但在这个特定位置不放地址。\n\n**接口类**\n\n接口类中只有函数原型定义，没有任何数据定义。多重继承接口不会带来二义性和复杂性问题。接口类只是一个功能声明，并不是功能实现，子类需要根据功能说明定义功能实现。\n\n注意:除了析构函数外，其他声明都是纯虚函数。\n\n**虚析构函数**\n\n虚析构函数是为了解决[基类]的[指针]指向派生类对象，并用基类的指针删除派生类对象。\n\n**虚函数和虚继承的异同**\n\n在这里我们可以对比虚函数的实现原理：他们有相似之处，都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间）。\n\n虚基类指针依旧存在继承类中，只占用存储空间；基类虚函数指针不存在于子类中，不占用存储空间。\n\n虚基类表存储的是虚基类相对直接继承类的偏移；而虚函数表存储的是虚函数地址。\n\n## 15、函数模板的机制\n\n**函数模板机制结论：**\n\n- 编译器并不是把函数模板处理成能够处理任何类型的函数\n\n- 函数模板通过具体类型产生不同的函数\n\n- 编译器会对函数模板进行两次编译，在声明的地方对模板代码本身进行编译，在调用的地方对参数替换后的代码进行编译。\n\n**局限性**:\n\n编写的模板函数很可能无法处理某些类型，另一方面，有时候通用化是有意义的，但C++语法不允许这样做。为了解决这种问题，可以提供**模板的重载**，为这些特定的类型提供具体化的模板。\n\n```c++\nclass Person\n{\npublic:\n    Person(string name, int age)\n    {\n        this->mName = name;\n        this->mAge = age;\n    }\n    string mName;\n    int mAge;\n};\n\n\n//普通交换函数\ntemplate <class T>\nvoid mySwap(T &a,T &b)\n{\n    T temp = a;\n    a = b;\n    b = temp;\n}\n\n\n//第三代具体化，显示具体化的原型和定意思以template<>开头，并通过名称来指出类型\n//具体化优先于常规模板\ntemplate<>void mySwap<Person>(Person &p1, Person &p2)\n{\n    string nameTemp;\n    int ageTemp;\n\n    nameTemp = p1.mName;\n    p1.mName = p2.mName;\n    p2.mName = nameTemp;\n\n    ageTemp = p1.mAge;\n    p1.mAge = p2.mAge;\n    p2.mAge = ageTemp;\n\n}\n\nvoid test()\n{\n    Person P1(\"Tom\", 10);\n    Person P2(\"Jerry\", 20);\n\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n    mySwap(P1, P2);\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n}\n```\n\n## 16、C++类型转换\n\n**静态类型转换static_cast**\n\n- 用于[类层次结构](http://baike.baidu.com/view/2405425.htm)中基类（父类）和[派生类](http://baike.baidu.com/view/535532.htm)（子类）之间指针或引用的转换。\n\n  - 进行上行转换（把派生类的指针或引用转换成基类表示）是安全的；\n\n  - 进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。\n\n- 用于基本数据类型之间的转换，如把int转换成char，把char转换成int。这种转换的安全性也要开发人员来保证。\n\n**动态类型转换(dynamic_cast)**\n\n- dynamic_cast主要用于类层次间的上行转换和下行转换；\n\n- 在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；\n\n- 在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全；\n\n**常量转换(const_cast)**\n\n该运算符用来修改类型的const属性。。\n\n- 常量指针被转化成非常量指针，并且仍然指向原来的对象；\n\n- 常量引用被转换成非常量引用，并且仍然指向原来的对象；\n\n \n\n\n\n**重新解释转换(reinterpret_cast)**\n\n这是最不安全的一种转换机制，最有可能出问题。\n\n主要用于将一种数据类型从一种类型转换为另一种类型。它可以将一个指针转换成一个整数，也可以将一个整数转换成一个指针.\n\n\n\n# 三、STL\n\n## 1、STL六大组件简介\n\nSTL提供了六大组件，彼此之间可以组合套用，这六大组件分别是:容器、算法、迭代器、仿函数、适配器（配接器）、空间配置器。\n\n**容器：**各种数据结构，如vector、list、deque、set、map等, 用来存放数据，从实现角度来看，STL容器是一种class template。\n\n**算法：**各种常用的算法，如sort、find、copy、for_each。从实现的角度来看，STL算法是一种function tempalte.\n\n**迭代器：**扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，**迭代器是一种将operator* , operator-> , operator++,operator--等指针相关操作予以重载的class template.** 所有STL容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。原生指针(native pointer)也是一种迭代器。\n\n**仿函数：**行为类似函数，可作为算法的某种策略。从实现角度来看，仿函数是一种重载了operator()的class 或者class template\n\n**适配器：**一种用来修饰容器或者仿函数或迭代器接口的东西。\n\n**空间配置器：**负责空间的配置与管理。从实现角度看，配置器是一个实现了动态空间配置、空间管理、空间释放的class tempalte.\n\n## 2、string容器\n\nC风格字符串(以空字符结尾的字符数组)太过复杂难于掌握，不适合大程序的开发，所以C++标准库定义了一种string类，定义在头文件<string>。\n\nString和c风格字符串对比：\n\n- char*是一个指针，String是一个类\n\n  string封装了char\\*，管理这个字符串，是一个char*型的容器。\n\n- String封装了很多实用的成员方法\n\n  查找find，拷贝copy，删除delete 替换replace，插入insert\n\n- 不用考虑内存释放和越界\n\n   string管理char*所分配的内存。每一次string的复制，取值都由string类负责维护，不用担心复制越界和取值越界等。\n\n## 3、vector容器\n\nvector的数据安排以及操作方式，与array非常相似，两者的唯一差别在于空间的运用的灵活性。Array是静态空间，一旦配置了就不能改变，要换大一点或者小一点的空间，可以，一切琐碎得由自己来，首先配置一块新的空间，然后将旧空间的数据搬往新空间，再释放原来的空间。Vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新元素。因此vector的运用对于内存的合理利用与运用的灵活性有很大的帮助，我们再也不必害怕空间不足而一开始就要求一个大块头的array了。\n\n所谓动态增加大小，并不是在原空间之后续接新空间(因为无法保证原空间之后尚有可配置的空间)，而是一块更大的内存空间，然后将原数据拷贝新空间，并释放原空间。因此，对vector的任何操作，一旦引起空间的重新配置，指向原vector的所有迭代器就都失效了。这是程序员容易犯的一个错误，务必小心。\n\n为了降低空间配置时的速度成本，vector实际配置的大小可能比客户端需求大一些，以备将来可能的扩充，这边是**容量**的概念。换句话说，**一个vector的容量永远大于或等于其大小，一旦容量等于大小，便是满载，下次再有新增元素，整个vector容器就得另觅居所。**\n\n## 4、deque容器\n\nVector容器是单向开口的连续内存空间，deque则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector容器也可以在头尾两端插入元素，但是在其头部操作效率奇差，无法被接受。\n\nDeque容器和vector容器最大的差异，一在于deque允许使用常数项时间对头端进行元素的插入和删除操作。二在于deque没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，换句话说，像vector那样，”旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque身上是不会发生的。也因此，deque没有必须要提供所谓的空间保留(reserve)功能.\n\n虽然deque容器也提供了Random Access Iterator,但是它的迭代器并不是普通的指针，其复杂度和vector不是一个量级，这当然影响各个运算的层面。因此，除非有必要，我们应该尽可能的使用vector，而不是deque。对deque进行的排序操作，为了最高效率，可将deque先完整的复制到一个vector中，对vector容器进行排序，再复制回deque.\n\n既然deque是分段连续内存空间，那么就必须有中央控制，维持整体连续的假象，数据结构的设计及迭代器的前进后退操作颇为繁琐。Deque代码的实现远比vector或list都多得多。\n\nDeque采取一块所谓的map(注意，不是STL的map容器)作为主控，这里所谓的map是一小块连续的内存空间，其中每一个元素(此处成为一个结点)都是一个指针，指向另一段连续性内存空间，称作缓冲区。缓冲区才是deque的存储空间的主体。\n\n![](06.png)\n\n\n\n## 5、stack容器\n\nstack是一种先进后出(First In Last Out,FILO)的数据结构，它只有一个出口，形式如图所示。stack容器允许新增元素，移除元素，取得栈顶元素，但是除了最顶端外，没有任何其他方法可以存取stack的其他元素。换言之，stack不允许有遍历行为。\n\n有元素推入栈的操作称为:push,将元素推出stack的操作称为pop.\n\nStack所有元素的进出都必须符合”先进后出”的条件，只有stack顶端的元素，才有机会被外界取用。Stack不提供遍历功能，也不提供迭代器。\n\n\n\n## 6、queue容器\n\nQueue是一种先进先出(First In First Out,FIFO)的数据结构，它有两个出口，queue容器允许从一端新增元素，从另一端移除元素。\n\nQueue所有元素的进出都必须符合”先进先出”的条件，只有queue的顶端元素，才有机会被外界取用。Queue不提供遍历功能，也不提供迭代器。\n\n## 7、list容器\n\n链表是一种物理[存储单元](http://baike.baidu.com/view/1223079.htm)上非连续、非顺序的[存储结构](http://baike.baidu.com/view/2820182.htm)，[数据元素](http://baike.baidu.com/view/38785.htm)的逻辑顺序是通过链表中的[指针](http://baike.baidu.com/view/159417.htm)链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储[数据元素](http://baike.baidu.com/view/38785.htm)的数据域，另一个是存储下一个结点地址的[指针](http://baike.baidu.com/view/159417.htm)域。\n\n相较于vector的连续线性空间，list就显得负责许多，它的好处是每次插入或者删除一个元素，就是配置或者释放一个元素的空间。因此，list对于空间的运用有绝对的精准，一点也不浪费。而且，对于任何位置的元素插入或元素的移除，list永远是常数时间。\n\nList和vector是两个最常被使用的容器。\n\nList容器是一个双向链表。\n\n- 采用动态存储分配，不会造成内存浪费和溢出\n\n- 链表执行插入和删除操作十分方便，修改指针即可，不需要移动大量元素\n\n- 链表灵活，但是空间和时间额外耗费较大\n\nList有一个重要的性质，插入操作和删除操作都不会造成原有list迭代器的失效。这在vector是不成立的，因为vector的插入操作可能造成记忆体重新配置，导致原有的迭代器全部失效，甚至List元素的删除，也只有被删除的那个元素的迭代器失效，其他迭代器不受任何影响。\n\n\n\n## 8、set/multiset容器\n\nSet的特性是。所有元素都会根据元素的键值自动被排序。Set的元素不像map那样可以同时拥有实值和键值，set的元素即是键值又是实值。Set不允许两个元素有相同的键值。\n\n我们可以通过set的迭代器改变set元素的值吗？不行，因为set元素值就是其键值，关系到set元素的排序规则。如果任意改变set元素值，会严重破坏set组织。换句话说，set的iterator是一种const_iterator.\n\nset拥有和list某些相同的性质，当对容器中的元素进行插入操作或者删除操作的时候，操作之前所有的迭代器，在操作完成之后依然有效，被删除的那个元素的迭代器必然是一个例外。\n\n\n\n## 9、map/multimap容器\n\nMap的特性是，所有元素都会根据元素的键值自动排序。Map所有的元素都是pair,同时拥有实值和键值，pair的第一元素被视为键值，第二元素被视为实值，map不允许两个元素有相同的键值。\n\n我们可以通过map的迭代器改变map的键值吗？答案是不行，因为map的键值关系到map元素的排列规则，任意改变map键值将会严重破坏map组织。如果想要修改元素的实值，那么是可以的。\n\nMap和list拥有相同的某些性质，当对它的容器元素进行新增操作或者删除操作时，操作之前的所有迭代器，在操作完成之后依然有效，当然被删除的那个元素的迭代器必然是个例外。\n\nMultimap和map的操作类似，唯一区别multimap键值可重复。\n\nMap和multimap都是以红黑树为底层实现机制。\n\n## 10、STL容器使用时机\n\n|              | vector   | deque    | list     | set    | multiset | map             | multimap      |\n| ------------ | -------- | -------- | -------- | ------ | -------- | --------------- | ------------- |\n| 典型内存结构 | 单端数组 | 双端数组 | 双向链表 | 二叉树 | 二叉树   | 二叉树          | 二叉树        |\n| 可随机存取   | 是       | 是       | 否       | 否     | 否       | 对key而言：不是 | 否            |\n| 元素搜寻速度 | 慢       | 慢       | 非常慢   | 快     | 快       | 对key而言：快   | 对key而言：快 |\n| 元素安插移除 | 尾端     | 头尾两端 | 任何位置 | -      | -        | -               | -             |\n\n- vector的使用场景：比如软件历史操作记录的存储，我们经常要查看历史记录，比如上一次的记录，上上次的记录，但却不会去删除记录，因为记录是事实的描述。\n\n- deque的使用场景：比如排队购票系统，对排队者的存储可以采用deque，支持头端的快速移除，尾端的快速添加。如果采用vector，则头端移除时，会移动大量的数据，速度慢。\n\n  ​     vector与deque的比较：\n\n  ​      一：vector.at()比deque.at()效率高，比如vector.at(0)是固定的，deque的开始位置却是不固定的。\n\n  ​    二：如果有大量释放操作的话，vector花的时间更少，这跟二者的内部实现有关。\n\n  ​    三：deque支持头部的快速插入与快速移除，这是deque的优点。\n\n- list的使用场景：比如公交车乘客的存储，随时可能有乘客下车，支持频繁的不确实位置元素的移除插入。\n\n- set的使用场景：比如对手机游戏的个人得分记录的存储，存储要求从高分到低分的顺序排列。 \n\n- map的使用场景：比如按ID号存储十万个用户，想要快速要通过ID查找对应的用户。二叉树的查找效率，这时就体现出来了。如果是vector容器，最坏的情况下可能要遍历完整个容器才能找到该用户。","slug":"C和C++语法总结","published":1,"updated":"2020-01-04T16:37:39.707Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck5454tuk0034zsv5ijho38pk","content":"<h1 id=\"一、C语言篇\"><a href=\"#一、C语言篇\" class=\"headerlink\" title=\"一、C语言篇\"></a>一、C语言篇</h1><h2 id=\"1、gcc-g\"><a href=\"#1、gcc-g\" class=\"headerlink\" title=\"1、gcc/g++\"></a>1、gcc/g++</h2><p><strong>1.1、为什么需要gcc/g++</strong></p>\n<p>编辑器(如vi、记事本)是指我用它来写程序的（编辑代码），而我们写的代码语句，电脑是不懂的，我们需要把它转成电脑能懂的语句，编译器就是这样的转化工具。就是说，<strong>我们用编辑器编写程序，由编译器编译后才可以运行！</strong></p>\n<p><strong>1.2、gcc编译器介绍</strong></p>\n<p>编译器是将易于编写、阅读和维护的高级计算机语言翻译为计算机能解读、运行的低级机器语言的程序。</p>\n<p>gcc（GNU Compiler Collection，GNU 编译器套件），是由 GNU 开发的编程语言编译器。gcc原本作为GNU操作系统的官方编译器，现已被大多数类Unix操作系统（如Linux、BSD、Mac OS X等）采纳为标准的编译器，gcc同样适用于微软的Windows。</p>\n<p>gcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。</p>\n<p>gcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。</p>\n<p><strong>1.3、编译命令</strong></p>\n<p>编译命令格式：</p>\n<p>gcc [-option1] … <filename></filename></p>\n<p>g++ [-option1] … <filename></filename></p>\n<ul>\n<li><p>命令、选项和源文件之间使用空格分隔</p>\n</li>\n<li><p>一行命令中可以有零个、一个或多个选项</p>\n</li>\n<li><p>文件名可以包含文件的绝对路径，也可以使用相对路径</p>\n</li>\n<li><p>如果命令中不包含输出可执行文件的文件名，可执行文件的文件名会自动生成一个默认名，Linux平台为<strong>a.out</strong>，Windows平台为<strong>a.exe</strong></p>\n</li>\n</ul>\n<p>gcc、g++编译常用选项说明：</p>\n<table>\n<thead>\n<tr>\n<th><strong>选项</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-o file</td>\n<td>指定生成的输出文件名为file</td>\n</tr>\n<tr>\n<td>-E</td>\n<td>只进行预处理</td>\n</tr>\n<tr>\n<td>-S(大写)</td>\n<td>只进行预处理和编译</td>\n</tr>\n<tr>\n<td>-c(小写)</td>\n<td>只进行预处理、编译和汇编</td>\n</tr>\n</tbody></table>\n<p><strong>1.4、gcc/g++的区别</strong></p>\n<ol>\n<li><p>gcc与g++都可以编译c代码和c++代码, 但是: 后缀为.c的，gcc会把它当做C程序, 而g++当做是C++程序; 后缀为.cpp的，两者都会认为是C++程序.</p>\n</li>\n<li><p>编译阶段，可以使用gcc/g++, g++会自动调用gcc。而链接阶段,可以用g++或者gcc -lstdc++</p>\n<p>,因为gcc命令不能自动和c++程序使用的库链接，通常用g++来完成。</p>\n</li>\n</ol>\n<h2 id=\"2、C-C-编译过程\"><a href=\"#2、C-C-编译过程\" class=\"headerlink\" title=\"2、C/C++编译过程\"></a>2、C/C++编译过程</h2><p><strong>2.1、编译步骤</strong></p>\n<p>C代码编译成可执行程序经过4步：</p>\n<p>1）预处理：宏定义展开、头文件展开、条件编译等，同时将代码中的注释删除，这里并不会检查语法</p>\n<p>2)   编译：检查语法，将预处理后文件编译生成汇编文件</p>\n<p>3）汇编：将汇编文件生成目标文件(二进制文件)</p>\n<p>4）链接：C语言写的程序是需要依赖各种库的，所以编译之后还需要把库链接到最终的可执行程序中去</p>\n<p><img src=\"file:////tmp/wps-yang-pc/ksohtml/wpsOWVxwY.jpg\" alt=\"img\"></p>\n<p><strong>2.2、gcc编译过程</strong></p>\n<p>1) 分步编译</p>\n<p>预处理：<code>gcc -E hello.c -o hello.i</code></p>\n<p>编  译：<code>gcc -S hello.i -o hello.s</code></p>\n<p>汇  编：<code>gcc -c hello.s -o hello.o</code></p>\n<p>链  接：<code>gcc   hello.o -o hello_elf</code></p>\n<table>\n<thead>\n<tr>\n<th><strong>选项</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-E</td>\n<td>只进行预处理</td>\n</tr>\n<tr>\n<td>-S(大写)</td>\n<td>只进行预处理和编译</td>\n</tr>\n<tr>\n<td>-c(小写)</td>\n<td>只进行预处理、编译和汇编</td>\n</tr>\n<tr>\n<td>-o file</td>\n<td>指定生成的输出文件名为 file</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong><em>\\</em>文件后缀**</strong></th>\n<th><strong><em>\\</em>含义**</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.c</td>\n<td>C 语言文件</td>\n</tr>\n<tr>\n<td>.i</td>\n<td>预处理后的 C 语言文件</td>\n</tr>\n<tr>\n<td>.s</td>\n<td>编译后的汇编文件</td>\n</tr>\n<tr>\n<td>.o</td>\n<td>编译后的目标文件</td>\n</tr>\n</tbody></table>\n<p>2) 一步编译</p>\n<p><code>gcc hello.c -o demo</code></p>\n<p><strong>2.3、查找程序所依赖的动态库</strong></p>\n<p>１）Linux平台下，<code>ldd</code>(“l”为字母) 可执行程序</p>\n<p>２）Windows平台下，需要相应软件(<code>Depends.exe</code>)</p>\n<h2 id=\"3、CPU内部结构与寄存器\"><a href=\"#3、CPU内部结构与寄存器\" class=\"headerlink\" title=\"3、CPU内部结构与寄存器\"></a>3、CPU内部结构与寄存器</h2><h3 id=\"3-1、CPU总线\"><a href=\"#3-1、CPU总线\" class=\"headerlink\" title=\"3.1、CPU总线\"></a>3.1、CPU总线</h3><p><strong>数据总线</strong></p>\n<p>（1） 是CPU与内存或其他器件之间的数据传送的通道。</p>\n<p>（2）数据总线的宽度决定了CPU和外界的数据传送速度。</p>\n<p>（3）每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据(即一个字节)。</p>\n<p>（4）数据总线是数据线数量之和。</p>\n<p>数据总线数据总线是CPU与存储器、CPU与I/O接口设备之间传送数据信息(各种指令数据信息)的总线，这些信号通过数据总线往返于CPU与存储器、CPU与I/O接口设备之间，因此，数据总线上的信息是双向传输的。</p>\n<p><strong>地址总线</strong></p>\n<p>（1）CPU是通过地址总线来指定存储单元的。</p>\n<p>（2）地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据（1024个内存单元）(1B)</p>\n<p>（3）地址总线是地址线数量之和。</p>\n<p>地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。它是单向的，只能从CPU传向外部存储器或I/O端口</p>\n<p>有个说法：64位系统装了64位操作系统，最大物理内存理论上=2的64次方；然而实际上地址总线只用到了35位，所以最大物理内存是32G大小</p>\n<p><strong>控制总线</strong></p>\n<p>（1）CPU通过控制总线对外部器件进行控制。</p>\n<p>（2）控制总线的宽度决定了CPU对外部器件的控制能力。</p>\n<p>（3）控制总线是控制线数量之和。</p>\n<p>控制总线，英文名称：ControlBus，简称：CB。控制总线主要用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和输入输出设备接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的</p>\n<h3 id=\"3-2、64位和32位系统区别\"><a href=\"#3-2、64位和32位系统区别\" class=\"headerlink\" title=\"3.2、64位和32位系统区别\"></a>3.2、64位和32位系统区别</h3><ul>\n<li><p>寄存器是CPU内部最基本的存储单元</p>\n</li>\n<li><p>CPU的主要组成包括了运算器和控制器。运算器是由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。</p>\n</li>\n<li><p>CPU位数=CPU中寄存器的位数=CPU能够一次并行处理的数据宽度（位数）=数据总线宽度</p>\n</li>\n<li><p><strong>CPU的位宽(位数)一般是以 min{ALU位宽、通用寄存器位宽、数据总线位宽}决定的</strong></p>\n</li>\n<li><p>CPU对外是通过总线(地址、控制、数据)来和外部设备交互的，总线的宽度是8位，同时CPU的寄存器也是8位，那么这个CPU就叫8位CPU</p>\n</li>\n<li><p>如果总线是32位，寄存器也是32位的，那么这个CPU就是32位CPU</p>\n</li>\n<li><p>有一种CPU内部的寄存器是32位的，但总线是16位，准32位CPU</p>\n</li>\n<li><p>所有的64位CPU兼容32位的指令，32位要兼容16位的指令，所以在64位的CPU上是可以识别32位的指令</p>\n</li>\n<li><p>在64位的CPU构架上运行了64位的软件操作系统，那么这个系统是64位</p>\n</li>\n<li><p>在64位的CPU构架上，运行了32位的软件操作系统，那么这个系统就是32位</p>\n</li>\n<li><p>64位的软件不能运行在32位的CPU之上</p>\n</li>\n</ul>\n<h3 id=\"3-3、寄存器、缓存、内存三者关系\"><a href=\"#3-3、寄存器、缓存、内存三者关系\" class=\"headerlink\" title=\"3.3、寄存器、缓存、内存三者关系\"></a>3.3、寄存器、缓存、内存三者关系</h3><ol>\n<li><p>寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。</p>\n</li>\n<li><p>内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。</p>\n</li>\n<li><p>寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。</p>\n</li>\n<li><p>Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache(L1 Cache)和二级Cache(L2 Cache)，L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。</p>\n</li>\n</ol>\n<p>总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。</p>\n<h2 id=\"4、内存、地址和指针\"><a href=\"#4、内存、地址和指针\" class=\"headerlink\" title=\"4、内存、地址和指针\"></a>4、内存、地址和指针</h2><h3 id=\"4-1、内存\"><a href=\"#4-1、内存\" class=\"headerlink\" title=\"4.1、内存\"></a>4.1、内存</h3><p>内存含义：</p>\n<ul>\n<li><p>存储器：计算机的组成中，用来存储程序和数据，辅助CPU进行运算处理的重要部分。</p>\n</li>\n<li><p>内存：内部存贮器，暂存程序/数据——掉电丢失 SRAM、DRAM、DDR、DDR2、DDR3。</p>\n</li>\n<li><p>外存：外部存储器，长时间保存程序/数据—掉电不丢ROM、ERRROM、FLASH（NAND、NOR）、硬盘、光盘。</p>\n</li>\n</ul>\n<p>内存是沟通CPU与硬盘的桥梁：</p>\n<ul>\n<li><p>暂存放CPU中的运算数据</p>\n</li>\n<li><p>暂存与硬盘等外部存储器交换的数据</p>\n</li>\n</ul>\n<h3 id=\"4-2、物理存储器和存储地址空间\"><a href=\"#4-2、物理存储器和存储地址空间\" class=\"headerlink\" title=\"4.2、物理存储器和存储地址空间\"></a>4.2、物理存储器和存储地址空间</h3><p>有关内存的两个概念：物理存储器和存储地址空间。</p>\n<p>物理存储器：实际存在的具体存储器芯片。</p>\n<ul>\n<li><p>主板上装插的内存条</p>\n</li>\n<li><p>显示卡上的显示RAM芯片</p>\n</li>\n<li><p>各种适配卡上的RAM芯片和ROM芯片</p>\n</li>\n</ul>\n<p>存储地址空间：对存储器编码的范围。我们在软件上常说的内存是指这一层含义。</p>\n<ul>\n<li><p>编码：对每个物理存储单元（一个字节）分配一个号码</p>\n</li>\n<li><p>寻址：可以根据分配的号码找到相应的存储单元，完成数据的读写</p>\n</li>\n</ul>\n<h3 id=\"4-3、内存地址\"><a href=\"#4-3、内存地址\" class=\"headerlink\" title=\"4.3、内存地址\"></a>4.3、内存地址</h3><ul>\n<li><p>将内存抽象成一个很大的一维字符数组。</p>\n</li>\n<li><p>编码就是对内存的每一个字节分配一个32位或64位的编号（与32位或者64位处理器相关）。</p>\n</li>\n<li><p>这个内存编号我们称之为内存地址。</p>\n</li>\n</ul>\n<p>内存中的每一个数据都会分配相应的地址：</p>\n<ul>\n<li><p>char:占一个字节分配一个地址</p>\n</li>\n<li><p>int: 占四个字节分配四个地址</p>\n</li>\n<li><p>float、struct、函数、数组等</p>\n</li>\n</ul>\n<h3 id=\"4-4、指针和指针变量\"><a href=\"#4-4、指针和指针变量\" class=\"headerlink\" title=\"4.4、指针和指针变量\"></a>4.4、指针和指针变量</h3><ul>\n<li><p>内存区的每一个字节都有一个编号，这就是“地址”。</p>\n</li>\n<li><p>如果在程序中定义了一个变量，在对程序进行编译或运行时，系统就会给这个变量分配内存单元，并确定它的内存地址(编号)</p>\n</li>\n<li><p>指针的实质就是内存“地址”。指针就是地址，地址就是指针。</p>\n</li>\n<li><p>指针是内存单元的编号，指针变量是存放地址的变量。</p>\n</li>\n<li><p><strong>通常我们叙述时会把指针变量简称为指针，实际他们含义并不一样</strong></p>\n</li>\n</ul>\n<h2 id=\"5、存储类型\"><a href=\"#5、存储类型\" class=\"headerlink\" title=\"5、存储类型\"></a>5、存储类型</h2><h3 id=\"5-1、auto\"><a href=\"#5-1、auto\" class=\"headerlink\" title=\"5.1、auto\"></a>5.1、auto</h3><ol>\n<li>普通局部变量，自动存储，该对象会自动创建和销毁，调用函数时分配内存，函数结束时释放内存。只在{}内有效，存放在堆栈中一般省略auto,  不会被默认初始化，初值不随机</li>\n<li>全局变量，<strong>不允许声明为auto变量</strong>， register不适用于全局变量，生命周期由定义到程序运行结束，没有初始化会<strong>自动赋值0或空字符</strong>。全局变量属于整个程序，不同文件中不能有同名的全局变量，通过<strong>extern</strong>在其他文件中引用使用</li>\n</ol>\n<h3 id=\"5-2、static\"><a href=\"#5-2、static\" class=\"headerlink\" title=\"5.2、static\"></a>5.2、static</h3><ol>\n<li>静态局部变量，生命周期由定义到程序运行结束，在编译时赋初值，<strong>只初始化一次，没有初始化会自动赋值0或空字符</strong>。只在当前{}内有效</li>\n<li>静态全局变量，生命周期由定义到程序运行结束，在编译时赋初值，只初始化一次，没有初始化会自动赋值0或空字符。从定义到文件结尾起作用，在一个程序中的其他文件中可以定义同名的静态全局变量，因为作用于不冲突。</li>\n</ol>\n<h3 id=\"5-3、extern\"><a href=\"#5-3、extern\" class=\"headerlink\" title=\"5.3、extern\"></a>5.3、extern</h3><ol>\n<li><p>外部变量声明，是指这是一个已在别的地方定义过的对象，这里只是对变量的一次重复引用，不会产生新的变量。</p>\n</li>\n<li><p>使用extern时，注意不能重复定义，否则编译报错</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token comment\" spellcheck=\"true\">//    程序文件一：</span>\n    <span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> a <span class=\"token operator\">=</span> <span class=\"token number\">10</span><span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">//编译警告，extern的变量最好不要初始化</span>\n<span class=\"token comment\" spellcheck=\"true\">//    程序文件二：</span>\n    <span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> a <span class=\"token operator\">=</span> <span class=\"token number\">20</span><span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">//重复定义，应改为extern int a;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n<li><p>如果我们希望该外部变量只能在本文件内使用，而不能被其他文件引用可以在外部变量定义时加static声明。防止别人写的模块误用。</p>\n</li>\n<li><p>在函数外部定义的全局变量，作用域开始于变量定义，结束于程序文件的结束。我们可以extern来声明外部变量来扩展它的作用域。同一个文件内，extern声明之后就可以作用域扩大到声明处到文件结束。比如在一个函数之后定义外部变量a，之后的函数可以使用该变量，但是之前的函数不能使用，加extern可以解决。</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span>\n\n<span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> g1<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">extern</span> <span class=\"token keyword\">int</span> g2<span class=\"token punctuation\">;</span>\n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%d,%d\\n\"</span><span class=\"token punctuation\">,</span> g1<span class=\"token punctuation\">,</span>g2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">int</span> g1 <span class=\"token operator\">=</span> <span class=\"token number\">77</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">int</span> g2 <span class=\"token operator\">=</span> <span class=\"token number\">88</span><span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<ol start=\"5\">\n<li>多个文件时，可以在未定义该外部变量的文件内做extern声明即可以使用。但是需要注意可能执行一个文件时改变了该全局变量的值，影响其他文件的调用。编译时遇到extern，会先在文件内找是否定义了该外部变量。如果未找到则在链接时在其他文件中找。</li>\n</ol>\n<h3 id=\"5-4、register\"><a href=\"#5-4、register\" class=\"headerlink\" title=\"5.4、register\"></a>5.4、register</h3><ol>\n<li><p>寄存器变量，请求编译器将这个变量保存在CPU的寄存器中，从而加快程序的运行.只是建议CPU这样做，非强制,声明变量为register,编译器并不一定会将它处理为寄存器变量</p>\n</li>\n<li><p>动态和静态变量都是存放在内存中，程序中遇到该值时用控制器发指令将变量的值送到运算器中，需要存数再保存到内存中。如果频繁使用一个变量，比如一个函数体内的多次循环每次都引用该局部变量，我们则可以把局部变量的值放到CPU的寄存器中，叫寄存器变量。不需要多次到内存中存取提高效率。</p>\n</li>\n<li><p>但是只能局部自动变量和形参可以做寄存器变量。在函数调用时占用一些寄存器，函数结束时释放。不同系统对register要求也不一样，比如对定义register变量个数，数据类型等限制，有的默认为自动变量处理。所以在程序一般也不用。</p>\n</li>\n<li><p>register是不能取址的。比如 <code>int i</code>；(自动为auto)<code>int *p=&amp;i;</code>是对的， 但<code>register int j; int *p = &amp;j;</code>是错的，因为无法对寄存器的定址。</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;time.h></span></span>\n\n<span class=\"token macro property\">#<span class=\"token directive keyword\">define</span> TIME 1000000000</span>\n\n<span class=\"token keyword\">int</span> m<span class=\"token punctuation\">,</span> n <span class=\"token operator\">=</span> TIME<span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">/* 全局变量 */</span>\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    time_t start<span class=\"token punctuation\">,</span> stop<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">register</span> <span class=\"token keyword\">int</span> a<span class=\"token punctuation\">,</span> b <span class=\"token operator\">=</span> TIME<span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">/* 寄存器变量 */</span>\n    <span class=\"token keyword\">int</span> x<span class=\"token punctuation\">,</span> y <span class=\"token operator\">=</span> TIME<span class=\"token punctuation\">;</span>          <span class=\"token comment\" spellcheck=\"true\">/* 一般变量   */</span>\n\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>a <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> a <span class=\"token operator\">&lt;</span> b<span class=\"token punctuation\">;</span> a<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>stop<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"寄存器变量用时: %d 秒\\n\"</span><span class=\"token punctuation\">,</span> stop <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>x <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> x <span class=\"token operator\">&lt;</span> y<span class=\"token punctuation\">;</span> x<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>stop<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"一般变量用时: %d 秒\\n\"</span><span class=\"token punctuation\">,</span> stop <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>m <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> m <span class=\"token operator\">&lt;</span> n<span class=\"token punctuation\">;</span> m<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">time</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>stop<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"全局变量用时: %d 秒\\n\"</span><span class=\"token punctuation\">,</span> stop <span class=\"token operator\">-</span> start<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ol>\n<h3 id=\"5-5、volatile\"><a href=\"#5-5、volatile\" class=\"headerlink\" title=\"5.5、volatile\"></a>5.5、volatile</h3><pre class=\"line-numbers language-c\"><code class=\"language-c\">    程序在使用变量时<span class=\"token punctuation\">,</span> 特别是连续多次使用变量时<span class=\"token punctuation\">,</span> 一般是载入寄存器<span class=\"token punctuation\">,</span> 直接从寄存器存取<span class=\"token punctuation\">,</span> 之后再还回内存<span class=\"token punctuation\">;</span>但如果此变量在返回内存时<span class=\"token punctuation\">,</span> 假如内存中的值已经改变了<span class=\"token punctuation\">(</span>从外部修改了<span class=\"token punctuation\">)</span>怎么办<span class=\"token operator\">?</span>\n为了避免这种情况的发生<span class=\"token punctuation\">,</span> 可以用 <span class=\"token keyword\">volatile</span> 说明此变量<span class=\"token punctuation\">,</span> 以保证变量的每次使用都是直接从内存存取<span class=\"token punctuation\">.</span>\n但这样肯定会影响效率<span class=\"token punctuation\">,</span> 幸好它并不常用<span class=\"token punctuation\">.</span>\n\n另外<span class=\"token punctuation\">:</span> 如果 <span class=\"token keyword\">const</span> <span class=\"token keyword\">volatile</span> 同时使用<span class=\"token punctuation\">,</span> 这表示此变量只接受外部的修改<span class=\"token punctuation\">.</span>\n\n<span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span>\n\n<span class=\"token keyword\">volatile</span> <span class=\"token keyword\">int</span> num <span class=\"token operator\">=</span> <span class=\"token number\">123</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>    \n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%d\\n\"</span><span class=\"token punctuation\">,</span> num<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">getchar</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h3 id=\"5-6、总结\"><a href=\"#5-6、总结\" class=\"headerlink\" title=\"5.6、总结\"></a>5.6、总结</h3><table>\n<thead>\n<tr>\n<th>关键字</th>\n<th>类型</th>\n<th>生命周期</th>\n<th>作用域</th>\n<th>修饰对象</th>\n<th>所属区</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>auto[可省略]</td>\n<td>普通局部变量</td>\n<td>定义到{}运行结束</td>\n<td>｛｝</td>\n<td>变量</td>\n<td>栈区</td>\n</tr>\n<tr>\n<td>static</td>\n<td>静态局部变量</td>\n<td>定义到程序运行结束</td>\n<td>｛｝</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td></td>\n<td>全局变量</td>\n<td>定义到程序运行结束</td>\n<td>定义到文件结尾</td>\n<td></td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>extern</td>\n<td>全局变量</td>\n<td>定义到程序运行结束</td>\n<td>声明处到文件结尾</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>static</td>\n<td>全局变量</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>register</td>\n<td>寄存器变量</td>\n<td>定义到{}运行结束</td>\n<td>｛｝</td>\n<td>变量</td>\n<td>运行时存储在CPU寄存器</td>\n</tr>\n<tr>\n<td>extern</td>\n<td>函数</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td></td>\n<td>代码区</td>\n</tr>\n<tr>\n<td>static</td>\n<td>函数</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td></td>\n<td>代码区</td>\n</tr>\n</tbody></table>\n<h2 id=\"6、内存分区\"><a href=\"#6、内存分区\" class=\"headerlink\" title=\"6、内存分区\"></a>6、内存分区</h2><p>C代码经过预处理、编译、汇编、链接4步后生成一个可执行程序。在 Linux 下，程序是一个普通的可执行文件，以下列出一个二进制可执行文件的基本情况：</p>\n<p><img src=\"01.png\" alt></p>\n<p>通过上图可以得知，在没有运行程序前，也就是说程序没有加载到内存前，可执行程序内部已经分好3段信息，分别为代码区（text）、数据区（data）和未初始化数据区（bss）3 个部分（有些人直接把data和bss合起来叫做静态区或全局区）。</p>\n<ul>\n<li><p><strong>代码区</strong></p>\n<p>存放 CPU 执行的机器指令。通常代码区是<strong>可共享</strong>的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区通常是<strong>只读的</strong>，使其只读的原因是防止程序意外地修改了它的指令。另外，代码区还规划了局部变量的相关信息。</p>\n</li>\n<li><p><strong>全局初始化数据区/静态数据区（data段）</strong></p>\n<p>该区包含了在程序中明确被初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。</p>\n</li>\n<li><p><strong>未初始化数据区（又叫 bss 区）</strong></p>\n<p>存入的是全局未初始化变量和未初始化静态变量。未初始化数据区的数据在程序开始执行之前被内核初始化为 0 或者空（NULL）。</p>\n</li>\n</ul>\n<p>程序在加载到内存前，代码区和全局区(data和bss)的大小就是固定的，程序运行期间不能改变。然后，运行可执行程序，系统把程序加载到内存，除了根据可执行程序的信息分出代码区（text）、数据区（data）和未初始化数据区（bss）之外，还额外增加了栈区、堆区。</p>\n<p><img src=\"02.png\" alt></p>\n<ul>\n<li><p>代码区（text segment）</p>\n<p>加载的是可执行文件代码段，所有的可执行代码都加载到代码区，这块内存是不可以在运行期间修改的。</p>\n</li>\n<li><p>只读数据区（文字常量区 RO data）</p>\n<p>只读数区是程序使用的一些不会被更改的数据。一般是const修饰的变量以及程序中使用的文字常量。</p>\n</li>\n<li><p>已初始化数据区 （RW data）</p>\n<p>加载的是可执行文件数据段，存储于数据段（全局初始化，静态初始化数据）的数据的生存周期为整个程序运行过程。</p>\n</li>\n<li><p>未初始化数据区（BSS）</p>\n<p>加载的是可执行文件BSS段，位置可以分开亦可以紧靠数据段，存储于数据段的数据（全局未初始化，静态未初始化数据）的生存周期为整个程序运行过程。</p>\n</li>\n<li><p>堆区（heap）</p>\n<p>堆是一个大容器，它的容量要远远大于栈，但没有栈那样先进后出的顺序。用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收。</p>\n</li>\n<li><p>栈区（stack）</p>\n<p>栈是一种先进后出的内存结构，由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。</p>\n</li>\n</ul>\n<h2 id=\"7、结构体字节对齐\"><a href=\"#7、结构体字节对齐\" class=\"headerlink\" title=\"7、结构体字节对齐\"></a>7、结构体字节对齐</h2><p><strong>7.1、内存对齐原因</strong></p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token keyword\">struct</span> data\n<span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">char</span> c<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">int</span> i<span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%d\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">struct</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">// 5还是 8?</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>在Linux 32位架构下，假设变量stu存放在内存中的起始地址为0x00，那么c的起始地址为0x00、i的起始地址为0x01，变量stu共占用了5个字节：</p>\n<ul>\n<li><p>对变量c访问：CPU只需要一个读周期</p>\n</li>\n<li><p>变量i访问：</p>\n</li>\n</ul>\n<ol>\n<li><p>首先CPU用一个读周期，从0x00处读取了4个字节(32位架构)，然后将0x01-0x03的3个字节暂存。</p>\n</li>\n<li><p>再花一个读周期读取了从0x04-0x07的4字节数据，将0x04这个字节与刚刚暂存的3个字节进行拼接从而读取到成员变量i的值。</p>\n</li>\n<li><p>读取一个成员变量i，CPU却花费了2个读周期。</p>\n</li>\n</ol>\n<p>如果数据成员i的起始地址被放在了0x04处</p>\n<ul>\n<li><p>读取c成员，花费周期为1  </p>\n</li>\n<li><p>读取i所花费的周期也变成了1</p>\n</li>\n<li><p>引入字节对齐可以避免读取效率的下降，同时也浪费了3个字节的空间(0x01-0x03)</p>\n<p><strong>结构体内部成员对齐是为了实现用空间换取时间。</strong></p>\n</li>\n</ul>\n<p><strong>7.2、内存对齐原则</strong></p>\n<ul>\n<li><p>原则1：数据成员的对齐规则</p>\n<p>1) 最大对齐单位以CPU架构对齐，如Linux 32位最大以4字节对齐，Linux 64位最大以8字节对齐，vs(32位、64位)最大对齐单位为8字节</p>\n<p>2) 需要和<strong>结构体的最大成员</strong>和CPU架构(32位或64位)对比，取小的作为对齐单位</p>\n<p>3) 字节对齐也可以通过程序控制，采用指令：</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(xx)   </span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(1)     </span><span class=\"token comment\" spellcheck=\"true\">//1字节对齐</span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(2)     </span><span class=\"token comment\" spellcheck=\"true\">//2字节对齐</span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(4)     </span><span class=\"token comment\" spellcheck=\"true\">//4字节对齐</span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(8)     </span><span class=\"token comment\" spellcheck=\"true\">//8字节对齐</span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">pragma</span> pack(16)    </span><span class=\"token comment\" spellcheck=\"true\">//16字节对齐</span>\n\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n</li>\n</ul>\n<p>  #include &lt;stdio.h&gt;</p>\n<p>  #include &lt;stdlib.h&gt;</p>\n<p>  #pragma pack(2)</p>\n<p>  typedef struct<br>  {<br>    int aa1; //4个字节对齐 1111<br>    char bb1;//1个字节对齐 1<br>    short cc1;//2个字节对齐 011<br>    char dd1; //1个字节对齐 1<br>    } testlength;</p>\n<p>  int length = sizeof(testlength); //2个字节对齐，length = 10</p>\n<p>  int main(){<br>      printf(“length=%d\\n”, length); // length=10<br>  }</p>\n<pre><code>\n  尽管通过pragma pack(xx)可以指定字节对齐单位，但需要和结构体的最大成员、CPU架构(32位或64位)对比，取最小的作为对齐单位。\n\n- 原则2：数据成员的偏移起点\n\n  结构体（struct）的数据成员，第一个数据成员放在偏移量为0的地方，以后每个数据成员存放在偏移量为该数据成员类型大小的整数倍的地方（比如int在32位机器为４字节，则要从4的整数倍地址开始存储）\n\n- 原则3：收尾工作 \n\n  结构体的总大小，也就是sizeof的结果，必须是对齐单位的整数倍，不足的要补齐。\n\n\n\n## 8、typedef与define的区别\n\n- typedef为C语言的关键字，作用是为一种数据类型(基本类型或自定义数据类型)定义一个新名字，不能创建新类型。\n\n- 与#define不同，typedef仅限于数据类型，而不是能是表达式或具体的值\n\n- define发生在预处理，typedef发生在编译阶段\n\n\n\n## 9、C语言文本操作的区别\n\n### 9.1 二进制文件和文本文件\n\n\n\n- b是二进制模式的意思，b只是在Windows有效，在Linux用r和rb的结果是一样的\n\n- Unix和Linux下所有的文本文件行都是\\n结尾，而Windows所有的文本文件行都是\\r\\n结尾\n\n- 在Windows平台下，以“文本”方式打开文件，不加b：\n\n  - 当读取文件的时候，系统会将所有的 &quot;\\r\\n&quot; 转换成 &quot;\\n&quot;\n\n  - 当写入文件的时候，系统会将 &quot;\\n&quot; 转换成 &quot;\\r\\n&quot; 写入 \n\n  - 以&quot;二进制&quot;方式打开文件，则读\\写都不会进行这样的转换\n\n- 在Unix/Linux平台下，“文本”与“二进制”模式没有区别，&quot;\\r\\n&quot; 作为两个字符原样输入输出\n\n### 9.2 文本结尾\n\n在C语言中，EOF表示文件结束符(end of file)。在while循环中以EOF作为文件结束标志，这种以EOF作为文件结束标志的文件，必须是文本文件。在文本文件中，数据都是以字符的ASCII代码值的形式存放。我们知道，ASCII代码值的范围是0~127，不可能出现-1，因此可以用EOF作为文件结束标志。\n\n`#define EOF (-1)`\n\n当把数据以二进制形式存放到文件中时，就会有-1值的出现，因此不能采用EOF作为二进制文件的结束标志。为解决这一个问题，ANSI C提供一个feof函数，用来判断文件是否结束。feof函数既可用以判断二进制文件又可用以判断文本文件。\n\n```c\n#include &lt;stdio.h&gt;\nint feof(FILE * stream);\n功能：检测是否读取到了文件结尾。判断的是最后一次“读操作的内容”，不是当前位置内容(上一个内容)。\n参数：\n    stream：文件指针\n返回值：\n    非0值：已经到文件结尾\n    0：没有到文件结尾</code></pre><h2 id=\"10、void\"><a href=\"#10、void\" class=\"headerlink\" title=\"10、void\"></a>10、void</h2><p><strong>void的作用</strong></p>\n<ul>\n<li>对函数参数的限定：当不需要传入参数时，即 <code>function (void);</code></li>\n<li>对函数返回值的限定：当函数没有返回值时，即 <code>void function(void);</code></li>\n</ul>\n<p><strong>void指针的作用</strong></p>\n<p>（1）void指针可以指向任意的数据类型，即任意类型的指针可以赋值给void指针</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token keyword\">int</span> <span class=\"token operator\">*</span>a<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>p<span class=\"token punctuation\">;</span>\np<span class=\"token operator\">=</span>a<span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p>如果void指针赋值给其他类型，则需要强制转换；<code>a=（int *）p;</code></p>\n<p>（2）在ANSI C标准中不允许对void指针进行算术运算，因为没有特定的数据类型，即在内存中不知道移动多少个字节；而在GNU标准中，认为void指针和char指针等同。</p>\n<p><strong>应用</strong></p>\n<p>（1）void指针一般用于应用的底层，比如malloc函数的返回类型是void指针，需要再强制转换； </p>\n<p>（2）文件句柄HANDLE也是void指针类型，这也是句柄和指针的区别； </p>\n<p>（3）内存操作函数的原型也需要void指针限定传入参数：</p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span> <span class=\"token function\">memcpy</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>dest<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>src<span class=\"token punctuation\">,</span> size_t len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">void</span> <span class=\"token operator\">*</span> <span class=\"token function\">memset</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>buffer<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> c<span class=\"token punctuation\">,</span> size_t num <span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span></span></code></pre>\n<p>（4）面向对象函数中底层对基类的抽象。</p>\n<h2 id=\"11、数据类型的本质\"><a href=\"#11、数据类型的本质\" class=\"headerlink\" title=\"11、数据类型的本质\"></a>11、数据类型的本质</h2><ul>\n<li><p>数据类型可理解为创建变量的模具：是固定内存大小的别名。</p>\n</li>\n<li><p>数据类型的作用：编译器预算对象（变量）分配的内存空间大小。</p>\n</li>\n</ul>\n<h2 id=\"12、变量的本质\"><a href=\"#12、变量的本质\" class=\"headerlink\" title=\"12、变量的本质\"></a>12、变量的本质</h2><p>变量的本质：一段连续内存空间的别名。</p>\n<pre><code>1）程序通过变量来申请和命名内存空间 int a = 0\n\n2）通过变量名访问内存空间\n\n3）不是向变量读写数据，而是向变量所代表的内存空间中读写数据</code></pre><h2 id=\"13、数组与指针的关系\"><a href=\"#13、数组与指针的关系\" class=\"headerlink\" title=\"13、数组与指针的关系\"></a>13、数组与指针的关系</h2><p>数组不是指针，数组名也只有在表达式中才会被当成一个指针常量。数组名在表达式中使用的时候，编译器才会产生一个指针常量。</p>\n<ul>\n<li><p><code>p[i]</code>这种写法只不过是<code>*(p + i)</code>的简便写法。实际上，至少对于编译器来说，[]这样的运算符完全可以不存在。[]运算符是为了方便人们读写而引入的，是一种语法糖。</p>\n</li>\n<li><p>当数组名作为sizeof操作符的操作数的时候，此时sizeof返回的是整个数组的长度，而不是指针数组指针的长度。</p>\n</li>\n<li><p>当数组名作为 &amp; 操作符的操作数的时候，此时返回的是一个指向数组的指针，而不是指向某个数组元素的指针常量。</p>\n</li>\n<li><p>二级指针是指向指针的指针，而指针数组则是元素类型为指针的数组。虽然它们是不一样的，但是在表达式中，它们是等效的。</p>\n</li>\n</ul>\n<h2 id=\"14、字节序-大端、小端\"><a href=\"#14、字节序-大端、小端\" class=\"headerlink\" title=\"14、字节序(大端、小端)\"></a>14、字节序(大端、小端)</h2><p><strong>14.1 大端和小端</strong></p>\n<p>计算机的内存最小单位是字节。字节序是指多字节(大于1字节)数据的存储顺序，在设计计算机系统的时候，有两种处理内存中数据的方法：大端格式、小端格式。</p>\n<ul>\n<li><p>小端格式(Little-Endian)：将低位字节数据存储在低地址。X86和ARM都是小端对齐。</p>\n</li>\n<li><p>大端格式(Big-Endian)：将高位字节数据存储在低地址。很多Unix服务器的CPU是大端对齐的、网络上数据是以大端对齐。</p>\n</li>\n</ul>\n<p><img src=\"03.png\" alt></p>\n<p>对于整形 0x12345678，它在大端格式和小端格式的系统中，分别如下图所示的方式存放：</p>\n<p><img src=\"04.png\" alt></p>\n<p><strong>14.2 网络字节序和主机字节序</strong></p>\n<p>网络字节顺序NBO(Network Byte Order)</p>\n<pre><code>在网络上使用统一的大端模式，低字节存储在高地址，高字节存储在低地址。</code></pre><p>主机字节序顺序HBO(Host Byte Order)</p>\n<pre><code>不同的机器HBO不相同，与CPU设计相关，数据的顺序是由CPU决定的，而与操作系统无关。</code></pre><p>处理器 |操作系统  |字节排序|</p>\n<p>Alpha    全部    Little endian<br>HP-PA    NT    Little endian<br>HP-PA    UNIX    Big endian<br>Intelx86    全部    Little endian &lt;—–x86系统是小端字节序系统<br>Motorola680x()    全部    Big endian<br>MIPS    NT    Little endian<br>MIPS    UNIX    Big endian<br>PowerPC    NT    Little endian<br>PowerPC    非NT    Big endian  &lt;—–PPC系统是大端字节序系统<br>RS/6000    UNIX    Big endian<br>SPARC    UNIX    Big endian<br>IXP1200 ARM核心    全部    Little endian </p>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\">相关函数：\nhtons 把<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">short</span>类型从主机序转换到网络序\nhtonl 把<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">long</span>类型从主机序转换到网络序\nntohs 把<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">short</span>类型从网络序转换到主机序\nntohl 把<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">long</span>类型从网络序转换到主机序\n\n头文件：#include <span class=\"token operator\">&lt;</span>netinet<span class=\"token operator\">/</span>in<span class=\"token punctuation\">.</span>h<span class=\"token operator\">></span>\n定义函数：<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">short</span> <span class=\"token function\">ntohs</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">short</span> netshort<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n函数说明：<span class=\"token function\">ntohs</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>用来将参数指定的<span class=\"token number\">16</span> 位netshort 转换成主机字符顺序<span class=\"token punctuation\">.</span>\n返回值：返回对应的主机顺序<span class=\"token punctuation\">.</span>\n范例：参考<span class=\"token function\">getservent</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;stdio.h></span></span>\n<span class=\"token macro property\">#<span class=\"token directive keyword\">include</span> <span class=\"token string\">&lt;winsock.h></span> </span><span class=\"token comment\" spellcheck=\"true\">// windows使用winsock.h</span>\n\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token comment\" spellcheck=\"true\">//左边是高位，右边是低位，高位放高地址，低位放低地址</span>\n    <span class=\"token keyword\">int</span> a <span class=\"token operator\">=</span> <span class=\"token number\">0x11223344</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> <span class=\"token operator\">*</span>p <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">)</span><span class=\"token operator\">&amp;</span>a<span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">int</span> i<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> <span class=\"token number\">4</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">{</span>\n        <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%x\\n\"</span><span class=\"token punctuation\">,</span> p<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n\n    u_long b <span class=\"token operator\">=</span> <span class=\"token function\">htonl</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> <span class=\"token operator\">*</span>q <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">)</span><span class=\"token operator\">&amp;</span>b<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">&lt;</span><span class=\"token number\">4</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n        <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%x\\n\"</span><span class=\"token punctuation\">,</span> q<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\" spellcheck=\"true\">//  gcc hello.c -lwsock32 -o hello</span>\n<span class=\"token comment\" spellcheck=\"true\">// 编译时添加-lwsock32，不然会报错undefined reference to `htonl@4'</span>\n<span class=\"token comment\" spellcheck=\"true\">// 在编译socket程序的时候，一定要加上-l wsock32选项，因为mingw默认没有包含windows库</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"15、数组指针\"><a href=\"#15、数组指针\" class=\"headerlink\" title=\"15、数组指针\"></a>15、数组指针</h2><pre class=\"line-numbers language-c\"><code class=\"language-c\"><span class=\"token comment\" spellcheck=\"true\">// 1) 先定义数组类型，再根据类型定义指针变量</span>\n<span class=\"token keyword\">typedef</span> <span class=\"token keyword\">int</span> A<span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\nA <span class=\"token operator\">*</span>p <span class=\"token operator\">=</span> <span class=\"token constant\">NULL</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token comment\" spellcheck=\"true\">// 2) 先定义数组指针类型，根据类型定义指针变量</span>\n<span class=\"token keyword\">typedef</span> <span class=\"token keyword\">int</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>P<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">//第一个()代表指针，第二个[]代表数组</span>\nP q<span class=\"token punctuation\">;</span> <span class=\"token comment\" spellcheck=\"true\">//数据组指针变量</span>\n\n<span class=\"token comment\" spellcheck=\"true\">// 3) 直接定义数组指针变量</span>\n<span class=\"token keyword\">int</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>q<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"16、深拷贝和浅拷贝\"><a href=\"#16、深拷贝和浅拷贝\" class=\"headerlink\" title=\"16、深拷贝和浅拷贝\"></a>16、深拷贝和浅拷贝</h2><p><strong>结构体</strong>:</p>\n<p>浅拷贝  不同结构体成员指针变量指向同一块内存</p>\n<p>深拷贝  不同结构体成员指针变量指向不同的内存</p>\n<p><strong>类</strong>:</p>\n<p>浅拷贝 类中有动态分配的空间的指针指向相同</p>\n<p>深拷贝 类中有动态分配的空间的指针指向不同的内存空间</p>\n<h2 id=\"17、-include-lt-gt-与-include-“”的区别\"><a href=\"#17、-include-lt-gt-与-include-“”的区别\" class=\"headerlink\" title=\"17、#include&lt; &gt; 与 #include “”的区别\"></a>17、#include&lt; &gt; 与 #include “”的区别</h2><ul>\n<li><p>“” 表示系统先在file1.c所在的当前目录找file1.h，如果找不到，再按系统指定的目录检索。</p>\n</li>\n<li><p>&lt; &gt; 表示系统直接按系统指定的目录检索。</p>\n</li>\n</ul>\n<p>注意：</p>\n<ul>\n<li>#include &lt;&gt;常用于包含库函数的头文件</li>\n<li>#include “”常用于包含自定义的头文件</li>\n<li>理论上#include可以包含任意格式的文件(.c .h等) ，但我们一般用于头文件的包含。</li>\n</ul>\n<h2 id=\"18、静态库和动态库\"><a href=\"#18、静态库和动态库\" class=\"headerlink\" title=\"18、静态库和动态库\"></a>18、静态库和动态库</h2><p><strong>18.1、静态库优缺点</strong></p>\n<ul>\n<li><p>静态库在程序的链接阶段被复制到了程序中，和程序运行的时候没有关系；</p>\n</li>\n<li><p>程序在运行时与函数库再无瓜葛，移植方便；</p>\n</li>\n<li><p>浪费空间和资源，所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。</p>\n</li>\n</ul>\n<p><strong>18.2、动态库</strong></p>\n<p>要解决空间浪费和更新困难这两个问题，最简单的办法就是把程序的模块相互分割开来，形成独立的文件，而不是将他们静态的链接在一起。</p>\n<p>简单地讲，就是不对哪些组成程序的目标程序进行链接，等程序运行的时候才进行链接。也就是说，把整个链接过程推迟到了运行时再进行，这就是动态链接的基本思想。</p>\n<p><strong>18.3、动态库的lib文件和静态库的lib文件的区别</strong></p>\n<p>在使用动态库的时候，往往提供两个文件：一个引入库（.lib）文件（也称“导入库文件”）和一个DLL（.dll）文件。 </p>\n<p>虽然引入库的后缀名也是“lib”，但是，动态库的引入库文件和静态库文件有着本质的区别，对一个DLL文件来说，其引入库文件（.lib）包含该DLL导出的函数和变量的符号名，而.dll文件包含该DLL实际的函数和数据。</p>\n<p>在使用动态库的情况下，在编译链接可执行文件时，只需要链接该DLL的引入库文件，该DLL中的函数代码和数据并不复制到可执行文件，直到可执行程序运行时，才去加载所需的DLL，将该DLL映射到进程的地址空间中，然后访问DLL中导出的函数。</p>\n<h1 id=\"二、C-篇\"><a href=\"#二、C-篇\" class=\"headerlink\" title=\"二、C++篇\"></a>二、C++篇</h1><h2 id=\"1、c语言和c-语言的关系\"><a href=\"#1、c语言和c-语言的关系\" class=\"headerlink\" title=\"1、c语言和c++语言的关系\"></a>1、c语言和c++语言的关系</h2><p>“c++”中的++来自于c语言中的递增运算符++，该运算符将变量加1。c++起初也叫”c with clsss”。通过名称表明，c++是对C的扩展，因此c++是c语言的超集，这意味着任何有效的c程序都是有效的c++程序。c++程序可以使用已有的c程序库。</p>\n<p>c++语言在c语言的基础上添加了<strong>面向对象编程</strong>和<strong>泛型编程</strong>的支持。c++继承了c语言高效，简洁，快速和可移植的传统。</p>\n<p>c++融合了3种不同的编程方式:</p>\n<ul>\n<li><p>c语言代表的过程性语言.</p>\n</li>\n<li><p>c++在c语言基础上添加的类代表的面向对象语言.</p>\n</li>\n<li><p>c++模板支持的泛型编程。</p>\n</li>\n</ul>\n<h2 id=\"2、左值和右值\"><a href=\"#2、左值和右值\" class=\"headerlink\" title=\"2、左值和右值\"></a>2、左值和右值</h2><p>判断是否是左值，有一个简单的方法，就是看看能否取它的地址，能取地址就是左值，否则就是右值。</p>\n<p>当一个对象成为右值时，使用的是它的值(内容), 而成为左值时，使用的是它的身份（在内存中的位置）。</p>\n<p>平常所说的引用，实际上指的就是左值引用<code>lvalue reference</code>, 常用单个&amp;来表示。左值引用只能接收左值，不能接收右值。<strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></p>\n<p><strong>为了支持移动操作，在c++11版本，增加了右值引用。</strong>右值引用一般用于绑定到一个即将销毁的对象，所以右值引用又通常出现在移动构造函数中。</p>\n<p>看完下面的例子，左值和右值基本就清楚了，左值具有持久的状态，有独立的内存空间，右值要么是字面常量，要么就是表达式求值过程中创建的临时对象</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">int i = 66;\nint &r = i ; //r 是一个左引用，绑定左值 i\n\nint &&rr = i ; //rr是一个右引用，绑定到左值i , 错误！\nint &r2 = i*42 ; //  r2 是一个左引用， 而i*42是一个表达式，计算出来的结果是一个右值。 错误！\n\nconst int &r3 = i*42; // const修饰的左值引用 正确\nint &&rr2 = i*42 ; // 右引用，绑定右值 正确<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"3、c-对c的扩展\"><a href=\"#3、c-对c的扩展\" class=\"headerlink\" title=\"3、c++对c的扩展\"></a>3、c++对c的扩展</h2><p><strong>3.1 三目运算符</strong></p>\n<p>c语言三目运算表达式返回值为数据值，为右值，不能赋值。</p>\n<p>c++语言三目运算表达式返回值为变量本身(引用)，为左值，可以赋值。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">int a = 10;\nint b = 20;\nprintf(\"ret:%d\\n\", a > b ? a : b);\n//思考一个问题，(a > b ? a : b) 三目运算表达式返回的是什么？\n\ncout << \"b:\" << b << endl;\n//返回的是左值，变量的引用\n(a > b ? a : b) = 100;//返回的是左值，变量的引用\ncout << \"b:\" << b << endl;<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>3.2 bool</strong></p>\n<p>c++中新增bool类型关键字</p>\n<ul>\n<li>bool类型只有两个值，true(1)， false（0）</li>\n<li>bool类型占1个字节</li>\n<li>给bool类型赋值时, 非0值会自动转换为true(1), 0值会自动转换为false（0）</li>\n</ul>\n<p>C语言中也有bool类型，在c99标准之前是没有bool关键字，c99标准已经有bool类型，包含头文件<code>stdbool.h</code>,就可以使用和c++一样的bool类型。</p>\n<p><strong>3.3 struct类型增强</strong></p>\n<ul>\n<li><p>c中定义结构体变量需要加上struct关键字，c++不需要。</p>\n</li>\n<li><p>c中的结构体只能定义成员变量，不能定义成员函数。c++即可以定义成员变量，也可以定义成员函数。</p>\n</li>\n</ul>\n<p><strong>3.4 更严格的类型转换</strong></p>\n<p>在C++中，不同类型的变量一般是不能直接赋值的，需要相应的强转。</p>\n<p>在C++中，所有的变量和函数都必须有类型</p>\n<p><strong>3.4 全局变量检测增强</strong></p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">int a = 10; //赋值，当做定义\nint a; //没有赋值，当做声明\n\nint main(){\n    printf(\"a:%d\\n\",a);\n    return EXIT_SUCCESS;\n}\n\n// 上面的代码在c++下编译失败，在c下编译通过。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"4、内部连接和外部连接\"><a href=\"#4、内部连接和外部连接\" class=\"headerlink\" title=\"4、内部连接和外部连接\"></a>4、内部连接和外部连接</h2><p>内部连接：如果一个名称对编译单元(.cpp)来说是局部的，在链接的时候其他的编译单元无法链接到它且不会与其它编译单元(.cpp)中的同样的名称相冲突。例如static函数，inline函数等（注 : 用static修饰的函数，本限定在本源码文件中，不能被本源码文件以外的代码文件调用。而普通的函数，默认是extern的，也就是说，可以被其它代码文件调用该函数。）</p>\n<p>外部连接：如果一个名称对编译单元(.cpp)来说不是局部的，而在链接的时候其他的编译单元可以访问它，也就是说它可以和别的编译单元交互。 例如全局变量就是外部链接 。</p>\n<h2 id=\"5、C-C-中const的区别\"><a href=\"#5、C-C-中const的区别\" class=\"headerlink\" title=\"5、C/C++中const的区别\"></a>5、C/C++中const的区别</h2><p>1、const全局变量</p>\n<p>c语言全局const会被存储到只读数据区。c++中全局const当声明extern或者对变量取地址时，编译器会分配存储地址，变量存储在只读数据段。两个都受到了只读数据区的保护，不可修改。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">const int constA = 10;\nint main(){\n    int* p = (int*)&constA;\n    *p = 200;\n}\n\n// 以上代码在c/c++中编译通过，在运行期，修改constA的值时，发生写入错误。原因是修改只读数据段的数据。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>2、const 局部变量</p>\n<p>c语言中局部const存储在堆栈区，只是不能通过变量直接修改const只读变量的值，但是可以跳过编译器的检查，通过指针间接修改const值。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">const int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\nprintf(\"constA:%d\\n\",constA);\nprintf(\"*p:%d\\n\", *p);\n\n// constA:300\n// *p:300<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>c++中对于局部的const变量要区别对待：</p>\n<ul>\n<li>对于基础数据类型，也就是const int a = 10这种，编译器会把它放到符号表中，不分配内存，当对其取地址时，会分配内存。</li>\n</ul>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">const int constA = 10;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:10\n// *p:300\n// constA在符号表中，当我们对constA取地址，这个时候为constA分配了新的空间，*p操作的是分配的空间，而constA是从符号表获得的值。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>​    </p>\n<ul>\n<li>对于基础数据类型，如果用一个变量初始化const变量，如果const int a = b,那么也是会给a分配内存。</li>\n</ul>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">int b = 10;\nconst int constA = b;\nint* p = (int*)&constA;\n*p = 300;\ncout << \"constA:\" << constA << endl;\ncout << \"*p:\" << *p << endl;\n\n// constA:300\n// *p:300 <span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<ul>\n<li>对于自定数据类型，比如类对象，那么也会分配内存。</li>\n</ul>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">const Person person; //未初始化age\n//person.age = 50; //不可修改\nPerson* pPerson = (Person*)&person;\n//指针间接修改\npPerson->age = 100;\ncout << \"pPerson->age:\" << pPerson->age << endl;\npPerson->age = 200;\ncout << \"pPerson->age:\" << pPerson->age << endl;\n\n// pPerson->age:100\n// pPerson->age:200\n//为person分配了内存，所以我们可以通过指针的间接赋值修改person对象。<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>3、链接方式</p>\n<p>c中const默认为外部连接，c++中const默认为内部连接.当c语言两个文件中都有const int a的时候，编译器会报重定义的错误。而在c++中，则不会，因为c++中的const默认是内部连接的。如果想让c++中的const具有外部连接，必须显示声明为: extern const int a = 10;</p>\n<h2 id=\"6、const与-define的区别\"><a href=\"#6、const与-define的区别\" class=\"headerlink\" title=\"6、const与#define的区别\"></a>6、const与#define的区别</h2><ul>\n<li><p>const有数据类型，可进行编译器类型安全检查。#define无类型，不可以进行类型检查</p>\n</li>\n<li><p>const有作用域，而#define不重视作用域，默认定义处到文件结尾。如果想定义在指定作用域下有效的常量，那么#define就不能用。</p>\n</li>\n</ul>\n<h2 id=\"7、引用\"><a href=\"#7、引用\" class=\"headerlink\" title=\"7、引用\"></a>7、引用</h2><ol>\n<li><p>&amp;在此不是求地址运算，而是起标识作用。</p>\n</li>\n<li><p>类型标识符是指目标变量的类型</p>\n</li>\n<li><p>必须在声明引用变量时进行初始化。</p>\n</li>\n<li><p>引用初始化之后不能改变。</p>\n</li>\n<li><p>不能有NULL引用。必须确保引用是和一块合法的存储单元关联。</p>\n</li>\n<li><p><strong>可以建立对数组的引用。</strong></p>\n</li>\n<li><p>函数不能返回局部变量的引用</p>\n</li>\n<li><p>函数当左值，必须返回引用</p>\n</li>\n</ol>\n<p><strong>引用的本质</strong></p>\n<p>引用的本质是在c++内部实现一个指针常量</p>\n<p><code>Type&amp; ref = val;  // Type* const ref = val;</code></p>\n<p>c++编译器在编译过程中使用常指针作为引用的内部实现，因此引用所占用的空间大小与指针相同，只是这个过程是编译器内部实现，用户不可见。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">//发现是引用，转换为 int* const ref = &a;\nvoid testFunc(int& ref){\n    ref = 100; // ref是引用，转换为*ref = 100\n}\nint main(){\n    int a = 10;\n    int& aRef = a; //自动转换为 int* const aRef = &a;这也能说明引用为什么必须初始化\n    aRef = 20; //内部发现aRef是引用，自动帮我们转换为: *aRef = 20;\n    cout << \"a:\" << a << endl;\n    cout << \"aRef:\" << aRef << endl;\n    testFunc(a);\n    return EXIT_SUCCESS;\n}\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"8、面向对象的三大特性\"><a href=\"#8、面向对象的三大特性\" class=\"headerlink\" title=\"8、面向对象的三大特性\"></a>8、面向对象的三大特性</h2><p><strong>封装</strong></p>\n<ol>\n<li>把变量（属性）和函数（操作）合成一个整体，封装在一个类中</li>\n<li>对变量和函数进行访问控制</li>\n</ol>\n<p><strong>继承</strong></p>\n<pre><code>c++最重要的特征是代码重用，通过继承机制可以利用已有的数据类型来定义新的数据类型，新的类不仅拥有旧类的成员，还拥有新定义的成员。\n\n派生类继承基类，派生类拥有基类中全部成员变量和成员方法（除了构造和析构之外的成员方法），但是在派生类中，继承的成员并不一定能直接访问，不同的继承方式会导致不同的访问权限。\n\n任何时候重新定义基类中的一个重载函数，在新类中所有的其他版本将被自动隐藏.\n\noperator=也不能被继承，因为它完成类似构造函数的行为。</code></pre><p><strong>多态</strong></p>\n<pre><code>c++支持编译时多态(静态多态)和运行时多态(动态多态)，运算符重载和函数重载就是编译时多态，而派生类和虚函数实现运行时多态。\n\n静态多态和动态多态的区别就是函数地址是早绑定(静态联编)还是晚绑定(动态联编)。如果函数的调用，在编译阶段就可以确定函数的调用地址，并产生代码，就是静态多态(编译时多态)，就是说地址是早绑定的。而如果函数的调用地址不能编译不能在编译期间确定，而需要在运行时才能决定，这这就属于晚绑定(动态多态,运行时多态)。\n\n多态性改善了代码的可读性和组织性，同时也使创建的程序具有可扩展性。</code></pre><h2 id=\"9、C-编译器优化技术：RVO-NRVO和复制省略\"><a href=\"#9、C-编译器优化技术：RVO-NRVO和复制省略\" class=\"headerlink\" title=\"9、C++编译器优化技术：RVO/NRVO和复制省略\"></a>9、C++编译器优化技术：RVO/NRVO和复制省略</h2><p>现代编译器缺省会使用RVO（return value optimization，返回值优化）、NRVO（named return value optimization、命名返回值优化）和复制省略（Copy elision）技术，来减少拷贝次数来提升代码的运行效率</p>\n<p>注1：vc6、vs没有提供编译选项来关闭该优化，无论是debug还是release都会进行RVO和复制省略优化</p>\n<p>注2：vc6、vs2005以下及vs2005+ Debug上不支持NRVO优化，vs2005+ Release支持NRVO优化</p>\n<p>注3：g++支持这三种优化，并且可通过编译选项：-fno-elide-constructors来关闭优化</p>\n<p><strong>RVO</strong></p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">#include <stdio.h>\nclass A\n{\npublic:\n    A()\n    {\n        printf(\"%p construct\\n\", this);\n    }\n    A(const A& cp)\n    {\n        printf(\"%p copy construct\\n\", this);\n    }\n    ~A() \n    {\n        printf(\"%p destruct\\n\", this);\n    }\n};\n\nA GetA()\n{\n    return A();\n}\n\nint main()\n{\n    {\n        A a = GetA();\n    }\n\n    return 0;\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下：</p>\n<pre><code>0x7ffc46947d4f construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量temp\n0x7ffc46947d7f copy construct // 函数GetA return语句处，把临时变量temp做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来\n0x7ffc46947d4f destruct // 函数GetA执行完return语句后，临时变量temp生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e copy construct // 函数GetA调用结束，返回上层main函数后，把返回值变量ret做为参数传入并调用拷贝构造函数A(const A&amp; cp)将变量A a构造出来\n0x7ffc46947d7f destruct // A a = GetA()语句结束后，返回值ret生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：临时变量temp、返回值ret均为匿名变量</p>\n<p><strong>NRVO</strong></p>\n<p>g++编译器、vs2005+ Release（开启/O2及以上优化开关）</p>\n<p>修改上述代码，将GetA的实现修改成：</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">A GetA()\n{\n    A o;\n    return o;\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>在g++、vs2005+ Release中，上述代码也仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct</code></pre><p>g++加上-fno-elide-constructors选项关闭优化后，和上述结果一样</p>\n<pre><code>0x7ffc46947d4f construct\n0x7ffc46947d7f copy construct\n0x7ffc46947d4f destruct\n0x7ffc46947d7e copy construct\n0x7ffc46947d7f destruct\n0x7ffc46947d7e destruct</code></pre><p>但在vc6、vs2005以下、vs2005+ Debug中，没有进行NRVO优化，输出结果为：</p>\n<pre><code>18fec4 construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量o\n18ff44 copy construct  // 函数GetA return语句处，把临时变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来\n18fec4 destruct  // 函数GetA执行完return语句后，临时变量o生命周期结束，调用其析构函数~A()\n18ff44 destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：<strong>与g++、vs2005+ Release相比，vc6、vs2005以下、vs2005+ Debug只优化掉了返回值到变量a的拷贝，命名局部变量o没有被优化掉，所以最后一共有2次构造和析构的调用</strong></p>\n<p><strong>复制省略</strong></p>\n<p>典型情况是：调用构造函数进行值类型传参</p>\n<pre><code>void Func(A a) \n{\n}\n\nint main()\n{\n    {\n        Func(A());\n    }\n\n    return 0;\n}</code></pre><p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffeb5148d0f construct\n0x7ffeb5148d0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下： </p>\n<pre><code>0x7ffc53c141ef construct   // 在main函数中，调用无参构造函数构造实参变量o\n0x7ffc53c141ee copy construct // 调用Func函数后，将实参变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将形参变量a构造出来\n0x7ffc53c141ee destruct // 函数Func执行完后，形参变量a生命周期结束，调用其析构函数~A()\n0x7ffc53c141ef destruct // 返回main函数后，实参变量o要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p><strong>优化失效的情况</strong></p>\n<ol>\n<li>根据不同的条件分支，返回不同变量</li>\n<li>返回参数变量</li>\n<li>返回全局变量</li>\n<li>返回复合函数类型中的成员变量</li>\n<li>返回值赋值给已构造好的变量（此时会调用operator==赋值运算符）</li>\n</ol>\n<p><a href=\"https://www.cnblogs.com/kekec/p/11303391.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/kekec/p/11303391.html</a></p>\n<h2 id=\"10、explicit关键字\"><a href=\"#10、explicit关键字\" class=\"headerlink\" title=\"10、explicit关键字\"></a>10、explicit关键字</h2><ul>\n<li><p>explicit用于修饰构造函数,防止隐式转化。</p>\n</li>\n<li><p>是针对单参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造)而言。</p>\n</li>\n</ul>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">class MyString{\npublic:\n    explicit MyString(int n){\n        cout << \"MyString(int n)!\" << endl;\n    }\n    MyString(const char* str){\n        cout << \"MyString(const char* str)\" << endl;\n    }\n};\n\nint main(){\n\n    //给字符串赋值？还是初始化？\n    //MyString str1 = 1; \n    MyString str2(10);\n\n    //寓意非常明确，给字符串赋值\n    MyString str3 = \"abcd\";\n    MyString str4(\"abcd\");\n\n    return EXIT_SUCCESS;\n}\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"11、new-delete-malloc-free\"><a href=\"#11、new-delete-malloc-free\" class=\"headerlink\" title=\"11、new/delete/malloc/free\"></a>11、new/delete/malloc/free</h2><p><strong>11.1、new</strong></p>\n<ol>\n<li>内存申请成功后，会返回一个指向该内存的地址。</li>\n<li>若内存申请失败，则抛出异常，</li>\n<li>申请成功后，如果是程序员定义的类型，会执行相应的构造函数</li>\n</ol>\n<p><strong>11.2、delete</strong></p>\n<ol>\n<li>如果指针的值是0 ，delete不会执行任何操作，有检测机制</li>\n<li>delete只是释放内存，不会修改指针，指针仍然会指向原来的地址</li>\n<li>重复delete，有可能出现异常</li>\n<li>如果是自定义类型，会执行析构函数</li>\n</ol>\n<p><strong>11.3、malloc</strong></p>\n<ol>\n<li>malloc 申请成功之后，返回的是void类型的指针。需要将void*指针转换成我们需要的类型。1.</li>\n<li>malloc 要求制定申请的内存大小 ， 而new由编译器自行计算。</li>\n<li>申请失败，返回的是NULL ， 比如： 内存不足。</li>\n<li>不会执行自定义类型的构造函数</li>\n</ol>\n<p><strong>11.4、free</strong></p>\n<ol>\n<li>如果是空指针，多次释放没有问题，非空指针，重复释放有问题</li>\n<li>不会执行对应的析构</li>\n<li>delete的底层执行的是free</li>\n</ol>\n<h2 id=\"12、const\"><a href=\"#12、const\" class=\"headerlink\" title=\"12、const\"></a>12、const</h2><ol>\n<li>const修饰静态成员变量时可以在类内部初始化</li>\n<li>const 修饰成员函数时，修饰的是this指针，所以成员函数内不可以修改任何普通成员变量，当成员变量类型符前用mutable修饰时例外</li>\n<li>常对象(cons修饰的对象)只能调用const修饰的成员函数</li>\n<li>常对象可以访问成员属性，但是不能修改</li>\n<li><strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></li>\n</ol>\n<h2 id=\"13、虚继承\"><a href=\"#13、虚继承\" class=\"headerlink\" title=\"13、虚继承\"></a>13、虚继承</h2><p>虚继承是解决C++多重继承问题的一种手段，从不同途径继承来的同一基类，会在子类中存在多份拷贝。这将存在两个问题：其一，浪费存储空间；第二，存在二义性问题，多重继承可能存在一个基类的多份拷贝，这就出现了二义性。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">class BigBase{\npublic:\n    BigBase(){ mParam = 0; }\n    void func(){ cout << \"BigBase::func\" << endl; }\npublic:\n    int mParam;\n};\n\nclass Base1 : public BigBase{};\nclass Base2 : public BigBase{};\nclass Derived : public Base1, public Base2{};\n\nint main(){\n\n    Derived derived;\n    //1. 对“func”的访问不明确\n    //derived.func();\n    //cout << derived.mParam << endl;\n    cout << \"derived.Base1::mParam:\" << derived.Base1::mParam << endl;\n    cout << \"derived.Base2::mParam:\" << derived.Base2::mParam << endl;\n\n    //2. 重复继承\n    cout << \"Derived size:\" << sizeof(Derived) << endl; //8\n\n    return EXIT_SUCCESS;\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>虚继承可以解决多种继承前面提到的两个问题：</p>\n<p>虚继承底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，64位8字节/windows 4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类指针依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，<strong>虚基类指针也会被继承。</strong></p>\n<p>实际上，<code>vbptr</code>指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。</p>\n<p><img src=\"05.png\" alt></p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">#include <iostream>\n\nusing namespace std;\n\n\nclass A  //大小为4\n{\npublic:\n    int a;\n};\n\nclass B:virtual public A{ // vbptr 8, int b 4, int a 4 = 12\npublic:\n    int b;\n};\n\nclass C:virtual public A{// vbptr 8, int c 4, int a 4 = 12\npublic:\n    int c;\n};\n\nclass D:public B, public C{\n    // int a, b, c, d=16\n    // class B  vbptr 4\n    // class C  vbptr 4  = 24\npublic:\n    int d;\n};\n\nint main()\n{\n    A a;\n    B b;\n    C c;\n    D d;\n    cout << sizeof(a) << endl; // 4 \n    cout << sizeof(b) << endl; // 16\n    cout << sizeof(c) << endl; // 16\n    cout << sizeof(d) << endl; // 24\n    return 0;\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>链接:<a href=\"https://blog.csdn.net/bxw1992/article/details/77726390\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/bxw1992/article/details/77726390</a></p>\n<h2 id=\"14、虚函数\"><a href=\"#14、虚函数\" class=\"headerlink\" title=\"14、虚函数\"></a>14、虚函数</h2><p><strong>问题</strong></p>\n<pre><code>父类引用或指针可以指向子类对象，通过父类指针或引用来操作子类对象。但是由于编译阶段编译器根据对象的指针或者引用选择函数调用，所以会调用父类的函数。\n\n解决问题的方法是迟绑定(动态绑定)，在运行时根据对象的实际类型决定。</code></pre><p><strong>解决</strong></p>\n<pre><code>C++动态多态性是通过虚函数来实现的，虚函数允许子类（派生类）重新定义父类（基类）成员函数，而子类（派生类）重新定义父类（基类）虚函数的做法称为覆盖(override)，或者称为重写。\n\n对于特定的函数进行动态绑定，c++要求在基类中声明这个函数的时候使用virtual关键字,动态绑定也就对virtual函数起作用.</code></pre><ul>\n<li><p>为创建一个需要动态绑定的虚成员函数，可以简单在这个函数声明前面加上virtual关键字，定义时候不需要.</p>\n</li>\n<li><p>如果一个函数在基类中被声明为virtual，那么在所有派生类中它都是virtual的.</p>\n</li>\n<li><p>在派生类中virtual函数的重定义称为重写(override).</p>\n</li>\n<li><p>Virtual关键字只能修饰成员函数.</p>\n</li>\n<li><p>构造函数不能为虚函数</p>\n</li>\n</ul>\n<p><strong>虚函数原理</strong></p>\n<p>首先，我们看看编译器如何处理虚函数。当编译器发现我们的类中有虚函数的时候，编译器会创建一张虚函数表（virtual function table ），表中存储着类对象的虚函数地址，并且给类增加一个指针，这个指针就是<code>vpointer</code>(缩写<code>vptr</code>)，这个指针是指向虚函数表。</p>\n<p>父类对象包含的指针，指向父类的虚函数表地址，子类对象包含的指针，指向子类的虚函数表地址。</p>\n<p>如果子类重新定义了父类的函数，那么函数表中存放的是新的地址，如果子类没有重新定义，那么表中存放的是父类的函数地址。如果子类有自己的虚函数，则只需要添加到表中即可。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">class A{\npublic:\n    virtual void func1(){}\n    virtual void func2(){}\n};\n\n//B类为空，那么大小应该是1字节，实际情况是这样吗？\nclass B : public A{};\n\nvoid test(){\n    cout << \"A size:\" << sizeof(A) << endl; // win指针字节为4, linux 64 字节为8\n    cout << \"B size:\" << sizeof(B) << endl; // 4\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p><strong>多态的成立条件：</strong></p>\n<ul>\n<li><p>有继承</p>\n</li>\n<li><p>子类重写父类虚函数函数</p>\n<pre><code>a) 返回值，函数名字，函数参数，必须和父类完全一致(析构函数除外) \n\nb) 子类中virtual关键字可写可不写，建议写</code></pre></li>\n<li><p>类型兼容，父类指针，父类引用指向子类对象</p>\n</li>\n</ul>\n<p><strong>抽象类和纯虚函数</strong></p>\n<p>当基类中有至少一个纯虚函数则为抽象类</p>\n<ul>\n<li><p>纯虚函数使用关键字virtual，并在其后面加上=0。如果试图去实例化一个抽象类，编译器则会阻止这种操作。</p>\n</li>\n<li><p>当继承一个抽象类的时候，必须实现所有的纯虚函数，否则由抽象类派生的类也是一个抽象类。</p>\n</li>\n<li><p>Virtual void fun() = 0;告诉编译器在vtable中为函数保留一个位置，但在这个特定位置不放地址。</p>\n</li>\n</ul>\n<p><strong>接口类</strong></p>\n<p>接口类中只有函数原型定义，没有任何数据定义。多重继承接口不会带来二义性和复杂性问题。接口类只是一个功能声明，并不是功能实现，子类需要根据功能说明定义功能实现。</p>\n<p>注意:除了析构函数外，其他声明都是纯虚函数。</p>\n<p><strong>虚析构函数</strong></p>\n<p>虚析构函数是为了解决[基类]的[指针]指向派生类对象，并用基类的指针删除派生类对象。</p>\n<p><strong>虚函数和虚继承的异同</strong></p>\n<p>在这里我们可以对比虚函数的实现原理：他们有相似之处，都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间）。</p>\n<p>虚基类指针依旧存在继承类中，只占用存储空间；基类虚函数指针不存在于子类中，不占用存储空间。</p>\n<p>虚基类表存储的是虚基类相对直接继承类的偏移；而虚函数表存储的是虚函数地址。</p>\n<h2 id=\"15、函数模板的机制\"><a href=\"#15、函数模板的机制\" class=\"headerlink\" title=\"15、函数模板的机制\"></a>15、函数模板的机制</h2><p><strong>函数模板机制结论：</strong></p>\n<ul>\n<li><p>编译器并不是把函数模板处理成能够处理任何类型的函数</p>\n</li>\n<li><p>函数模板通过具体类型产生不同的函数</p>\n</li>\n<li><p>编译器会对函数模板进行两次编译，在声明的地方对模板代码本身进行编译，在调用的地方对参数替换后的代码进行编译。</p>\n</li>\n</ul>\n<p><strong>局限性</strong>:</p>\n<p>编写的模板函数很可能无法处理某些类型，另一方面，有时候通用化是有意义的，但C++语法不允许这样做。为了解决这种问题，可以提供<strong>模板的重载</strong>，为这些特定的类型提供具体化的模板。</p>\n<pre class=\"line-numbers language-c++\"><code class=\"language-c++\">class Person\n{\npublic:\n    Person(string name, int age)\n    {\n        this->mName = name;\n        this->mAge = age;\n    }\n    string mName;\n    int mAge;\n};\n\n\n//普通交换函数\ntemplate <class T>\nvoid mySwap(T &a,T &b)\n{\n    T temp = a;\n    a = b;\n    b = temp;\n}\n\n\n//第三代具体化，显示具体化的原型和定意思以template<>开头，并通过名称来指出类型\n//具体化优先于常规模板\ntemplate<>void mySwap<Person>(Person &p1, Person &p2)\n{\n    string nameTemp;\n    int ageTemp;\n\n    nameTemp = p1.mName;\n    p1.mName = p2.mName;\n    p2.mName = nameTemp;\n\n    ageTemp = p1.mAge;\n    p1.mAge = p2.mAge;\n    p2.mAge = ageTemp;\n\n}\n\nvoid test()\n{\n    Person P1(\"Tom\", 10);\n    Person P2(\"Jerry\", 20);\n\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n    mySwap(P1, P2);\n    cout << \"P1 Name = \" << P1.mName << \" P1 Age = \" << P1.mAge << endl;\n    cout << \"P2 Name = \" << P2.mName << \" P2 Age = \" << P2.mAge << endl;\n}<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h2 id=\"16、C-类型转换\"><a href=\"#16、C-类型转换\" class=\"headerlink\" title=\"16、C++类型转换\"></a>16、C++类型转换</h2><p><strong>静态类型转换static_cast</strong></p>\n<ul>\n<li><p>用于<a href=\"http://baike.baidu.com/view/2405425.htm\" target=\"_blank\" rel=\"noopener\">类层次结构</a>中基类（父类）和<a href=\"http://baike.baidu.com/view/535532.htm\" target=\"_blank\" rel=\"noopener\">派生类</a>（子类）之间指针或引用的转换。</p>\n<ul>\n<li><p>进行上行转换（把派生类的指针或引用转换成基类表示）是安全的；</p>\n</li>\n<li><p>进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。</p>\n</li>\n</ul>\n</li>\n<li><p>用于基本数据类型之间的转换，如把int转换成char，把char转换成int。这种转换的安全性也要开发人员来保证。</p>\n</li>\n</ul>\n<p><strong>动态类型转换(dynamic_cast)</strong></p>\n<ul>\n<li><p>dynamic_cast主要用于类层次间的上行转换和下行转换；</p>\n</li>\n<li><p>在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；</p>\n</li>\n<li><p>在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全；</p>\n</li>\n</ul>\n<p><strong>常量转换(const_cast)</strong></p>\n<p>该运算符用来修改类型的const属性。。</p>\n<ul>\n<li><p>常量指针被转化成非常量指针，并且仍然指向原来的对象；</p>\n</li>\n<li><p>常量引用被转换成非常量引用，并且仍然指向原来的对象；</p>\n</li>\n</ul>\n<p><strong>重新解释转换(reinterpret_cast)</strong></p>\n<p>这是最不安全的一种转换机制，最有可能出问题。</p>\n<p>主要用于将一种数据类型从一种类型转换为另一种类型。它可以将一个指针转换成一个整数，也可以将一个整数转换成一个指针.</p>\n<h1 id=\"三、STL\"><a href=\"#三、STL\" class=\"headerlink\" title=\"三、STL\"></a>三、STL</h1><h2 id=\"1、STL六大组件简介\"><a href=\"#1、STL六大组件简介\" class=\"headerlink\" title=\"1、STL六大组件简介\"></a>1、STL六大组件简介</h2><p>STL提供了六大组件，彼此之间可以组合套用，这六大组件分别是:容器、算法、迭代器、仿函数、适配器（配接器）、空间配置器。</p>\n<p><strong>容器：</strong>各种数据结构，如vector、list、deque、set、map等, 用来存放数据，从实现角度来看，STL容器是一种class template。</p>\n<p><strong>算法：</strong>各种常用的算法，如sort、find、copy、for_each。从实现的角度来看，STL算法是一种function tempalte.</p>\n<p><strong>迭代器：</strong>扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，<strong>迭代器是一种将operator* , operator-&gt; , operator++,operator–等指针相关操作予以重载的class template.</strong> 所有STL容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。原生指针(native pointer)也是一种迭代器。</p>\n<p><strong>仿函数：</strong>行为类似函数，可作为算法的某种策略。从实现角度来看，仿函数是一种重载了operator()的class 或者class template</p>\n<p><strong>适配器：</strong>一种用来修饰容器或者仿函数或迭代器接口的东西。</p>\n<p><strong>空间配置器：</strong>负责空间的配置与管理。从实现角度看，配置器是一个实现了动态空间配置、空间管理、空间释放的class tempalte.</p>\n<h2 id=\"2、string容器\"><a href=\"#2、string容器\" class=\"headerlink\" title=\"2、string容器\"></a>2、string容器</h2><p>C风格字符串(以空字符结尾的字符数组)太过复杂难于掌握，不适合大程序的开发，所以C++标准库定义了一种string类，定义在头文件<string>。</string></p>\n<p>String和c风格字符串对比：</p>\n<ul>\n<li><p>char*是一个指针，String是一个类</p>\n<p>string封装了char*，管理这个字符串，是一个char*型的容器。</p>\n</li>\n<li><p>String封装了很多实用的成员方法</p>\n<p>查找find，拷贝copy，删除delete 替换replace，插入insert</p>\n</li>\n<li><p>不用考虑内存释放和越界</p>\n<p> string管理char*所分配的内存。每一次string的复制，取值都由string类负责维护，不用担心复制越界和取值越界等。</p>\n</li>\n</ul>\n<h2 id=\"3、vector容器\"><a href=\"#3、vector容器\" class=\"headerlink\" title=\"3、vector容器\"></a>3、vector容器</h2><p>vector的数据安排以及操作方式，与array非常相似，两者的唯一差别在于空间的运用的灵活性。Array是静态空间，一旦配置了就不能改变，要换大一点或者小一点的空间，可以，一切琐碎得由自己来，首先配置一块新的空间，然后将旧空间的数据搬往新空间，再释放原来的空间。Vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新元素。因此vector的运用对于内存的合理利用与运用的灵活性有很大的帮助，我们再也不必害怕空间不足而一开始就要求一个大块头的array了。</p>\n<p>所谓动态增加大小，并不是在原空间之后续接新空间(因为无法保证原空间之后尚有可配置的空间)，而是一块更大的内存空间，然后将原数据拷贝新空间，并释放原空间。因此，对vector的任何操作，一旦引起空间的重新配置，指向原vector的所有迭代器就都失效了。这是程序员容易犯的一个错误，务必小心。</p>\n<p>为了降低空间配置时的速度成本，vector实际配置的大小可能比客户端需求大一些，以备将来可能的扩充，这边是<strong>容量</strong>的概念。换句话说，<strong>一个vector的容量永远大于或等于其大小，一旦容量等于大小，便是满载，下次再有新增元素，整个vector容器就得另觅居所。</strong></p>\n<h2 id=\"4、deque容器\"><a href=\"#4、deque容器\" class=\"headerlink\" title=\"4、deque容器\"></a>4、deque容器</h2><p>Vector容器是单向开口的连续内存空间，deque则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector容器也可以在头尾两端插入元素，但是在其头部操作效率奇差，无法被接受。</p>\n<p>Deque容器和vector容器最大的差异，一在于deque允许使用常数项时间对头端进行元素的插入和删除操作。二在于deque没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，换句话说，像vector那样，”旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque身上是不会发生的。也因此，deque没有必须要提供所谓的空间保留(reserve)功能.</p>\n<p>虽然deque容器也提供了Random Access Iterator,但是它的迭代器并不是普通的指针，其复杂度和vector不是一个量级，这当然影响各个运算的层面。因此，除非有必要，我们应该尽可能的使用vector，而不是deque。对deque进行的排序操作，为了最高效率，可将deque先完整的复制到一个vector中，对vector容器进行排序，再复制回deque.</p>\n<p>既然deque是分段连续内存空间，那么就必须有中央控制，维持整体连续的假象，数据结构的设计及迭代器的前进后退操作颇为繁琐。Deque代码的实现远比vector或list都多得多。</p>\n<p>Deque采取一块所谓的map(注意，不是STL的map容器)作为主控，这里所谓的map是一小块连续的内存空间，其中每一个元素(此处成为一个结点)都是一个指针，指向另一段连续性内存空间，称作缓冲区。缓冲区才是deque的存储空间的主体。</p>\n<p><img src=\"06.png\" alt></p>\n<h2 id=\"5、stack容器\"><a href=\"#5、stack容器\" class=\"headerlink\" title=\"5、stack容器\"></a>5、stack容器</h2><p>stack是一种先进后出(First In Last Out,FILO)的数据结构，它只有一个出口，形式如图所示。stack容器允许新增元素，移除元素，取得栈顶元素，但是除了最顶端外，没有任何其他方法可以存取stack的其他元素。换言之，stack不允许有遍历行为。</p>\n<p>有元素推入栈的操作称为:push,将元素推出stack的操作称为pop.</p>\n<p>Stack所有元素的进出都必须符合”先进后出”的条件，只有stack顶端的元素，才有机会被外界取用。Stack不提供遍历功能，也不提供迭代器。</p>\n<h2 id=\"6、queue容器\"><a href=\"#6、queue容器\" class=\"headerlink\" title=\"6、queue容器\"></a>6、queue容器</h2><p>Queue是一种先进先出(First In First Out,FIFO)的数据结构，它有两个出口，queue容器允许从一端新增元素，从另一端移除元素。</p>\n<p>Queue所有元素的进出都必须符合”先进先出”的条件，只有queue的顶端元素，才有机会被外界取用。Queue不提供遍历功能，也不提供迭代器。</p>\n<h2 id=\"7、list容器\"><a href=\"#7、list容器\" class=\"headerlink\" title=\"7、list容器\"></a>7、list容器</h2><p>链表是一种物理<a href=\"http://baike.baidu.com/view/1223079.htm\" target=\"_blank\" rel=\"noopener\">存储单元</a>上非连续、非顺序的<a href=\"http://baike.baidu.com/view/2820182.htm\" target=\"_blank\" rel=\"noopener\">存储结构</a>，<a href=\"http://baike.baidu.com/view/38785.htm\" target=\"_blank\" rel=\"noopener\">数据元素</a>的逻辑顺序是通过链表中的<a href=\"http://baike.baidu.com/view/159417.htm\" target=\"_blank\" rel=\"noopener\">指针</a>链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储<a href=\"http://baike.baidu.com/view/38785.htm\" target=\"_blank\" rel=\"noopener\">数据元素</a>的数据域，另一个是存储下一个结点地址的<a href=\"http://baike.baidu.com/view/159417.htm\" target=\"_blank\" rel=\"noopener\">指针</a>域。</p>\n<p>相较于vector的连续线性空间，list就显得负责许多，它的好处是每次插入或者删除一个元素，就是配置或者释放一个元素的空间。因此，list对于空间的运用有绝对的精准，一点也不浪费。而且，对于任何位置的元素插入或元素的移除，list永远是常数时间。</p>\n<p>List和vector是两个最常被使用的容器。</p>\n<p>List容器是一个双向链表。</p>\n<ul>\n<li><p>采用动态存储分配，不会造成内存浪费和溢出</p>\n</li>\n<li><p>链表执行插入和删除操作十分方便，修改指针即可，不需要移动大量元素</p>\n</li>\n<li><p>链表灵活，但是空间和时间额外耗费较大</p>\n</li>\n</ul>\n<p>List有一个重要的性质，插入操作和删除操作都不会造成原有list迭代器的失效。这在vector是不成立的，因为vector的插入操作可能造成记忆体重新配置，导致原有的迭代器全部失效，甚至List元素的删除，也只有被删除的那个元素的迭代器失效，其他迭代器不受任何影响。</p>\n<h2 id=\"8、set-multiset容器\"><a href=\"#8、set-multiset容器\" class=\"headerlink\" title=\"8、set/multiset容器\"></a>8、set/multiset容器</h2><p>Set的特性是。所有元素都会根据元素的键值自动被排序。Set的元素不像map那样可以同时拥有实值和键值，set的元素即是键值又是实值。Set不允许两个元素有相同的键值。</p>\n<p>我们可以通过set的迭代器改变set元素的值吗？不行，因为set元素值就是其键值，关系到set元素的排序规则。如果任意改变set元素值，会严重破坏set组织。换句话说，set的iterator是一种const_iterator.</p>\n<p>set拥有和list某些相同的性质，当对容器中的元素进行插入操作或者删除操作的时候，操作之前所有的迭代器，在操作完成之后依然有效，被删除的那个元素的迭代器必然是一个例外。</p>\n<h2 id=\"9、map-multimap容器\"><a href=\"#9、map-multimap容器\" class=\"headerlink\" title=\"9、map/multimap容器\"></a>9、map/multimap容器</h2><p>Map的特性是，所有元素都会根据元素的键值自动排序。Map所有的元素都是pair,同时拥有实值和键值，pair的第一元素被视为键值，第二元素被视为实值，map不允许两个元素有相同的键值。</p>\n<p>我们可以通过map的迭代器改变map的键值吗？答案是不行，因为map的键值关系到map元素的排列规则，任意改变map键值将会严重破坏map组织。如果想要修改元素的实值，那么是可以的。</p>\n<p>Map和list拥有相同的某些性质，当对它的容器元素进行新增操作或者删除操作时，操作之前的所有迭代器，在操作完成之后依然有效，当然被删除的那个元素的迭代器必然是个例外。</p>\n<p>Multimap和map的操作类似，唯一区别multimap键值可重复。</p>\n<p>Map和multimap都是以红黑树为底层实现机制。</p>\n<h2 id=\"10、STL容器使用时机\"><a href=\"#10、STL容器使用时机\" class=\"headerlink\" title=\"10、STL容器使用时机\"></a>10、STL容器使用时机</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>vector</th>\n<th>deque</th>\n<th>list</th>\n<th>set</th>\n<th>multiset</th>\n<th>map</th>\n<th>multimap</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>典型内存结构</td>\n<td>单端数组</td>\n<td>双端数组</td>\n<td>双向链表</td>\n<td>二叉树</td>\n<td>二叉树</td>\n<td>二叉树</td>\n<td>二叉树</td>\n</tr>\n<tr>\n<td>可随机存取</td>\n<td>是</td>\n<td>是</td>\n<td>否</td>\n<td>否</td>\n<td>否</td>\n<td>对key而言：不是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>元素搜寻速度</td>\n<td>慢</td>\n<td>慢</td>\n<td>非常慢</td>\n<td>快</td>\n<td>快</td>\n<td>对key而言：快</td>\n<td>对key而言：快</td>\n</tr>\n<tr>\n<td>元素安插移除</td>\n<td>尾端</td>\n<td>头尾两端</td>\n<td>任何位置</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>vector的使用场景：比如软件历史操作记录的存储，我们经常要查看历史记录，比如上一次的记录，上上次的记录，但却不会去删除记录，因为记录是事实的描述。</p>\n</li>\n<li><p>deque的使用场景：比如排队购票系统，对排队者的存储可以采用deque，支持头端的快速移除，尾端的快速添加。如果采用vector，则头端移除时，会移动大量的数据，速度慢。</p>\n<p>​     vector与deque的比较：</p>\n<p>​      一：vector.at()比deque.at()效率高，比如vector.at(0)是固定的，deque的开始位置却是不固定的。</p>\n<p>​    二：如果有大量释放操作的话，vector花的时间更少，这跟二者的内部实现有关。</p>\n<p>​    三：deque支持头部的快速插入与快速移除，这是deque的优点。</p>\n</li>\n<li><p>list的使用场景：比如公交车乘客的存储，随时可能有乘客下车，支持频繁的不确实位置元素的移除插入。</p>\n</li>\n<li><p>set的使用场景：比如对手机游戏的个人得分记录的存储，存储要求从高分到低分的顺序排列。 </p>\n</li>\n<li><p>map的使用场景：比如按ID号存储十万个用户，想要快速要通过ID查找对应的用户。二叉树的查找效率，这时就体现出来了。如果是vector容器，最坏的情况下可能要遍历完整个容器才能找到该用户。</p>\n</li>\n</ul>\n","site":{"data":{"friends":[],"musics":[{"name":"Sound of Silence","artist":"Emiliana Torrini","url":"/medias/music/Silence.mp3","cover":"/medias/music/avatars/yequ.jpg"},{"name":"挥着翅膀的女孩","artist":"朴素妍","url":"/medias/music/fly.mp3","cover":"/medias/music/avatars/yiluxiangbei.jpg"},{"name":"ひとり上手","artist":"中島みゆき","url":"/medias/music/yinlong.mp3","cover":"/medias/music/avatars/tiantangdemogui.jpg"},{"name":"鬼迷心窍","artist":"李宗盛","url":"/medias/music/guimixinqiao.mp3","cover":"/medias/music/avatars/daoshu.jpg"}]}},"excerpt":"","more":"<h1 id=\"一、C语言篇\"><a href=\"#一、C语言篇\" class=\"headerlink\" title=\"一、C语言篇\"></a>一、C语言篇</h1><h2 id=\"1、gcc-g\"><a href=\"#1、gcc-g\" class=\"headerlink\" title=\"1、gcc/g++\"></a>1、gcc/g++</h2><p><strong>1.1、为什么需要gcc/g++</strong></p>\n<p>编辑器(如vi、记事本)是指我用它来写程序的（编辑代码），而我们写的代码语句，电脑是不懂的，我们需要把它转成电脑能懂的语句，编译器就是这样的转化工具。就是说，<strong>我们用编辑器编写程序，由编译器编译后才可以运行！</strong></p>\n<p><strong>1.2、gcc编译器介绍</strong></p>\n<p>编译器是将易于编写、阅读和维护的高级计算机语言翻译为计算机能解读、运行的低级机器语言的程序。</p>\n<p>gcc（GNU Compiler Collection，GNU 编译器套件），是由 GNU 开发的编程语言编译器。gcc原本作为GNU操作系统的官方编译器，现已被大多数类Unix操作系统（如Linux、BSD、Mac OS X等）采纳为标准的编译器，gcc同样适用于微软的Windows。</p>\n<p>gcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。</p>\n<p>gcc最初用于编译C语言，随着项目的发展gcc已经成为了能够编译C、C++、Java、Ada、fortran、Object C、Object C++、Go语言的编译器大家族。</p>\n<p><strong>1.3、编译命令</strong></p>\n<p>编译命令格式：</p>\n<p>gcc [-option1] … <filename></filename></p>\n<p>g++ [-option1] … <filename></filename></p>\n<ul>\n<li><p>命令、选项和源文件之间使用空格分隔</p>\n</li>\n<li><p>一行命令中可以有零个、一个或多个选项</p>\n</li>\n<li><p>文件名可以包含文件的绝对路径，也可以使用相对路径</p>\n</li>\n<li><p>如果命令中不包含输出可执行文件的文件名，可执行文件的文件名会自动生成一个默认名，Linux平台为<strong>a.out</strong>，Windows平台为<strong>a.exe</strong></p>\n</li>\n</ul>\n<p>gcc、g++编译常用选项说明：</p>\n<table>\n<thead>\n<tr>\n<th><strong>选项</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-o file</td>\n<td>指定生成的输出文件名为file</td>\n</tr>\n<tr>\n<td>-E</td>\n<td>只进行预处理</td>\n</tr>\n<tr>\n<td>-S(大写)</td>\n<td>只进行预处理和编译</td>\n</tr>\n<tr>\n<td>-c(小写)</td>\n<td>只进行预处理、编译和汇编</td>\n</tr>\n</tbody></table>\n<p><strong>1.4、gcc/g++的区别</strong></p>\n<ol>\n<li><p>gcc与g++都可以编译c代码和c++代码, 但是: 后缀为.c的，gcc会把它当做C程序, 而g++当做是C++程序; 后缀为.cpp的，两者都会认为是C++程序.</p>\n</li>\n<li><p>编译阶段，可以使用gcc/g++, g++会自动调用gcc。而链接阶段,可以用g++或者gcc -lstdc++</p>\n<p>,因为gcc命令不能自动和c++程序使用的库链接，通常用g++来完成。</p>\n</li>\n</ol>\n<h2 id=\"2、C-C-编译过程\"><a href=\"#2、C-C-编译过程\" class=\"headerlink\" title=\"2、C/C++编译过程\"></a>2、C/C++编译过程</h2><p><strong>2.1、编译步骤</strong></p>\n<p>C代码编译成可执行程序经过4步：</p>\n<p>1）预处理：宏定义展开、头文件展开、条件编译等，同时将代码中的注释删除，这里并不会检查语法</p>\n<p>2)   编译：检查语法，将预处理后文件编译生成汇编文件</p>\n<p>3）汇编：将汇编文件生成目标文件(二进制文件)</p>\n<p>4）链接：C语言写的程序是需要依赖各种库的，所以编译之后还需要把库链接到最终的可执行程序中去</p>\n<p><img src=\"file:////tmp/wps-yang-pc/ksohtml/wpsOWVxwY.jpg\" alt=\"img\"></p>\n<p><strong>2.2、gcc编译过程</strong></p>\n<p>1) 分步编译</p>\n<p>预处理：<code>gcc -E hello.c -o hello.i</code></p>\n<p>编  译：<code>gcc -S hello.i -o hello.s</code></p>\n<p>汇  编：<code>gcc -c hello.s -o hello.o</code></p>\n<p>链  接：<code>gcc   hello.o -o hello_elf</code></p>\n<table>\n<thead>\n<tr>\n<th><strong>选项</strong></th>\n<th><strong>含义</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>-E</td>\n<td>只进行预处理</td>\n</tr>\n<tr>\n<td>-S(大写)</td>\n<td>只进行预处理和编译</td>\n</tr>\n<tr>\n<td>-c(小写)</td>\n<td>只进行预处理、编译和汇编</td>\n</tr>\n<tr>\n<td>-o file</td>\n<td>指定生成的输出文件名为 file</td>\n</tr>\n</tbody></table>\n<table>\n<thead>\n<tr>\n<th><strong><em>\\</em>文件后缀**</strong></th>\n<th><strong><em>\\</em>含义**</strong></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>.c</td>\n<td>C 语言文件</td>\n</tr>\n<tr>\n<td>.i</td>\n<td>预处理后的 C 语言文件</td>\n</tr>\n<tr>\n<td>.s</td>\n<td>编译后的汇编文件</td>\n</tr>\n<tr>\n<td>.o</td>\n<td>编译后的目标文件</td>\n</tr>\n</tbody></table>\n<p>2) 一步编译</p>\n<p><code>gcc hello.c -o demo</code></p>\n<p><strong>2.3、查找程序所依赖的动态库</strong></p>\n<p>１）Linux平台下，<code>ldd</code>(“l”为字母) 可执行程序</p>\n<p>２）Windows平台下，需要相应软件(<code>Depends.exe</code>)</p>\n<h2 id=\"3、CPU内部结构与寄存器\"><a href=\"#3、CPU内部结构与寄存器\" class=\"headerlink\" title=\"3、CPU内部结构与寄存器\"></a>3、CPU内部结构与寄存器</h2><h3 id=\"3-1、CPU总线\"><a href=\"#3-1、CPU总线\" class=\"headerlink\" title=\"3.1、CPU总线\"></a>3.1、CPU总线</h3><p><strong>数据总线</strong></p>\n<p>（1） 是CPU与内存或其他器件之间的数据传送的通道。</p>\n<p>（2）数据总线的宽度决定了CPU和外界的数据传送速度。</p>\n<p>（3）每条传输线一次只能传输1位二进制数据。eg: 8根数据线一次可传送一个8位二进制数据(即一个字节)。</p>\n<p>（4）数据总线是数据线数量之和。</p>\n<p>数据总线数据总线是CPU与存储器、CPU与I/O接口设备之间传送数据信息(各种指令数据信息)的总线，这些信号通过数据总线往返于CPU与存储器、CPU与I/O接口设备之间，因此，数据总线上的信息是双向传输的。</p>\n<p><strong>地址总线</strong></p>\n<p>（1）CPU是通过地址总线来指定存储单元的。</p>\n<p>（2）地址总线决定了cpu所能访问的最大内存空间的大小。eg: 10根地址线能访问的最大的内存为1024位二进制数据（1024个内存单元）(1B)</p>\n<p>（3）地址总线是地址线数量之和。</p>\n<p>地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。它是单向的，只能从CPU传向外部存储器或I/O端口</p>\n<p>有个说法：64位系统装了64位操作系统，最大物理内存理论上=2的64次方；然而实际上地址总线只用到了35位，所以最大物理内存是32G大小</p>\n<p><strong>控制总线</strong></p>\n<p>（1）CPU通过控制总线对外部器件进行控制。</p>\n<p>（2）控制总线的宽度决定了CPU对外部器件的控制能力。</p>\n<p>（3）控制总线是控制线数量之和。</p>\n<p>控制总线，英文名称：ControlBus，简称：CB。控制总线主要用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和输入输出设备接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的</p>\n<h3 id=\"3-2、64位和32位系统区别\"><a href=\"#3-2、64位和32位系统区别\" class=\"headerlink\" title=\"3.2、64位和32位系统区别\"></a>3.2、64位和32位系统区别</h3><ul>\n<li><p>寄存器是CPU内部最基本的存储单元</p>\n</li>\n<li><p>CPU的主要组成包括了运算器和控制器。运算器是由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。</p>\n</li>\n<li><p>CPU位数=CPU中寄存器的位数=CPU能够一次并行处理的数据宽度（位数）=数据总线宽度</p>\n</li>\n<li><p><strong>CPU的位宽(位数)一般是以 min{ALU位宽、通用寄存器位宽、数据总线位宽}决定的</strong></p>\n</li>\n<li><p>CPU对外是通过总线(地址、控制、数据)来和外部设备交互的，总线的宽度是8位，同时CPU的寄存器也是8位，那么这个CPU就叫8位CPU</p>\n</li>\n<li><p>如果总线是32位，寄存器也是32位的，那么这个CPU就是32位CPU</p>\n</li>\n<li><p>有一种CPU内部的寄存器是32位的，但总线是16位，准32位CPU</p>\n</li>\n<li><p>所有的64位CPU兼容32位的指令，32位要兼容16位的指令，所以在64位的CPU上是可以识别32位的指令</p>\n</li>\n<li><p>在64位的CPU构架上运行了64位的软件操作系统，那么这个系统是64位</p>\n</li>\n<li><p>在64位的CPU构架上，运行了32位的软件操作系统，那么这个系统就是32位</p>\n</li>\n<li><p>64位的软件不能运行在32位的CPU之上</p>\n</li>\n</ul>\n<h3 id=\"3-3、寄存器、缓存、内存三者关系\"><a href=\"#3-3、寄存器、缓存、内存三者关系\" class=\"headerlink\" title=\"3.3、寄存器、缓存、内存三者关系\"></a>3.3、寄存器、缓存、内存三者关系</h3><ol>\n<li><p>寄存器是中央处理器内的组成部份。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。</p>\n</li>\n<li><p>内存包含的范围非常广，一般分为只读存储器（ROM）、随机存储器（RAM）和高速缓存存储器（cache）。</p>\n</li>\n<li><p>寄存器是CPU内部的元件，寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。</p>\n</li>\n<li><p>Cache ：即高速缓冲存储器，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用,这样就减少了CPU的等待时间,提高了系统的效率。Cache又分为一级Cache(L1 Cache)和二级Cache(L2 Cache)，L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上,现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。</p>\n</li>\n</ol>\n<p>总结：大致来说数据是通过内存-Cache-寄存器，Cache缓存则是为了弥补CPU与内存之间运算速度的差异而设置的的部件。</p>\n<h2 id=\"4、内存、地址和指针\"><a href=\"#4、内存、地址和指针\" class=\"headerlink\" title=\"4、内存、地址和指针\"></a>4、内存、地址和指针</h2><h3 id=\"4-1、内存\"><a href=\"#4-1、内存\" class=\"headerlink\" title=\"4.1、内存\"></a>4.1、内存</h3><p>内存含义：</p>\n<ul>\n<li><p>存储器：计算机的组成中，用来存储程序和数据，辅助CPU进行运算处理的重要部分。</p>\n</li>\n<li><p>内存：内部存贮器，暂存程序/数据——掉电丢失 SRAM、DRAM、DDR、DDR2、DDR3。</p>\n</li>\n<li><p>外存：外部存储器，长时间保存程序/数据—掉电不丢ROM、ERRROM、FLASH（NAND、NOR）、硬盘、光盘。</p>\n</li>\n</ul>\n<p>内存是沟通CPU与硬盘的桥梁：</p>\n<ul>\n<li><p>暂存放CPU中的运算数据</p>\n</li>\n<li><p>暂存与硬盘等外部存储器交换的数据</p>\n</li>\n</ul>\n<h3 id=\"4-2、物理存储器和存储地址空间\"><a href=\"#4-2、物理存储器和存储地址空间\" class=\"headerlink\" title=\"4.2、物理存储器和存储地址空间\"></a>4.2、物理存储器和存储地址空间</h3><p>有关内存的两个概念：物理存储器和存储地址空间。</p>\n<p>物理存储器：实际存在的具体存储器芯片。</p>\n<ul>\n<li><p>主板上装插的内存条</p>\n</li>\n<li><p>显示卡上的显示RAM芯片</p>\n</li>\n<li><p>各种适配卡上的RAM芯片和ROM芯片</p>\n</li>\n</ul>\n<p>存储地址空间：对存储器编码的范围。我们在软件上常说的内存是指这一层含义。</p>\n<ul>\n<li><p>编码：对每个物理存储单元（一个字节）分配一个号码</p>\n</li>\n<li><p>寻址：可以根据分配的号码找到相应的存储单元，完成数据的读写</p>\n</li>\n</ul>\n<h3 id=\"4-3、内存地址\"><a href=\"#4-3、内存地址\" class=\"headerlink\" title=\"4.3、内存地址\"></a>4.3、内存地址</h3><ul>\n<li><p>将内存抽象成一个很大的一维字符数组。</p>\n</li>\n<li><p>编码就是对内存的每一个字节分配一个32位或64位的编号（与32位或者64位处理器相关）。</p>\n</li>\n<li><p>这个内存编号我们称之为内存地址。</p>\n</li>\n</ul>\n<p>内存中的每一个数据都会分配相应的地址：</p>\n<ul>\n<li><p>char:占一个字节分配一个地址</p>\n</li>\n<li><p>int: 占四个字节分配四个地址</p>\n</li>\n<li><p>float、struct、函数、数组等</p>\n</li>\n</ul>\n<h3 id=\"4-4、指针和指针变量\"><a href=\"#4-4、指针和指针变量\" class=\"headerlink\" title=\"4.4、指针和指针变量\"></a>4.4、指针和指针变量</h3><ul>\n<li><p>内存区的每一个字节都有一个编号，这就是“地址”。</p>\n</li>\n<li><p>如果在程序中定义了一个变量，在对程序进行编译或运行时，系统就会给这个变量分配内存单元，并确定它的内存地址(编号)</p>\n</li>\n<li><p>指针的实质就是内存“地址”。指针就是地址，地址就是指针。</p>\n</li>\n<li><p>指针是内存单元的编号，指针变量是存放地址的变量。</p>\n</li>\n<li><p><strong>通常我们叙述时会把指针变量简称为指针，实际他们含义并不一样</strong></p>\n</li>\n</ul>\n<h2 id=\"5、存储类型\"><a href=\"#5、存储类型\" class=\"headerlink\" title=\"5、存储类型\"></a>5、存储类型</h2><h3 id=\"5-1、auto\"><a href=\"#5-1、auto\" class=\"headerlink\" title=\"5.1、auto\"></a>5.1、auto</h3><ol>\n<li>普通局部变量，自动存储，该对象会自动创建和销毁，调用函数时分配内存，函数结束时释放内存。只在{}内有效，存放在堆栈中一般省略auto,  不会被默认初始化，初值不随机</li>\n<li>全局变量，<strong>不允许声明为auto变量</strong>， register不适用于全局变量，生命周期由定义到程序运行结束，没有初始化会<strong>自动赋值0或空字符</strong>。全局变量属于整个程序，不同文件中不能有同名的全局变量，通过<strong>extern</strong>在其他文件中引用使用</li>\n</ol>\n<h3 id=\"5-2、static\"><a href=\"#5-2、static\" class=\"headerlink\" title=\"5.2、static\"></a>5.2、static</h3><ol>\n<li>静态局部变量，生命周期由定义到程序运行结束，在编译时赋初值，<strong>只初始化一次，没有初始化会自动赋值0或空字符</strong>。只在当前{}内有效</li>\n<li>静态全局变量，生命周期由定义到程序运行结束，在编译时赋初值，只初始化一次，没有初始化会自动赋值0或空字符。从定义到文件结尾起作用，在一个程序中的其他文件中可以定义同名的静态全局变量，因为作用于不冲突。</li>\n</ol>\n<h3 id=\"5-3、extern\"><a href=\"#5-3、extern\" class=\"headerlink\" title=\"5.3、extern\"></a>5.3、extern</h3><ol>\n<li><p>外部变量声明，是指这是一个已在别的地方定义过的对象，这里只是对变量的一次重复引用，不会产生新的变量。</p>\n</li>\n<li><p>使用extern时，注意不能重复定义，否则编译报错</p>\n<pre><code class=\"c\">//    程序文件一：\n    extern int a = 10; //编译警告，extern的变量最好不要初始化\n//    程序文件二：\n    extern int a = 20; //重复定义，应改为extern int a;</code></pre>\n</li>\n<li><p>如果我们希望该外部变量只能在本文件内使用，而不能被其他文件引用可以在外部变量定义时加static声明。防止别人写的模块误用。</p>\n</li>\n<li><p>在函数外部定义的全局变量，作用域开始于变量定义，结束于程序文件的结束。我们可以extern来声明外部变量来扩展它的作用域。同一个文件内，extern声明之后就可以作用域扩大到声明处到文件结束。比如在一个函数之后定义外部变量a，之后的函数可以使用该变量，但是之前的函数不能使用，加extern可以解决。</p>\n<pre><code class=\"c\">#include &lt;stdio.h&gt;\n\nextern int g1;\nint main(void)\n{\n    extern int g2;\n    printf(&quot;%d,%d\\n&quot;, g1,g2);\n    return 0;\n}\nint g1 = 77;\nint g2 = 88;</code></pre>\n</li>\n</ol>\n<ol start=\"5\">\n<li>多个文件时，可以在未定义该外部变量的文件内做extern声明即可以使用。但是需要注意可能执行一个文件时改变了该全局变量的值，影响其他文件的调用。编译时遇到extern，会先在文件内找是否定义了该外部变量。如果未找到则在链接时在其他文件中找。</li>\n</ol>\n<h3 id=\"5-4、register\"><a href=\"#5-4、register\" class=\"headerlink\" title=\"5.4、register\"></a>5.4、register</h3><ol>\n<li><p>寄存器变量，请求编译器将这个变量保存在CPU的寄存器中，从而加快程序的运行.只是建议CPU这样做，非强制,声明变量为register,编译器并不一定会将它处理为寄存器变量</p>\n</li>\n<li><p>动态和静态变量都是存放在内存中，程序中遇到该值时用控制器发指令将变量的值送到运算器中，需要存数再保存到内存中。如果频繁使用一个变量，比如一个函数体内的多次循环每次都引用该局部变量，我们则可以把局部变量的值放到CPU的寄存器中，叫寄存器变量。不需要多次到内存中存取提高效率。</p>\n</li>\n<li><p>但是只能局部自动变量和形参可以做寄存器变量。在函数调用时占用一些寄存器，函数结束时释放。不同系统对register要求也不一样，比如对定义register变量个数，数据类型等限制，有的默认为自动变量处理。所以在程序一般也不用。</p>\n</li>\n<li><p>register是不能取址的。比如 <code>int i</code>；(自动为auto)<code>int *p=&amp;i;</code>是对的， 但<code>register int j; int *p = &amp;j;</code>是错的，因为无法对寄存器的定址。</p>\n<pre><code class=\"c\">#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\n#define TIME 1000000000\n\nint m, n = TIME; /* 全局变量 */\nint main(void)\n{\n    time_t start, stop;\n    register int a, b = TIME; /* 寄存器变量 */\n    int x, y = TIME;          /* 一般变量   */\n\n    time(&amp;start);\n    for (a = 0; a &lt; b; a++);\n    time(&amp;stop);\n    printf(&quot;寄存器变量用时: %d 秒\\n&quot;, stop - start);\n    time(&amp;start);\n    for (x = 0; x &lt; y; x++);\n    time(&amp;stop);\n    printf(&quot;一般变量用时: %d 秒\\n&quot;, stop - start);\n    time(&amp;start);\n    for (m = 0; m &lt; n; m++);\n    time(&amp;stop);\n    printf(&quot;全局变量用时: %d 秒\\n&quot;, stop - start);\n\n    return 0;\n}</code></pre>\n</li>\n</ol>\n<h3 id=\"5-5、volatile\"><a href=\"#5-5、volatile\" class=\"headerlink\" title=\"5.5、volatile\"></a>5.5、volatile</h3><pre><code class=\"c\">    程序在使用变量时, 特别是连续多次使用变量时, 一般是载入寄存器, 直接从寄存器存取, 之后再还回内存;但如果此变量在返回内存时, 假如内存中的值已经改变了(从外部修改了)怎么办?\n为了避免这种情况的发生, 可以用 volatile 说明此变量, 以保证变量的每次使用都是直接从内存存取.\n但这样肯定会影响效率, 幸好它并不常用.\n\n另外: 如果 const volatile 同时使用, 这表示此变量只接受外部的修改.\n\n#include &lt;stdio.h&gt;\n\nvolatile int num = 123;\nint main(void)\n{    \n    printf(&quot;%d\\n&quot;, num);\n    getchar();\n    return 0;\n}</code></pre>\n<h3 id=\"5-6、总结\"><a href=\"#5-6、总结\" class=\"headerlink\" title=\"5.6、总结\"></a>5.6、总结</h3><table>\n<thead>\n<tr>\n<th>关键字</th>\n<th>类型</th>\n<th>生命周期</th>\n<th>作用域</th>\n<th>修饰对象</th>\n<th>所属区</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>auto[可省略]</td>\n<td>普通局部变量</td>\n<td>定义到{}运行结束</td>\n<td>｛｝</td>\n<td>变量</td>\n<td>栈区</td>\n</tr>\n<tr>\n<td>static</td>\n<td>静态局部变量</td>\n<td>定义到程序运行结束</td>\n<td>｛｝</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td></td>\n<td>全局变量</td>\n<td>定义到程序运行结束</td>\n<td>定义到文件结尾</td>\n<td></td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>extern</td>\n<td>全局变量</td>\n<td>定义到程序运行结束</td>\n<td>声明处到文件结尾</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>static</td>\n<td>全局变量</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td>变量和函数</td>\n<td>初始化在data段，未初始化在BSS段</td>\n</tr>\n<tr>\n<td>register</td>\n<td>寄存器变量</td>\n<td>定义到{}运行结束</td>\n<td>｛｝</td>\n<td>变量</td>\n<td>运行时存储在CPU寄存器</td>\n</tr>\n<tr>\n<td>extern</td>\n<td>函数</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td></td>\n<td>代码区</td>\n</tr>\n<tr>\n<td>static</td>\n<td>函数</td>\n<td>整个程序运行期</td>\n<td>声明处到文件结尾</td>\n<td></td>\n<td>代码区</td>\n</tr>\n</tbody></table>\n<h2 id=\"6、内存分区\"><a href=\"#6、内存分区\" class=\"headerlink\" title=\"6、内存分区\"></a>6、内存分区</h2><p>C代码经过预处理、编译、汇编、链接4步后生成一个可执行程序。在 Linux 下，程序是一个普通的可执行文件，以下列出一个二进制可执行文件的基本情况：</p>\n<p><img src=\"01.png\" alt></p>\n<p>通过上图可以得知，在没有运行程序前，也就是说程序没有加载到内存前，可执行程序内部已经分好3段信息，分别为代码区（text）、数据区（data）和未初始化数据区（bss）3 个部分（有些人直接把data和bss合起来叫做静态区或全局区）。</p>\n<ul>\n<li><p><strong>代码区</strong></p>\n<p>存放 CPU 执行的机器指令。通常代码区是<strong>可共享</strong>的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区通常是<strong>只读的</strong>，使其只读的原因是防止程序意外地修改了它的指令。另外，代码区还规划了局部变量的相关信息。</p>\n</li>\n<li><p><strong>全局初始化数据区/静态数据区（data段）</strong></p>\n<p>该区包含了在程序中明确被初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。</p>\n</li>\n<li><p><strong>未初始化数据区（又叫 bss 区）</strong></p>\n<p>存入的是全局未初始化变量和未初始化静态变量。未初始化数据区的数据在程序开始执行之前被内核初始化为 0 或者空（NULL）。</p>\n</li>\n</ul>\n<p>程序在加载到内存前，代码区和全局区(data和bss)的大小就是固定的，程序运行期间不能改变。然后，运行可执行程序，系统把程序加载到内存，除了根据可执行程序的信息分出代码区（text）、数据区（data）和未初始化数据区（bss）之外，还额外增加了栈区、堆区。</p>\n<p><img src=\"02.png\" alt></p>\n<ul>\n<li><p>代码区（text segment）</p>\n<p>加载的是可执行文件代码段，所有的可执行代码都加载到代码区，这块内存是不可以在运行期间修改的。</p>\n</li>\n<li><p>只读数据区（文字常量区 RO data）</p>\n<p>只读数区是程序使用的一些不会被更改的数据。一般是const修饰的变量以及程序中使用的文字常量。</p>\n</li>\n<li><p>已初始化数据区 （RW data）</p>\n<p>加载的是可执行文件数据段，存储于数据段（全局初始化，静态初始化数据）的数据的生存周期为整个程序运行过程。</p>\n</li>\n<li><p>未初始化数据区（BSS）</p>\n<p>加载的是可执行文件BSS段，位置可以分开亦可以紧靠数据段，存储于数据段的数据（全局未初始化，静态未初始化数据）的生存周期为整个程序运行过程。</p>\n</li>\n<li><p>堆区（heap）</p>\n<p>堆是一个大容器，它的容量要远远大于栈，但没有栈那样先进后出的顺序。用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时由操作系统回收。</p>\n</li>\n<li><p>栈区（stack）</p>\n<p>栈是一种先进后出的内存结构，由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。</p>\n</li>\n</ul>\n<h2 id=\"7、结构体字节对齐\"><a href=\"#7、结构体字节对齐\" class=\"headerlink\" title=\"7、结构体字节对齐\"></a>7、结构体字节对齐</h2><p><strong>7.1、内存对齐原因</strong></p>\n<pre><code class=\"c\">struct data\n{\n    char c;\n    int i;\n};\n\nint main()\n{\n    printf(&quot;%d\\n&quot;, sizeof(struct data)); // 5还是 8?\n\n    return 0;\n}</code></pre>\n<p>在Linux 32位架构下，假设变量stu存放在内存中的起始地址为0x00，那么c的起始地址为0x00、i的起始地址为0x01，变量stu共占用了5个字节：</p>\n<ul>\n<li><p>对变量c访问：CPU只需要一个读周期</p>\n</li>\n<li><p>变量i访问：</p>\n</li>\n</ul>\n<ol>\n<li><p>首先CPU用一个读周期，从0x00处读取了4个字节(32位架构)，然后将0x01-0x03的3个字节暂存。</p>\n</li>\n<li><p>再花一个读周期读取了从0x04-0x07的4字节数据，将0x04这个字节与刚刚暂存的3个字节进行拼接从而读取到成员变量i的值。</p>\n</li>\n<li><p>读取一个成员变量i，CPU却花费了2个读周期。</p>\n</li>\n</ol>\n<p>如果数据成员i的起始地址被放在了0x04处</p>\n<ul>\n<li><p>读取c成员，花费周期为1  </p>\n</li>\n<li><p>读取i所花费的周期也变成了1</p>\n</li>\n<li><p>引入字节对齐可以避免读取效率的下降，同时也浪费了3个字节的空间(0x01-0x03)</p>\n<p><strong>结构体内部成员对齐是为了实现用空间换取时间。</strong></p>\n</li>\n</ul>\n<p><strong>7.2、内存对齐原则</strong></p>\n<ul>\n<li><p>原则1：数据成员的对齐规则</p>\n<p>1) 最大对齐单位以CPU架构对齐，如Linux 32位最大以4字节对齐，Linux 64位最大以8字节对齐，vs(32位、64位)最大对齐单位为8字节</p>\n<p>2) 需要和<strong>结构体的最大成员</strong>和CPU架构(32位或64位)对比，取小的作为对齐单位</p>\n<p>3) 字节对齐也可以通过程序控制，采用指令：</p>\n<pre><code class=\"c\">#pragma pack(xx)   \n#pragma pack(1)     //1字节对齐\n#pragma pack(2)     //2字节对齐\n#pragma pack(4)     //4字节对齐\n#pragma pack(8)     //8字节对齐\n#pragma pack(16)    //16字节对齐\n\n</code></pre>\n</li>\n</ul>\n<p>  #include &lt;stdio.h&gt;</p>\n<p>  #include &lt;stdlib.h&gt;</p>\n<p>  #pragma pack(2)</p>\n<p>  typedef struct<br>  {<br>    int aa1; //4个字节对齐 1111<br>    char bb1;//1个字节对齐 1<br>    short cc1;//2个字节对齐 011<br>    char dd1; //1个字节对齐 1<br>    } testlength;</p>\n<p>  int length = sizeof(testlength); //2个字节对齐，length = 10</p>\n<p>  int main(){<br>      printf(“length=%d\\n”, length); // length=10<br>  }</p>\n<pre><code>\n  尽管通过pragma pack(xx)可以指定字节对齐单位，但需要和结构体的最大成员、CPU架构(32位或64位)对比，取最小的作为对齐单位。\n\n- 原则2：数据成员的偏移起点\n\n  结构体（struct）的数据成员，第一个数据成员放在偏移量为0的地方，以后每个数据成员存放在偏移量为该数据成员类型大小的整数倍的地方（比如int在32位机器为４字节，则要从4的整数倍地址开始存储）\n\n- 原则3：收尾工作 \n\n  结构体的总大小，也就是sizeof的结果，必须是对齐单位的整数倍，不足的要补齐。\n\n\n\n## 8、typedef与define的区别\n\n- typedef为C语言的关键字，作用是为一种数据类型(基本类型或自定义数据类型)定义一个新名字，不能创建新类型。\n\n- 与#define不同，typedef仅限于数据类型，而不是能是表达式或具体的值\n\n- define发生在预处理，typedef发生在编译阶段\n\n\n\n## 9、C语言文本操作的区别\n\n### 9.1 二进制文件和文本文件\n\n\n\n- b是二进制模式的意思，b只是在Windows有效，在Linux用r和rb的结果是一样的\n\n- Unix和Linux下所有的文本文件行都是\\n结尾，而Windows所有的文本文件行都是\\r\\n结尾\n\n- 在Windows平台下，以“文本”方式打开文件，不加b：\n\n  - 当读取文件的时候，系统会将所有的 &quot;\\r\\n&quot; 转换成 &quot;\\n&quot;\n\n  - 当写入文件的时候，系统会将 &quot;\\n&quot; 转换成 &quot;\\r\\n&quot; 写入 \n\n  - 以&quot;二进制&quot;方式打开文件，则读\\写都不会进行这样的转换\n\n- 在Unix/Linux平台下，“文本”与“二进制”模式没有区别，&quot;\\r\\n&quot; 作为两个字符原样输入输出\n\n### 9.2 文本结尾\n\n在C语言中，EOF表示文件结束符(end of file)。在while循环中以EOF作为文件结束标志，这种以EOF作为文件结束标志的文件，必须是文本文件。在文本文件中，数据都是以字符的ASCII代码值的形式存放。我们知道，ASCII代码值的范围是0~127，不可能出现-1，因此可以用EOF作为文件结束标志。\n\n`#define EOF (-1)`\n\n当把数据以二进制形式存放到文件中时，就会有-1值的出现，因此不能采用EOF作为二进制文件的结束标志。为解决这一个问题，ANSI C提供一个feof函数，用来判断文件是否结束。feof函数既可用以判断二进制文件又可用以判断文本文件。\n\n```c\n#include &lt;stdio.h&gt;\nint feof(FILE * stream);\n功能：检测是否读取到了文件结尾。判断的是最后一次“读操作的内容”，不是当前位置内容(上一个内容)。\n参数：\n    stream：文件指针\n返回值：\n    非0值：已经到文件结尾\n    0：没有到文件结尾</code></pre><h2 id=\"10、void\"><a href=\"#10、void\" class=\"headerlink\" title=\"10、void\"></a>10、void</h2><p><strong>void的作用</strong></p>\n<ul>\n<li>对函数参数的限定：当不需要传入参数时，即 <code>function (void);</code></li>\n<li>对函数返回值的限定：当函数没有返回值时，即 <code>void function(void);</code></li>\n</ul>\n<p><strong>void指针的作用</strong></p>\n<p>（1）void指针可以指向任意的数据类型，即任意类型的指针可以赋值给void指针</p>\n<pre><code class=\"c\">int *a;\nvoid *p;\np=a;</code></pre>\n<p>如果void指针赋值给其他类型，则需要强制转换；<code>a=（int *）p;</code></p>\n<p>（2）在ANSI C标准中不允许对void指针进行算术运算，因为没有特定的数据类型，即在内存中不知道移动多少个字节；而在GNU标准中，认为void指针和char指针等同。</p>\n<p><strong>应用</strong></p>\n<p>（1）void指针一般用于应用的底层，比如malloc函数的返回类型是void指针，需要再强制转换； </p>\n<p>（2）文件句柄HANDLE也是void指针类型，这也是句柄和指针的区别； </p>\n<p>（3）内存操作函数的原型也需要void指针限定传入参数：</p>\n<pre><code class=\"c\">void * memcpy (void *dest, const void *src, size_t len);\nvoid * memset (void *buffer, int c, size_t num );</code></pre>\n<p>（4）面向对象函数中底层对基类的抽象。</p>\n<h2 id=\"11、数据类型的本质\"><a href=\"#11、数据类型的本质\" class=\"headerlink\" title=\"11、数据类型的本质\"></a>11、数据类型的本质</h2><ul>\n<li><p>数据类型可理解为创建变量的模具：是固定内存大小的别名。</p>\n</li>\n<li><p>数据类型的作用：编译器预算对象（变量）分配的内存空间大小。</p>\n</li>\n</ul>\n<h2 id=\"12、变量的本质\"><a href=\"#12、变量的本质\" class=\"headerlink\" title=\"12、变量的本质\"></a>12、变量的本质</h2><p>变量的本质：一段连续内存空间的别名。</p>\n<pre><code>1）程序通过变量来申请和命名内存空间 int a = 0\n\n2）通过变量名访问内存空间\n\n3）不是向变量读写数据，而是向变量所代表的内存空间中读写数据</code></pre><h2 id=\"13、数组与指针的关系\"><a href=\"#13、数组与指针的关系\" class=\"headerlink\" title=\"13、数组与指针的关系\"></a>13、数组与指针的关系</h2><p>数组不是指针，数组名也只有在表达式中才会被当成一个指针常量。数组名在表达式中使用的时候，编译器才会产生一个指针常量。</p>\n<ul>\n<li><p><code>p[i]</code>这种写法只不过是<code>*(p + i)</code>的简便写法。实际上，至少对于编译器来说，[]这样的运算符完全可以不存在。[]运算符是为了方便人们读写而引入的，是一种语法糖。</p>\n</li>\n<li><p>当数组名作为sizeof操作符的操作数的时候，此时sizeof返回的是整个数组的长度，而不是指针数组指针的长度。</p>\n</li>\n<li><p>当数组名作为 &amp; 操作符的操作数的时候，此时返回的是一个指向数组的指针，而不是指向某个数组元素的指针常量。</p>\n</li>\n<li><p>二级指针是指向指针的指针，而指针数组则是元素类型为指针的数组。虽然它们是不一样的，但是在表达式中，它们是等效的。</p>\n</li>\n</ul>\n<h2 id=\"14、字节序-大端、小端\"><a href=\"#14、字节序-大端、小端\" class=\"headerlink\" title=\"14、字节序(大端、小端)\"></a>14、字节序(大端、小端)</h2><p><strong>14.1 大端和小端</strong></p>\n<p>计算机的内存最小单位是字节。字节序是指多字节(大于1字节)数据的存储顺序，在设计计算机系统的时候，有两种处理内存中数据的方法：大端格式、小端格式。</p>\n<ul>\n<li><p>小端格式(Little-Endian)：将低位字节数据存储在低地址。X86和ARM都是小端对齐。</p>\n</li>\n<li><p>大端格式(Big-Endian)：将高位字节数据存储在低地址。很多Unix服务器的CPU是大端对齐的、网络上数据是以大端对齐。</p>\n</li>\n</ul>\n<p><img src=\"03.png\" alt></p>\n<p>对于整形 0x12345678，它在大端格式和小端格式的系统中，分别如下图所示的方式存放：</p>\n<p><img src=\"04.png\" alt></p>\n<p><strong>14.2 网络字节序和主机字节序</strong></p>\n<p>网络字节顺序NBO(Network Byte Order)</p>\n<pre><code>在网络上使用统一的大端模式，低字节存储在高地址，高字节存储在低地址。</code></pre><p>主机字节序顺序HBO(Host Byte Order)</p>\n<pre><code>不同的机器HBO不相同，与CPU设计相关，数据的顺序是由CPU决定的，而与操作系统无关。</code></pre><p>处理器 |操作系统  |字节排序|</p>\n<p>Alpha    全部    Little endian<br>HP-PA    NT    Little endian<br>HP-PA    UNIX    Big endian<br>Intelx86    全部    Little endian &lt;—–x86系统是小端字节序系统<br>Motorola680x()    全部    Big endian<br>MIPS    NT    Little endian<br>MIPS    UNIX    Big endian<br>PowerPC    NT    Little endian<br>PowerPC    非NT    Big endian  &lt;—–PPC系统是大端字节序系统<br>RS/6000    UNIX    Big endian<br>SPARC    UNIX    Big endian<br>IXP1200 ARM核心    全部    Little endian </p>\n<pre><code class=\"c\">相关函数：\nhtons 把unsigned short类型从主机序转换到网络序\nhtonl 把unsigned long类型从主机序转换到网络序\nntohs 把unsigned short类型从网络序转换到主机序\nntohl 把unsigned long类型从网络序转换到主机序\n\n头文件：#include &lt;netinet/in.h&gt;\n定义函数：unsigned short ntohs(unsigned short netshort);\n函数说明：ntohs()用来将参数指定的16 位netshort 转换成主机字符顺序.\n返回值：返回对应的主机顺序.\n范例：参考getservent().</code></pre>\n<pre><code class=\"c\">#include &lt;stdio.h&gt;\n#include &lt;winsock.h&gt; // windows使用winsock.h\n\nint main()\n{\n    //左边是高位，右边是低位，高位放高地址，低位放低地址\n    int a = 0x11223344;\n    unsigned char *p = (unsigned char *)&amp;a;\n\n    int i;\n    for (i = 0; i &lt; 4; i++)\n    {\n        printf(&quot;%x\\n&quot;, p[i]);\n    }\n\n    u_long b = htonl(a);\n    unsigned char *q = (unsigned char *)&amp;b;\n    for (i = 0; i&lt;4; i++){\n        printf(&quot;%x\\n&quot;, q[i]);\n    }\n\n    return 0;\n}\n\n//  gcc hello.c -lwsock32 -o hello\n// 编译时添加-lwsock32，不然会报错undefined reference to `htonl@4&#39;\n// 在编译socket程序的时候，一定要加上-l wsock32选项，因为mingw默认没有包含windows库</code></pre>\n<h2 id=\"15、数组指针\"><a href=\"#15、数组指针\" class=\"headerlink\" title=\"15、数组指针\"></a>15、数组指针</h2><pre><code class=\"c\">// 1) 先定义数组类型，再根据类型定义指针变量\ntypedef int A[10];\nA *p = NULL;\n\n// 2) 先定义数组指针类型，根据类型定义指针变量\ntypedef int(*P)[10]; //第一个()代表指针，第二个[]代表数组\nP q; //数据组指针变量\n\n// 3) 直接定义数组指针变量\nint (*q)[10];\n</code></pre>\n<h2 id=\"16、深拷贝和浅拷贝\"><a href=\"#16、深拷贝和浅拷贝\" class=\"headerlink\" title=\"16、深拷贝和浅拷贝\"></a>16、深拷贝和浅拷贝</h2><p><strong>结构体</strong>:</p>\n<p>浅拷贝  不同结构体成员指针变量指向同一块内存</p>\n<p>深拷贝  不同结构体成员指针变量指向不同的内存</p>\n<p><strong>类</strong>:</p>\n<p>浅拷贝 类中有动态分配的空间的指针指向相同</p>\n<p>深拷贝 类中有动态分配的空间的指针指向不同的内存空间</p>\n<h2 id=\"17、-include-lt-gt-与-include-“”的区别\"><a href=\"#17、-include-lt-gt-与-include-“”的区别\" class=\"headerlink\" title=\"17、#include&lt; &gt; 与 #include “”的区别\"></a>17、#include&lt; &gt; 与 #include “”的区别</h2><ul>\n<li><p>“” 表示系统先在file1.c所在的当前目录找file1.h，如果找不到，再按系统指定的目录检索。</p>\n</li>\n<li><p>&lt; &gt; 表示系统直接按系统指定的目录检索。</p>\n</li>\n</ul>\n<p>注意：</p>\n<ul>\n<li>#include &lt;&gt;常用于包含库函数的头文件</li>\n<li>#include “”常用于包含自定义的头文件</li>\n<li>理论上#include可以包含任意格式的文件(.c .h等) ，但我们一般用于头文件的包含。</li>\n</ul>\n<h2 id=\"18、静态库和动态库\"><a href=\"#18、静态库和动态库\" class=\"headerlink\" title=\"18、静态库和动态库\"></a>18、静态库和动态库</h2><p><strong>18.1、静态库优缺点</strong></p>\n<ul>\n<li><p>静态库在程序的链接阶段被复制到了程序中，和程序运行的时候没有关系；</p>\n</li>\n<li><p>程序在运行时与函数库再无瓜葛，移植方便；</p>\n</li>\n<li><p>浪费空间和资源，所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。</p>\n</li>\n</ul>\n<p><strong>18.2、动态库</strong></p>\n<p>要解决空间浪费和更新困难这两个问题，最简单的办法就是把程序的模块相互分割开来，形成独立的文件，而不是将他们静态的链接在一起。</p>\n<p>简单地讲，就是不对哪些组成程序的目标程序进行链接，等程序运行的时候才进行链接。也就是说，把整个链接过程推迟到了运行时再进行，这就是动态链接的基本思想。</p>\n<p><strong>18.3、动态库的lib文件和静态库的lib文件的区别</strong></p>\n<p>在使用动态库的时候，往往提供两个文件：一个引入库（.lib）文件（也称“导入库文件”）和一个DLL（.dll）文件。 </p>\n<p>虽然引入库的后缀名也是“lib”，但是，动态库的引入库文件和静态库文件有着本质的区别，对一个DLL文件来说，其引入库文件（.lib）包含该DLL导出的函数和变量的符号名，而.dll文件包含该DLL实际的函数和数据。</p>\n<p>在使用动态库的情况下，在编译链接可执行文件时，只需要链接该DLL的引入库文件，该DLL中的函数代码和数据并不复制到可执行文件，直到可执行程序运行时，才去加载所需的DLL，将该DLL映射到进程的地址空间中，然后访问DLL中导出的函数。</p>\n<h1 id=\"二、C-篇\"><a href=\"#二、C-篇\" class=\"headerlink\" title=\"二、C++篇\"></a>二、C++篇</h1><h2 id=\"1、c语言和c-语言的关系\"><a href=\"#1、c语言和c-语言的关系\" class=\"headerlink\" title=\"1、c语言和c++语言的关系\"></a>1、c语言和c++语言的关系</h2><p>“c++”中的++来自于c语言中的递增运算符++，该运算符将变量加1。c++起初也叫”c with clsss”。通过名称表明，c++是对C的扩展，因此c++是c语言的超集，这意味着任何有效的c程序都是有效的c++程序。c++程序可以使用已有的c程序库。</p>\n<p>c++语言在c语言的基础上添加了<strong>面向对象编程</strong>和<strong>泛型编程</strong>的支持。c++继承了c语言高效，简洁，快速和可移植的传统。</p>\n<p>c++融合了3种不同的编程方式:</p>\n<ul>\n<li><p>c语言代表的过程性语言.</p>\n</li>\n<li><p>c++在c语言基础上添加的类代表的面向对象语言.</p>\n</li>\n<li><p>c++模板支持的泛型编程。</p>\n</li>\n</ul>\n<h2 id=\"2、左值和右值\"><a href=\"#2、左值和右值\" class=\"headerlink\" title=\"2、左值和右值\"></a>2、左值和右值</h2><p>判断是否是左值，有一个简单的方法，就是看看能否取它的地址，能取地址就是左值，否则就是右值。</p>\n<p>当一个对象成为右值时，使用的是它的值(内容), 而成为左值时，使用的是它的身份（在内存中的位置）。</p>\n<p>平常所说的引用，实际上指的就是左值引用<code>lvalue reference</code>, 常用单个&amp;来表示。左值引用只能接收左值，不能接收右值。<strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></p>\n<p><strong>为了支持移动操作，在c++11版本，增加了右值引用。</strong>右值引用一般用于绑定到一个即将销毁的对象，所以右值引用又通常出现在移动构造函数中。</p>\n<p>看完下面的例子，左值和右值基本就清楚了，左值具有持久的状态，有独立的内存空间，右值要么是字面常量，要么就是表达式求值过程中创建的临时对象</p>\n<pre><code class=\"c++\">int i = 66;\nint &amp;r = i ; //r 是一个左引用，绑定左值 i\n\nint &amp;&amp;rr = i ; //rr是一个右引用，绑定到左值i , 错误！\nint &amp;r2 = i*42 ; //  r2 是一个左引用， 而i*42是一个表达式，计算出来的结果是一个右值。 错误！\n\nconst int &amp;r3 = i*42; // const修饰的左值引用 正确\nint &amp;&amp;rr2 = i*42 ; // 右引用，绑定右值 正确</code></pre>\n<h2 id=\"3、c-对c的扩展\"><a href=\"#3、c-对c的扩展\" class=\"headerlink\" title=\"3、c++对c的扩展\"></a>3、c++对c的扩展</h2><p><strong>3.1 三目运算符</strong></p>\n<p>c语言三目运算表达式返回值为数据值，为右值，不能赋值。</p>\n<p>c++语言三目运算表达式返回值为变量本身(引用)，为左值，可以赋值。</p>\n<pre><code class=\"c++\">int a = 10;\nint b = 20;\nprintf(&quot;ret:%d\\n&quot;, a &gt; b ? a : b);\n//思考一个问题，(a &gt; b ? a : b) 三目运算表达式返回的是什么？\n\ncout &lt;&lt; &quot;b:&quot; &lt;&lt; b &lt;&lt; endl;\n//返回的是左值，变量的引用\n(a &gt; b ? a : b) = 100;//返回的是左值，变量的引用\ncout &lt;&lt; &quot;b:&quot; &lt;&lt; b &lt;&lt; endl;</code></pre>\n<p><strong>3.2 bool</strong></p>\n<p>c++中新增bool类型关键字</p>\n<ul>\n<li>bool类型只有两个值，true(1)， false（0）</li>\n<li>bool类型占1个字节</li>\n<li>给bool类型赋值时, 非0值会自动转换为true(1), 0值会自动转换为false（0）</li>\n</ul>\n<p>C语言中也有bool类型，在c99标准之前是没有bool关键字，c99标准已经有bool类型，包含头文件<code>stdbool.h</code>,就可以使用和c++一样的bool类型。</p>\n<p><strong>3.3 struct类型增强</strong></p>\n<ul>\n<li><p>c中定义结构体变量需要加上struct关键字，c++不需要。</p>\n</li>\n<li><p>c中的结构体只能定义成员变量，不能定义成员函数。c++即可以定义成员变量，也可以定义成员函数。</p>\n</li>\n</ul>\n<p><strong>3.4 更严格的类型转换</strong></p>\n<p>在C++中，不同类型的变量一般是不能直接赋值的，需要相应的强转。</p>\n<p>在C++中，所有的变量和函数都必须有类型</p>\n<p><strong>3.4 全局变量检测增强</strong></p>\n<pre><code class=\"c++\">int a = 10; //赋值，当做定义\nint a; //没有赋值，当做声明\n\nint main(){\n    printf(&quot;a:%d\\n&quot;,a);\n    return EXIT_SUCCESS;\n}\n\n// 上面的代码在c++下编译失败，在c下编译通过。</code></pre>\n<h2 id=\"4、内部连接和外部连接\"><a href=\"#4、内部连接和外部连接\" class=\"headerlink\" title=\"4、内部连接和外部连接\"></a>4、内部连接和外部连接</h2><p>内部连接：如果一个名称对编译单元(.cpp)来说是局部的，在链接的时候其他的编译单元无法链接到它且不会与其它编译单元(.cpp)中的同样的名称相冲突。例如static函数，inline函数等（注 : 用static修饰的函数，本限定在本源码文件中，不能被本源码文件以外的代码文件调用。而普通的函数，默认是extern的，也就是说，可以被其它代码文件调用该函数。）</p>\n<p>外部连接：如果一个名称对编译单元(.cpp)来说不是局部的，而在链接的时候其他的编译单元可以访问它，也就是说它可以和别的编译单元交互。 例如全局变量就是外部链接 。</p>\n<h2 id=\"5、C-C-中const的区别\"><a href=\"#5、C-C-中const的区别\" class=\"headerlink\" title=\"5、C/C++中const的区别\"></a>5、C/C++中const的区别</h2><p>1、const全局变量</p>\n<p>c语言全局const会被存储到只读数据区。c++中全局const当声明extern或者对变量取地址时，编译器会分配存储地址，变量存储在只读数据段。两个都受到了只读数据区的保护，不可修改。</p>\n<pre><code class=\"c++\">const int constA = 10;\nint main(){\n    int* p = (int*)&amp;constA;\n    *p = 200;\n}\n\n// 以上代码在c/c++中编译通过，在运行期，修改constA的值时，发生写入错误。原因是修改只读数据段的数据。</code></pre>\n<p>2、const 局部变量</p>\n<p>c语言中局部const存储在堆栈区，只是不能通过变量直接修改const只读变量的值，但是可以跳过编译器的检查，通过指针间接修改const值。</p>\n<pre><code class=\"c++\">const int constA = 10;\nint* p = (int*)&amp;constA;\n*p = 300;\nprintf(&quot;constA:%d\\n&quot;,constA);\nprintf(&quot;*p:%d\\n&quot;, *p);\n\n// constA:300\n// *p:300</code></pre>\n<p>c++中对于局部的const变量要区别对待：</p>\n<ul>\n<li>对于基础数据类型，也就是const int a = 10这种，编译器会把它放到符号表中，不分配内存，当对其取地址时，会分配内存。</li>\n</ul>\n<pre><code class=\"c++\">const int constA = 10;\nint* p = (int*)&amp;constA;\n*p = 300;\ncout &lt;&lt; &quot;constA:&quot; &lt;&lt; constA &lt;&lt; endl;\ncout &lt;&lt; &quot;*p:&quot; &lt;&lt; *p &lt;&lt; endl;\n\n// constA:10\n// *p:300\n// constA在符号表中，当我们对constA取地址，这个时候为constA分配了新的空间，*p操作的是分配的空间，而constA是从符号表获得的值。</code></pre>\n<p>​    </p>\n<ul>\n<li>对于基础数据类型，如果用一个变量初始化const变量，如果const int a = b,那么也是会给a分配内存。</li>\n</ul>\n<pre><code class=\"c++\">int b = 10;\nconst int constA = b;\nint* p = (int*)&amp;constA;\n*p = 300;\ncout &lt;&lt; &quot;constA:&quot; &lt;&lt; constA &lt;&lt; endl;\ncout &lt;&lt; &quot;*p:&quot; &lt;&lt; *p &lt;&lt; endl;\n\n// constA:300\n// *p:300 </code></pre>\n<ul>\n<li>对于自定数据类型，比如类对象，那么也会分配内存。</li>\n</ul>\n<pre><code class=\"c++\">const Person person; //未初始化age\n//person.age = 50; //不可修改\nPerson* pPerson = (Person*)&amp;person;\n//指针间接修改\npPerson-&gt;age = 100;\ncout &lt;&lt; &quot;pPerson-&gt;age:&quot; &lt;&lt; pPerson-&gt;age &lt;&lt; endl;\npPerson-&gt;age = 200;\ncout &lt;&lt; &quot;pPerson-&gt;age:&quot; &lt;&lt; pPerson-&gt;age &lt;&lt; endl;\n\n// pPerson-&gt;age:100\n// pPerson-&gt;age:200\n//为person分配了内存，所以我们可以通过指针的间接赋值修改person对象。</code></pre>\n<p>3、链接方式</p>\n<p>c中const默认为外部连接，c++中const默认为内部连接.当c语言两个文件中都有const int a的时候，编译器会报重定义的错误。而在c++中，则不会，因为c++中的const默认是内部连接的。如果想让c++中的const具有外部连接，必须显示声明为: extern const int a = 10;</p>\n<h2 id=\"6、const与-define的区别\"><a href=\"#6、const与-define的区别\" class=\"headerlink\" title=\"6、const与#define的区别\"></a>6、const与#define的区别</h2><ul>\n<li><p>const有数据类型，可进行编译器类型安全检查。#define无类型，不可以进行类型检查</p>\n</li>\n<li><p>const有作用域，而#define不重视作用域，默认定义处到文件结尾。如果想定义在指定作用域下有效的常量，那么#define就不能用。</p>\n</li>\n</ul>\n<h2 id=\"7、引用\"><a href=\"#7、引用\" class=\"headerlink\" title=\"7、引用\"></a>7、引用</h2><ol>\n<li><p>&amp;在此不是求地址运算，而是起标识作用。</p>\n</li>\n<li><p>类型标识符是指目标变量的类型</p>\n</li>\n<li><p>必须在声明引用变量时进行初始化。</p>\n</li>\n<li><p>引用初始化之后不能改变。</p>\n</li>\n<li><p>不能有NULL引用。必须确保引用是和一块合法的存储单元关联。</p>\n</li>\n<li><p><strong>可以建立对数组的引用。</strong></p>\n</li>\n<li><p>函数不能返回局部变量的引用</p>\n</li>\n<li><p>函数当左值，必须返回引用</p>\n</li>\n</ol>\n<p><strong>引用的本质</strong></p>\n<p>引用的本质是在c++内部实现一个指针常量</p>\n<p><code>Type&amp; ref = val;  // Type* const ref = val;</code></p>\n<p>c++编译器在编译过程中使用常指针作为引用的内部实现，因此引用所占用的空间大小与指针相同，只是这个过程是编译器内部实现，用户不可见。</p>\n<pre><code class=\"c++\">//发现是引用，转换为 int* const ref = &amp;a;\nvoid testFunc(int&amp; ref){\n    ref = 100; // ref是引用，转换为*ref = 100\n}\nint main(){\n    int a = 10;\n    int&amp; aRef = a; //自动转换为 int* const aRef = &amp;a;这也能说明引用为什么必须初始化\n    aRef = 20; //内部发现aRef是引用，自动帮我们转换为: *aRef = 20;\n    cout &lt;&lt; &quot;a:&quot; &lt;&lt; a &lt;&lt; endl;\n    cout &lt;&lt; &quot;aRef:&quot; &lt;&lt; aRef &lt;&lt; endl;\n    testFunc(a);\n    return EXIT_SUCCESS;\n}\n</code></pre>\n<h2 id=\"8、面向对象的三大特性\"><a href=\"#8、面向对象的三大特性\" class=\"headerlink\" title=\"8、面向对象的三大特性\"></a>8、面向对象的三大特性</h2><p><strong>封装</strong></p>\n<ol>\n<li>把变量（属性）和函数（操作）合成一个整体，封装在一个类中</li>\n<li>对变量和函数进行访问控制</li>\n</ol>\n<p><strong>继承</strong></p>\n<pre><code>c++最重要的特征是代码重用，通过继承机制可以利用已有的数据类型来定义新的数据类型，新的类不仅拥有旧类的成员，还拥有新定义的成员。\n\n派生类继承基类，派生类拥有基类中全部成员变量和成员方法（除了构造和析构之外的成员方法），但是在派生类中，继承的成员并不一定能直接访问，不同的继承方式会导致不同的访问权限。\n\n任何时候重新定义基类中的一个重载函数，在新类中所有的其他版本将被自动隐藏.\n\noperator=也不能被继承，因为它完成类似构造函数的行为。</code></pre><p><strong>多态</strong></p>\n<pre><code>c++支持编译时多态(静态多态)和运行时多态(动态多态)，运算符重载和函数重载就是编译时多态，而派生类和虚函数实现运行时多态。\n\n静态多态和动态多态的区别就是函数地址是早绑定(静态联编)还是晚绑定(动态联编)。如果函数的调用，在编译阶段就可以确定函数的调用地址，并产生代码，就是静态多态(编译时多态)，就是说地址是早绑定的。而如果函数的调用地址不能编译不能在编译期间确定，而需要在运行时才能决定，这这就属于晚绑定(动态多态,运行时多态)。\n\n多态性改善了代码的可读性和组织性，同时也使创建的程序具有可扩展性。</code></pre><h2 id=\"9、C-编译器优化技术：RVO-NRVO和复制省略\"><a href=\"#9、C-编译器优化技术：RVO-NRVO和复制省略\" class=\"headerlink\" title=\"9、C++编译器优化技术：RVO/NRVO和复制省略\"></a>9、C++编译器优化技术：RVO/NRVO和复制省略</h2><p>现代编译器缺省会使用RVO（return value optimization，返回值优化）、NRVO（named return value optimization、命名返回值优化）和复制省略（Copy elision）技术，来减少拷贝次数来提升代码的运行效率</p>\n<p>注1：vc6、vs没有提供编译选项来关闭该优化，无论是debug还是release都会进行RVO和复制省略优化</p>\n<p>注2：vc6、vs2005以下及vs2005+ Debug上不支持NRVO优化，vs2005+ Release支持NRVO优化</p>\n<p>注3：g++支持这三种优化，并且可通过编译选项：-fno-elide-constructors来关闭优化</p>\n<p><strong>RVO</strong></p>\n<pre><code class=\"c++\">#include &lt;stdio.h&gt;\nclass A\n{\npublic:\n    A()\n    {\n        printf(&quot;%p construct\\n&quot;, this);\n    }\n    A(const A&amp; cp)\n    {\n        printf(&quot;%p copy construct\\n&quot;, this);\n    }\n    ~A() \n    {\n        printf(&quot;%p destruct\\n&quot;, this);\n    }\n};\n\nA GetA()\n{\n    return A();\n}\n\nint main()\n{\n    {\n        A a = GetA();\n    }\n\n    return 0;\n}</code></pre>\n<p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下：</p>\n<pre><code>0x7ffc46947d4f construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量temp\n0x7ffc46947d7f copy construct // 函数GetA return语句处，把临时变量temp做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来\n0x7ffc46947d4f destruct // 函数GetA执行完return语句后，临时变量temp生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e copy construct // 函数GetA调用结束，返回上层main函数后，把返回值变量ret做为参数传入并调用拷贝构造函数A(const A&amp; cp)将变量A a构造出来\n0x7ffc46947d7f destruct // A a = GetA()语句结束后，返回值ret生命周期结束，调用其析构函数~A()\n0x7ffc46947d7e destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：临时变量temp、返回值ret均为匿名变量</p>\n<p><strong>NRVO</strong></p>\n<p>g++编译器、vs2005+ Release（开启/O2及以上优化开关）</p>\n<p>修改上述代码，将GetA的实现修改成：</p>\n<pre><code class=\"c++\">A GetA()\n{\n    A o;\n    return o;\n}</code></pre>\n<p>在g++、vs2005+ Release中，上述代码也仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffe9d1edd0f construct\n0x7ffe9d1edd0f destruct</code></pre><p>g++加上-fno-elide-constructors选项关闭优化后，和上述结果一样</p>\n<pre><code>0x7ffc46947d4f construct\n0x7ffc46947d7f copy construct\n0x7ffc46947d4f destruct\n0x7ffc46947d7e copy construct\n0x7ffc46947d7f destruct\n0x7ffc46947d7e destruct</code></pre><p>但在vc6、vs2005以下、vs2005+ Debug中，没有进行NRVO优化，输出结果为：</p>\n<pre><code>18fec4 construct  // 在函数GetA中，调用无参构造函数A()构造出一个临时变量o\n18ff44 copy construct  // 函数GetA return语句处，把临时变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将返回值ret构造出来\n18fec4 destruct  // 函数GetA执行完return语句后，临时变量o生命周期结束，调用其析构函数~A()\n18ff44 destruct // A a要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p>注：<strong>与g++、vs2005+ Release相比，vc6、vs2005以下、vs2005+ Debug只优化掉了返回值到变量a的拷贝，命名局部变量o没有被优化掉，所以最后一共有2次构造和析构的调用</strong></p>\n<p><strong>复制省略</strong></p>\n<p>典型情况是：调用构造函数进行值类型传参</p>\n<pre><code>void Func(A a) \n{\n}\n\nint main()\n{\n    {\n        Func(A());\n    }\n\n    return 0;\n}</code></pre><p>在g++和vc6、vs中，上述代码仅仅只会调用一次构造函数和析构函数 ，输出结果如下：</p>\n<pre><code>0x7ffeb5148d0f construct\n0x7ffeb5148d0f destruct</code></pre><p>在g++中，加上-fno-elide-constructors选项关闭优化后，输出结果如下： </p>\n<pre><code>0x7ffc53c141ef construct   // 在main函数中，调用无参构造函数构造实参变量o\n0x7ffc53c141ee copy construct // 调用Func函数后，将实参变量o做为参数传入并调用拷贝构造函数A(const A&amp; cp)将形参变量a构造出来\n0x7ffc53c141ee destruct // 函数Func执行完后，形参变量a生命周期结束，调用其析构函数~A()\n0x7ffc53c141ef destruct // 返回main函数后，实参变量o要离开作用域，生命周期结束，调用其析构函数~A()</code></pre><p><strong>优化失效的情况</strong></p>\n<ol>\n<li>根据不同的条件分支，返回不同变量</li>\n<li>返回参数变量</li>\n<li>返回全局变量</li>\n<li>返回复合函数类型中的成员变量</li>\n<li>返回值赋值给已构造好的变量（此时会调用operator==赋值运算符）</li>\n</ol>\n<p><a href=\"https://www.cnblogs.com/kekec/p/11303391.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/kekec/p/11303391.html</a></p>\n<h2 id=\"10、explicit关键字\"><a href=\"#10、explicit关键字\" class=\"headerlink\" title=\"10、explicit关键字\"></a>10、explicit关键字</h2><ul>\n<li><p>explicit用于修饰构造函数,防止隐式转化。</p>\n</li>\n<li><p>是针对单参数的构造函数(或者除了第一个参数外其余参数都有默认值的多参构造)而言。</p>\n</li>\n</ul>\n<pre><code class=\"c++\">class MyString{\npublic:\n    explicit MyString(int n){\n        cout &lt;&lt; &quot;MyString(int n)!&quot; &lt;&lt; endl;\n    }\n    MyString(const char* str){\n        cout &lt;&lt; &quot;MyString(const char* str)&quot; &lt;&lt; endl;\n    }\n};\n\nint main(){\n\n    //给字符串赋值？还是初始化？\n    //MyString str1 = 1; \n    MyString str2(10);\n\n    //寓意非常明确，给字符串赋值\n    MyString str3 = &quot;abcd&quot;;\n    MyString str4(&quot;abcd&quot;);\n\n    return EXIT_SUCCESS;\n}\n</code></pre>\n<h2 id=\"11、new-delete-malloc-free\"><a href=\"#11、new-delete-malloc-free\" class=\"headerlink\" title=\"11、new/delete/malloc/free\"></a>11、new/delete/malloc/free</h2><p><strong>11.1、new</strong></p>\n<ol>\n<li>内存申请成功后，会返回一个指向该内存的地址。</li>\n<li>若内存申请失败，则抛出异常，</li>\n<li>申请成功后，如果是程序员定义的类型，会执行相应的构造函数</li>\n</ol>\n<p><strong>11.2、delete</strong></p>\n<ol>\n<li>如果指针的值是0 ，delete不会执行任何操作，有检测机制</li>\n<li>delete只是释放内存，不会修改指针，指针仍然会指向原来的地址</li>\n<li>重复delete，有可能出现异常</li>\n<li>如果是自定义类型，会执行析构函数</li>\n</ol>\n<p><strong>11.3、malloc</strong></p>\n<ol>\n<li>malloc 申请成功之后，返回的是void类型的指针。需要将void*指针转换成我们需要的类型。1.</li>\n<li>malloc 要求制定申请的内存大小 ， 而new由编译器自行计算。</li>\n<li>申请失败，返回的是NULL ， 比如： 内存不足。</li>\n<li>不会执行自定义类型的构造函数</li>\n</ol>\n<p><strong>11.4、free</strong></p>\n<ol>\n<li>如果是空指针，多次释放没有问题，非空指针，重复释放有问题</li>\n<li>不会执行对应的析构</li>\n<li>delete的底层执行的是free</li>\n</ol>\n<h2 id=\"12、const\"><a href=\"#12、const\" class=\"headerlink\" title=\"12、const\"></a>12、const</h2><ol>\n<li>const修饰静态成员变量时可以在类内部初始化</li>\n<li>const 修饰成员函数时，修饰的是this指针，所以成员函数内不可以修改任何普通成员变量，当成员变量类型符前用mutable修饰时例外</li>\n<li>常对象(cons修饰的对象)只能调用const修饰的成员函数</li>\n<li>常对象可以访问成员属性，但是不能修改</li>\n<li><strong>const关键字会让左值引用变得不同，它可以接收右值。</strong></li>\n</ol>\n<h2 id=\"13、虚继承\"><a href=\"#13、虚继承\" class=\"headerlink\" title=\"13、虚继承\"></a>13、虚继承</h2><p>虚继承是解决C++多重继承问题的一种手段，从不同途径继承来的同一基类，会在子类中存在多份拷贝。这将存在两个问题：其一，浪费存储空间；第二，存在二义性问题，多重继承可能存在一个基类的多份拷贝，这就出现了二义性。</p>\n<pre><code class=\"c++\">class BigBase{\npublic:\n    BigBase(){ mParam = 0; }\n    void func(){ cout &lt;&lt; &quot;BigBase::func&quot; &lt;&lt; endl; }\npublic:\n    int mParam;\n};\n\nclass Base1 : public BigBase{};\nclass Base2 : public BigBase{};\nclass Derived : public Base1, public Base2{};\n\nint main(){\n\n    Derived derived;\n    //1. 对“func”的访问不明确\n    //derived.func();\n    //cout &lt;&lt; derived.mParam &lt;&lt; endl;\n    cout &lt;&lt; &quot;derived.Base1::mParam:&quot; &lt;&lt; derived.Base1::mParam &lt;&lt; endl;\n    cout &lt;&lt; &quot;derived.Base2::mParam:&quot; &lt;&lt; derived.Base2::mParam &lt;&lt; endl;\n\n    //2. 重复继承\n    cout &lt;&lt; &quot;Derived size:&quot; &lt;&lt; sizeof(Derived) &lt;&lt; endl; //8\n\n    return EXIT_SUCCESS;\n}</code></pre>\n<p>虚继承可以解决多种继承前面提到的两个问题：</p>\n<p>虚继承底层实现原理与编译器相关，一般通过虚基类指针和虚基类表实现，每个虚继承的子类都有一个虚基类指针（占用一个指针的存储空间，64位8字节/windows 4字节）和虚基类表（不占用类对象的存储空间）（需要强调的是，虚基类指针依旧会在子类里面存在拷贝，只是仅仅最多存在一份而已，并不是不在子类里面了）；当虚继承的子类被当做父类继承时，<strong>虚基类指针也会被继承。</strong></p>\n<p>实际上，<code>vbptr</code>指的是虚基类表指针（virtual base table pointer），该指针指向了一个虚基类表（virtual table），虚表中记录了虚基类与本类的偏移地址；通过偏移地址，这样就找到了虚基类成员，而虚继承也不用像普通多继承那样维持着公共基类（虚基类）的两份同样的拷贝，节省了存储空间。</p>\n<p><img src=\"05.png\" alt></p>\n<pre><code class=\"c++\">#include &lt;iostream&gt;\n\nusing namespace std;\n\n\nclass A  //大小为4\n{\npublic:\n    int a;\n};\n\nclass B:virtual public A{ // vbptr 8, int b 4, int a 4 = 12\npublic:\n    int b;\n};\n\nclass C:virtual public A{// vbptr 8, int c 4, int a 4 = 12\npublic:\n    int c;\n};\n\nclass D:public B, public C{\n    // int a, b, c, d=16\n    // class B  vbptr 4\n    // class C  vbptr 4  = 24\npublic:\n    int d;\n};\n\nint main()\n{\n    A a;\n    B b;\n    C c;\n    D d;\n    cout &lt;&lt; sizeof(a) &lt;&lt; endl; // 4 \n    cout &lt;&lt; sizeof(b) &lt;&lt; endl; // 16\n    cout &lt;&lt; sizeof(c) &lt;&lt; endl; // 16\n    cout &lt;&lt; sizeof(d) &lt;&lt; endl; // 24\n    return 0;\n}</code></pre>\n<p>链接:<a href=\"https://blog.csdn.net/bxw1992/article/details/77726390\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/bxw1992/article/details/77726390</a></p>\n<h2 id=\"14、虚函数\"><a href=\"#14、虚函数\" class=\"headerlink\" title=\"14、虚函数\"></a>14、虚函数</h2><p><strong>问题</strong></p>\n<pre><code>父类引用或指针可以指向子类对象，通过父类指针或引用来操作子类对象。但是由于编译阶段编译器根据对象的指针或者引用选择函数调用，所以会调用父类的函数。\n\n解决问题的方法是迟绑定(动态绑定)，在运行时根据对象的实际类型决定。</code></pre><p><strong>解决</strong></p>\n<pre><code>C++动态多态性是通过虚函数来实现的，虚函数允许子类（派生类）重新定义父类（基类）成员函数，而子类（派生类）重新定义父类（基类）虚函数的做法称为覆盖(override)，或者称为重写。\n\n对于特定的函数进行动态绑定，c++要求在基类中声明这个函数的时候使用virtual关键字,动态绑定也就对virtual函数起作用.</code></pre><ul>\n<li><p>为创建一个需要动态绑定的虚成员函数，可以简单在这个函数声明前面加上virtual关键字，定义时候不需要.</p>\n</li>\n<li><p>如果一个函数在基类中被声明为virtual，那么在所有派生类中它都是virtual的.</p>\n</li>\n<li><p>在派生类中virtual函数的重定义称为重写(override).</p>\n</li>\n<li><p>Virtual关键字只能修饰成员函数.</p>\n</li>\n<li><p>构造函数不能为虚函数</p>\n</li>\n</ul>\n<p><strong>虚函数原理</strong></p>\n<p>首先，我们看看编译器如何处理虚函数。当编译器发现我们的类中有虚函数的时候，编译器会创建一张虚函数表（virtual function table ），表中存储着类对象的虚函数地址，并且给类增加一个指针，这个指针就是<code>vpointer</code>(缩写<code>vptr</code>)，这个指针是指向虚函数表。</p>\n<p>父类对象包含的指针，指向父类的虚函数表地址，子类对象包含的指针，指向子类的虚函数表地址。</p>\n<p>如果子类重新定义了父类的函数，那么函数表中存放的是新的地址，如果子类没有重新定义，那么表中存放的是父类的函数地址。如果子类有自己的虚函数，则只需要添加到表中即可。</p>\n<pre><code class=\"c++\">class A{\npublic:\n    virtual void func1(){}\n    virtual void func2(){}\n};\n\n//B类为空，那么大小应该是1字节，实际情况是这样吗？\nclass B : public A{};\n\nvoid test(){\n    cout &lt;&lt; &quot;A size:&quot; &lt;&lt; sizeof(A) &lt;&lt; endl; // win指针字节为4, linux 64 字节为8\n    cout &lt;&lt; &quot;B size:&quot; &lt;&lt; sizeof(B) &lt;&lt; endl; // 4\n}</code></pre>\n<p><strong>多态的成立条件：</strong></p>\n<ul>\n<li><p>有继承</p>\n</li>\n<li><p>子类重写父类虚函数函数</p>\n<pre><code>a) 返回值，函数名字，函数参数，必须和父类完全一致(析构函数除外) \n\nb) 子类中virtual关键字可写可不写，建议写</code></pre></li>\n<li><p>类型兼容，父类指针，父类引用指向子类对象</p>\n</li>\n</ul>\n<p><strong>抽象类和纯虚函数</strong></p>\n<p>当基类中有至少一个纯虚函数则为抽象类</p>\n<ul>\n<li><p>纯虚函数使用关键字virtual，并在其后面加上=0。如果试图去实例化一个抽象类，编译器则会阻止这种操作。</p>\n</li>\n<li><p>当继承一个抽象类的时候，必须实现所有的纯虚函数，否则由抽象类派生的类也是一个抽象类。</p>\n</li>\n<li><p>Virtual void fun() = 0;告诉编译器在vtable中为函数保留一个位置，但在这个特定位置不放地址。</p>\n</li>\n</ul>\n<p><strong>接口类</strong></p>\n<p>接口类中只有函数原型定义，没有任何数据定义。多重继承接口不会带来二义性和复杂性问题。接口类只是一个功能声明，并不是功能实现，子类需要根据功能说明定义功能实现。</p>\n<p>注意:除了析构函数外，其他声明都是纯虚函数。</p>\n<p><strong>虚析构函数</strong></p>\n<p>虚析构函数是为了解决[基类]的[指针]指向派生类对象，并用基类的指针删除派生类对象。</p>\n<p><strong>虚函数和虚继承的异同</strong></p>\n<p>在这里我们可以对比虚函数的实现原理：他们有相似之处，都利用了虚指针（均占用类的存储空间）和虚表（均不占用类的存储空间）。</p>\n<p>虚基类指针依旧存在继承类中，只占用存储空间；基类虚函数指针不存在于子类中，不占用存储空间。</p>\n<p>虚基类表存储的是虚基类相对直接继承类的偏移；而虚函数表存储的是虚函数地址。</p>\n<h2 id=\"15、函数模板的机制\"><a href=\"#15、函数模板的机制\" class=\"headerlink\" title=\"15、函数模板的机制\"></a>15、函数模板的机制</h2><p><strong>函数模板机制结论：</strong></p>\n<ul>\n<li><p>编译器并不是把函数模板处理成能够处理任何类型的函数</p>\n</li>\n<li><p>函数模板通过具体类型产生不同的函数</p>\n</li>\n<li><p>编译器会对函数模板进行两次编译，在声明的地方对模板代码本身进行编译，在调用的地方对参数替换后的代码进行编译。</p>\n</li>\n</ul>\n<p><strong>局限性</strong>:</p>\n<p>编写的模板函数很可能无法处理某些类型，另一方面，有时候通用化是有意义的，但C++语法不允许这样做。为了解决这种问题，可以提供<strong>模板的重载</strong>，为这些特定的类型提供具体化的模板。</p>\n<pre><code class=\"c++\">class Person\n{\npublic:\n    Person(string name, int age)\n    {\n        this-&gt;mName = name;\n        this-&gt;mAge = age;\n    }\n    string mName;\n    int mAge;\n};\n\n\n//普通交换函数\ntemplate &lt;class T&gt;\nvoid mySwap(T &amp;a,T &amp;b)\n{\n    T temp = a;\n    a = b;\n    b = temp;\n}\n\n\n//第三代具体化，显示具体化的原型和定意思以template&lt;&gt;开头，并通过名称来指出类型\n//具体化优先于常规模板\ntemplate&lt;&gt;void mySwap&lt;Person&gt;(Person &amp;p1, Person &amp;p2)\n{\n    string nameTemp;\n    int ageTemp;\n\n    nameTemp = p1.mName;\n    p1.mName = p2.mName;\n    p2.mName = nameTemp;\n\n    ageTemp = p1.mAge;\n    p1.mAge = p2.mAge;\n    p2.mAge = ageTemp;\n\n}\n\nvoid test()\n{\n    Person P1(&quot;Tom&quot;, 10);\n    Person P2(&quot;Jerry&quot;, 20);\n\n    cout &lt;&lt; &quot;P1 Name = &quot; &lt;&lt; P1.mName &lt;&lt; &quot; P1 Age = &quot; &lt;&lt; P1.mAge &lt;&lt; endl;\n    cout &lt;&lt; &quot;P2 Name = &quot; &lt;&lt; P2.mName &lt;&lt; &quot; P2 Age = &quot; &lt;&lt; P2.mAge &lt;&lt; endl;\n    mySwap(P1, P2);\n    cout &lt;&lt; &quot;P1 Name = &quot; &lt;&lt; P1.mName &lt;&lt; &quot; P1 Age = &quot; &lt;&lt; P1.mAge &lt;&lt; endl;\n    cout &lt;&lt; &quot;P2 Name = &quot; &lt;&lt; P2.mName &lt;&lt; &quot; P2 Age = &quot; &lt;&lt; P2.mAge &lt;&lt; endl;\n}</code></pre>\n<h2 id=\"16、C-类型转换\"><a href=\"#16、C-类型转换\" class=\"headerlink\" title=\"16、C++类型转换\"></a>16、C++类型转换</h2><p><strong>静态类型转换static_cast</strong></p>\n<ul>\n<li><p>用于<a href=\"http://baike.baidu.com/view/2405425.htm\" target=\"_blank\" rel=\"noopener\">类层次结构</a>中基类（父类）和<a href=\"http://baike.baidu.com/view/535532.htm\" target=\"_blank\" rel=\"noopener\">派生类</a>（子类）之间指针或引用的转换。</p>\n<ul>\n<li><p>进行上行转换（把派生类的指针或引用转换成基类表示）是安全的；</p>\n</li>\n<li><p>进行下行转换（把基类指针或引用转换成派生类表示）时，由于没有动态类型检查，所以是不安全的。</p>\n</li>\n</ul>\n</li>\n<li><p>用于基本数据类型之间的转换，如把int转换成char，把char转换成int。这种转换的安全性也要开发人员来保证。</p>\n</li>\n</ul>\n<p><strong>动态类型转换(dynamic_cast)</strong></p>\n<ul>\n<li><p>dynamic_cast主要用于类层次间的上行转换和下行转换；</p>\n</li>\n<li><p>在类层次间进行上行转换时，dynamic_cast和static_cast的效果是一样的；</p>\n</li>\n<li><p>在进行下行转换时，dynamic_cast具有类型检查的功能，比static_cast更安全；</p>\n</li>\n</ul>\n<p><strong>常量转换(const_cast)</strong></p>\n<p>该运算符用来修改类型的const属性。。</p>\n<ul>\n<li><p>常量指针被转化成非常量指针，并且仍然指向原来的对象；</p>\n</li>\n<li><p>常量引用被转换成非常量引用，并且仍然指向原来的对象；</p>\n</li>\n</ul>\n<p><strong>重新解释转换(reinterpret_cast)</strong></p>\n<p>这是最不安全的一种转换机制，最有可能出问题。</p>\n<p>主要用于将一种数据类型从一种类型转换为另一种类型。它可以将一个指针转换成一个整数，也可以将一个整数转换成一个指针.</p>\n<h1 id=\"三、STL\"><a href=\"#三、STL\" class=\"headerlink\" title=\"三、STL\"></a>三、STL</h1><h2 id=\"1、STL六大组件简介\"><a href=\"#1、STL六大组件简介\" class=\"headerlink\" title=\"1、STL六大组件简介\"></a>1、STL六大组件简介</h2><p>STL提供了六大组件，彼此之间可以组合套用，这六大组件分别是:容器、算法、迭代器、仿函数、适配器（配接器）、空间配置器。</p>\n<p><strong>容器：</strong>各种数据结构，如vector、list、deque、set、map等, 用来存放数据，从实现角度来看，STL容器是一种class template。</p>\n<p><strong>算法：</strong>各种常用的算法，如sort、find、copy、for_each。从实现的角度来看，STL算法是一种function tempalte.</p>\n<p><strong>迭代器：</strong>扮演了容器与算法之间的胶合剂，共有五种类型，从实现角度来看，<strong>迭代器是一种将operator* , operator-&gt; , operator++,operator–等指针相关操作予以重载的class template.</strong> 所有STL容器都附带有自己专属的迭代器，只有容器的设计者才知道如何遍历自己的元素。原生指针(native pointer)也是一种迭代器。</p>\n<p><strong>仿函数：</strong>行为类似函数，可作为算法的某种策略。从实现角度来看，仿函数是一种重载了operator()的class 或者class template</p>\n<p><strong>适配器：</strong>一种用来修饰容器或者仿函数或迭代器接口的东西。</p>\n<p><strong>空间配置器：</strong>负责空间的配置与管理。从实现角度看，配置器是一个实现了动态空间配置、空间管理、空间释放的class tempalte.</p>\n<h2 id=\"2、string容器\"><a href=\"#2、string容器\" class=\"headerlink\" title=\"2、string容器\"></a>2、string容器</h2><p>C风格字符串(以空字符结尾的字符数组)太过复杂难于掌握，不适合大程序的开发，所以C++标准库定义了一种string类，定义在头文件<string>。</string></p>\n<p>String和c风格字符串对比：</p>\n<ul>\n<li><p>char*是一个指针，String是一个类</p>\n<p>string封装了char*，管理这个字符串，是一个char*型的容器。</p>\n</li>\n<li><p>String封装了很多实用的成员方法</p>\n<p>查找find，拷贝copy，删除delete 替换replace，插入insert</p>\n</li>\n<li><p>不用考虑内存释放和越界</p>\n<p> string管理char*所分配的内存。每一次string的复制，取值都由string类负责维护，不用担心复制越界和取值越界等。</p>\n</li>\n</ul>\n<h2 id=\"3、vector容器\"><a href=\"#3、vector容器\" class=\"headerlink\" title=\"3、vector容器\"></a>3、vector容器</h2><p>vector的数据安排以及操作方式，与array非常相似，两者的唯一差别在于空间的运用的灵活性。Array是静态空间，一旦配置了就不能改变，要换大一点或者小一点的空间，可以，一切琐碎得由自己来，首先配置一块新的空间，然后将旧空间的数据搬往新空间，再释放原来的空间。Vector是动态空间，随着元素的加入，它的内部机制会自动扩充空间以容纳新元素。因此vector的运用对于内存的合理利用与运用的灵活性有很大的帮助，我们再也不必害怕空间不足而一开始就要求一个大块头的array了。</p>\n<p>所谓动态增加大小，并不是在原空间之后续接新空间(因为无法保证原空间之后尚有可配置的空间)，而是一块更大的内存空间，然后将原数据拷贝新空间，并释放原空间。因此，对vector的任何操作，一旦引起空间的重新配置，指向原vector的所有迭代器就都失效了。这是程序员容易犯的一个错误，务必小心。</p>\n<p>为了降低空间配置时的速度成本，vector实际配置的大小可能比客户端需求大一些，以备将来可能的扩充，这边是<strong>容量</strong>的概念。换句话说，<strong>一个vector的容量永远大于或等于其大小，一旦容量等于大小，便是满载，下次再有新增元素，整个vector容器就得另觅居所。</strong></p>\n<h2 id=\"4、deque容器\"><a href=\"#4、deque容器\" class=\"headerlink\" title=\"4、deque容器\"></a>4、deque容器</h2><p>Vector容器是单向开口的连续内存空间，deque则是一种双向开口的连续线性空间。所谓的双向开口，意思是可以在头尾两端分别做元素的插入和删除操作，当然，vector容器也可以在头尾两端插入元素，但是在其头部操作效率奇差，无法被接受。</p>\n<p>Deque容器和vector容器最大的差异，一在于deque允许使用常数项时间对头端进行元素的插入和删除操作。二在于deque没有容量的概念，因为它是动态的以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，换句话说，像vector那样，”旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque身上是不会发生的。也因此，deque没有必须要提供所谓的空间保留(reserve)功能.</p>\n<p>虽然deque容器也提供了Random Access Iterator,但是它的迭代器并不是普通的指针，其复杂度和vector不是一个量级，这当然影响各个运算的层面。因此，除非有必要，我们应该尽可能的使用vector，而不是deque。对deque进行的排序操作，为了最高效率，可将deque先完整的复制到一个vector中，对vector容器进行排序，再复制回deque.</p>\n<p>既然deque是分段连续内存空间，那么就必须有中央控制，维持整体连续的假象，数据结构的设计及迭代器的前进后退操作颇为繁琐。Deque代码的实现远比vector或list都多得多。</p>\n<p>Deque采取一块所谓的map(注意，不是STL的map容器)作为主控，这里所谓的map是一小块连续的内存空间，其中每一个元素(此处成为一个结点)都是一个指针，指向另一段连续性内存空间，称作缓冲区。缓冲区才是deque的存储空间的主体。</p>\n<p><img src=\"06.png\" alt></p>\n<h2 id=\"5、stack容器\"><a href=\"#5、stack容器\" class=\"headerlink\" title=\"5、stack容器\"></a>5、stack容器</h2><p>stack是一种先进后出(First In Last Out,FILO)的数据结构，它只有一个出口，形式如图所示。stack容器允许新增元素，移除元素，取得栈顶元素，但是除了最顶端外，没有任何其他方法可以存取stack的其他元素。换言之，stack不允许有遍历行为。</p>\n<p>有元素推入栈的操作称为:push,将元素推出stack的操作称为pop.</p>\n<p>Stack所有元素的进出都必须符合”先进后出”的条件，只有stack顶端的元素，才有机会被外界取用。Stack不提供遍历功能，也不提供迭代器。</p>\n<h2 id=\"6、queue容器\"><a href=\"#6、queue容器\" class=\"headerlink\" title=\"6、queue容器\"></a>6、queue容器</h2><p>Queue是一种先进先出(First In First Out,FIFO)的数据结构，它有两个出口，queue容器允许从一端新增元素，从另一端移除元素。</p>\n<p>Queue所有元素的进出都必须符合”先进先出”的条件，只有queue的顶端元素，才有机会被外界取用。Queue不提供遍历功能，也不提供迭代器。</p>\n<h2 id=\"7、list容器\"><a href=\"#7、list容器\" class=\"headerlink\" title=\"7、list容器\"></a>7、list容器</h2><p>链表是一种物理<a href=\"http://baike.baidu.com/view/1223079.htm\" target=\"_blank\" rel=\"noopener\">存储单元</a>上非连续、非顺序的<a href=\"http://baike.baidu.com/view/2820182.htm\" target=\"_blank\" rel=\"noopener\">存储结构</a>，<a href=\"http://baike.baidu.com/view/38785.htm\" target=\"_blank\" rel=\"noopener\">数据元素</a>的逻辑顺序是通过链表中的<a href=\"http://baike.baidu.com/view/159417.htm\" target=\"_blank\" rel=\"noopener\">指针</a>链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储<a href=\"http://baike.baidu.com/view/38785.htm\" target=\"_blank\" rel=\"noopener\">数据元素</a>的数据域，另一个是存储下一个结点地址的<a href=\"http://baike.baidu.com/view/159417.htm\" target=\"_blank\" rel=\"noopener\">指针</a>域。</p>\n<p>相较于vector的连续线性空间，list就显得负责许多，它的好处是每次插入或者删除一个元素，就是配置或者释放一个元素的空间。因此，list对于空间的运用有绝对的精准，一点也不浪费。而且，对于任何位置的元素插入或元素的移除，list永远是常数时间。</p>\n<p>List和vector是两个最常被使用的容器。</p>\n<p>List容器是一个双向链表。</p>\n<ul>\n<li><p>采用动态存储分配，不会造成内存浪费和溢出</p>\n</li>\n<li><p>链表执行插入和删除操作十分方便，修改指针即可，不需要移动大量元素</p>\n</li>\n<li><p>链表灵活，但是空间和时间额外耗费较大</p>\n</li>\n</ul>\n<p>List有一个重要的性质，插入操作和删除操作都不会造成原有list迭代器的失效。这在vector是不成立的，因为vector的插入操作可能造成记忆体重新配置，导致原有的迭代器全部失效，甚至List元素的删除，也只有被删除的那个元素的迭代器失效，其他迭代器不受任何影响。</p>\n<h2 id=\"8、set-multiset容器\"><a href=\"#8、set-multiset容器\" class=\"headerlink\" title=\"8、set/multiset容器\"></a>8、set/multiset容器</h2><p>Set的特性是。所有元素都会根据元素的键值自动被排序。Set的元素不像map那样可以同时拥有实值和键值，set的元素即是键值又是实值。Set不允许两个元素有相同的键值。</p>\n<p>我们可以通过set的迭代器改变set元素的值吗？不行，因为set元素值就是其键值，关系到set元素的排序规则。如果任意改变set元素值，会严重破坏set组织。换句话说，set的iterator是一种const_iterator.</p>\n<p>set拥有和list某些相同的性质，当对容器中的元素进行插入操作或者删除操作的时候，操作之前所有的迭代器，在操作完成之后依然有效，被删除的那个元素的迭代器必然是一个例外。</p>\n<h2 id=\"9、map-multimap容器\"><a href=\"#9、map-multimap容器\" class=\"headerlink\" title=\"9、map/multimap容器\"></a>9、map/multimap容器</h2><p>Map的特性是，所有元素都会根据元素的键值自动排序。Map所有的元素都是pair,同时拥有实值和键值，pair的第一元素被视为键值，第二元素被视为实值，map不允许两个元素有相同的键值。</p>\n<p>我们可以通过map的迭代器改变map的键值吗？答案是不行，因为map的键值关系到map元素的排列规则，任意改变map键值将会严重破坏map组织。如果想要修改元素的实值，那么是可以的。</p>\n<p>Map和list拥有相同的某些性质，当对它的容器元素进行新增操作或者删除操作时，操作之前的所有迭代器，在操作完成之后依然有效，当然被删除的那个元素的迭代器必然是个例外。</p>\n<p>Multimap和map的操作类似，唯一区别multimap键值可重复。</p>\n<p>Map和multimap都是以红黑树为底层实现机制。</p>\n<h2 id=\"10、STL容器使用时机\"><a href=\"#10、STL容器使用时机\" class=\"headerlink\" title=\"10、STL容器使用时机\"></a>10、STL容器使用时机</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>vector</th>\n<th>deque</th>\n<th>list</th>\n<th>set</th>\n<th>multiset</th>\n<th>map</th>\n<th>multimap</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>典型内存结构</td>\n<td>单端数组</td>\n<td>双端数组</td>\n<td>双向链表</td>\n<td>二叉树</td>\n<td>二叉树</td>\n<td>二叉树</td>\n<td>二叉树</td>\n</tr>\n<tr>\n<td>可随机存取</td>\n<td>是</td>\n<td>是</td>\n<td>否</td>\n<td>否</td>\n<td>否</td>\n<td>对key而言：不是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>元素搜寻速度</td>\n<td>慢</td>\n<td>慢</td>\n<td>非常慢</td>\n<td>快</td>\n<td>快</td>\n<td>对key而言：快</td>\n<td>对key而言：快</td>\n</tr>\n<tr>\n<td>元素安插移除</td>\n<td>尾端</td>\n<td>头尾两端</td>\n<td>任何位置</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>vector的使用场景：比如软件历史操作记录的存储，我们经常要查看历史记录，比如上一次的记录，上上次的记录，但却不会去删除记录，因为记录是事实的描述。</p>\n</li>\n<li><p>deque的使用场景：比如排队购票系统，对排队者的存储可以采用deque，支持头端的快速移除，尾端的快速添加。如果采用vector，则头端移除时，会移动大量的数据，速度慢。</p>\n<p>​     vector与deque的比较：</p>\n<p>​      一：vector.at()比deque.at()效率高，比如vector.at(0)是固定的，deque的开始位置却是不固定的。</p>\n<p>​    二：如果有大量释放操作的话，vector花的时间更少，这跟二者的内部实现有关。</p>\n<p>​    三：deque支持头部的快速插入与快速移除，这是deque的优点。</p>\n</li>\n<li><p>list的使用场景：比如公交车乘客的存储，随时可能有乘客下车，支持频繁的不确实位置元素的移除插入。</p>\n</li>\n<li><p>set的使用场景：比如对手机游戏的个人得分记录的存储，存储要求从高分到低分的顺序排列。 </p>\n</li>\n<li><p>map的使用场景：比如按ID号存储十万个用户，想要快速要通过ID查找对应的用户。二叉树的查找效率，这时就体现出来了。如果是vector容器，最坏的情况下可能要遍历完整个容器才能找到该用户。</p>\n</li>\n</ul>\n"}],"PostAsset":[{"_id":"source/_posts/Hexo-Github博客搭建/2.png","slug":"2.png","post":"ck5454trx0007zsv5yp03gkt9","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v2.png","slug":"v2.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v5.png","slug":"v5.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v6.png","slug":"v6.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v7.png","slug":"v7.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v8.png","slug":"v8.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v9.png","slug":"v9.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之超参数/01.png","slug":"01.png","post":"ck5454tsv000zzsv574n6a04y","modified":1,"renderable":0},{"_id":"source/_posts/神经网络简介/n5.png","slug":"n5.png","post":"ck5454tsz0012zsv5c1yzmnay","modified":1,"renderable":0},{"_id":"source/_posts/Hexo-Github博客搭建/1.png","slug":"1.png","post":"ck5454trx0007zsv5yp03gkt9","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/c2.png","slug":"c2.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v10.png","slug":"v10.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v12.png","slug":"v12.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之卷积神经网络/06.png","slug":"06.png","post":"ck5454tsg000mzsv5mtkyj6zd","modified":1,"renderable":0},{"_id":"source/_posts/神经网络简介/b2.png","slug":"b2.png","post":"ck5454tsz0012zsv5c1yzmnay","modified":1,"renderable":0},{"_id":"source/_posts/C和C++语法总结/02.png","slug":"02.png","post":"ck5454tuk0034zsv5ijho38pk","modified":1,"renderable":0},{"_id":"source/_posts/机器学习概述/5.jpg","slug":"5.jpg","post":"ck5454tsb000hzsv5hgxv5hke","modified":1,"renderable":0},{"_id":"source/_posts/神经网络简介/n7.png","slug":"n7.png","post":"ck5454tsz0012zsv5c1yzmnay","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之过拟合/01.png","slug":"01.png","post":"ck5454tss000xzsv58rjg2wct","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之过拟合/02.png","post":"ck5454tss000xzsv58rjg2wct","slug":"02.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之超参数/02.png","post":"ck5454tsv000zzsv574n6a04y","slug":"02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之评估指标/01.jpg","post":"ck5454tse000lzsv5bvsgyyi2","slug":"01.jpg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之评估指标/02.jpg","post":"ck5454tse000lzsv5bvsgyyi2","slug":"02.jpg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之评估指标/02.png","post":"ck5454tse000lzsv5bvsgyyi2","slug":"02.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习之评估指标/混淆矩阵.png","post":"ck5454tse000lzsv5bvsgyyi2","slug":"混淆矩阵.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之优化算法/adagrad.png","post":"ck5454tso000szsv5npcie8vs","slug":"adagrad.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之优化算法/adam.png","post":"ck5454tso000szsv5npcie8vs","slug":"adam.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之优化算法/mom.png","post":"ck5454tso000szsv5npcie8vs","slug":"mom.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之优化算法/sgd.png","post":"ck5454tso000szsv5npcie8vs","slug":"sgd.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之优化算法/v1.webp","slug":"v1.webp","post":"ck5454tso000szsv5npcie8vs","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之优化算法/v2.webp","slug":"v2.webp","post":"ck5454tso000szsv5npcie8vs","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/binarytree.png","slug":"binarytree.png","post":"ck5454ts8000fzsv5130vsj44","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/childs01.png","slug":"childs01.png","post":"ck5454ts8000fzsv5130vsj44","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/childs02.png","slug":"childs02.png","post":"ck5454ts8000fzsv5130vsj44","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/childs03.png","slug":"childs03.png","post":"ck5454ts8000fzsv5130vsj44","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/parchild.png","slug":"parchild.png","post":"ck5454ts8000fzsv5130vsj44","modified":1,"renderable":0},{"_id":"source/_posts/数据结构与算法之树/parents.png","post":"ck5454ts8000fzsv5130vsj44","slug":"parents.png","modified":1,"renderable":1},{"_id":"source/_posts/数据结构与算法之树/完全二叉树.png","post":"ck5454ts8000fzsv5130vsj44","slug":"完全二叉树.png","modified":1,"renderable":1},{"_id":"source/_posts/数据结构与算法之树/满二叉树.png","post":"ck5454ts8000fzsv5130vsj44","slug":"满二叉树.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/01.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"01.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/02.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"02.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/03.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"03.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/04.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"04.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/05.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"05.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/07.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"07.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/08.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"08.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之卷积神经网络/09.png","post":"ck5454tsg000mzsv5mtkyj6zd","slug":"09.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c1.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c1.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/1.png","post":"ck5454tsb000hzsv5hgxv5hke","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/11.png","slug":"11.png","post":"ck5454tsb000hzsv5hgxv5hke","modified":1,"renderable":0},{"_id":"source/_posts/机器学习概述/12.png","slug":"12.png","post":"ck5454tsb000hzsv5hgxv5hke","modified":1,"renderable":0},{"_id":"source/_posts/机器学习概述/13.png","slug":"13.png","post":"ck5454tsb000hzsv5hgxv5hke","modified":1,"renderable":0},{"_id":"source/_posts/机器学习概述/14.png","slug":"14.png","post":"ck5454tsb000hzsv5hgxv5hke","modified":1,"renderable":0},{"_id":"source/_posts/机器学习概述/2.jpeg","post":"ck5454tsb000hzsv5hgxv5hke","slug":"2.jpeg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/3.jpg","post":"ck5454tsb000hzsv5hgxv5hke","slug":"3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/4.jpg","post":"ck5454tsb000hzsv5hgxv5hke","slug":"4.jpg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/6.jpg","post":"ck5454tsb000hzsv5hgxv5hke","slug":"6.jpg","modified":1,"renderable":1},{"_id":"source/_posts/机器学习概述/7.png","post":"ck5454tsb000hzsv5hgxv5hke","slug":"7.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b5.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b5.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b1.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b1.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b3.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b3.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b4.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b4.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b6.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b6.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/b7.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"b7.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n1.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n1.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n2.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n2.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n3.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n3.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n4.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n4.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n6.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n6.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/n8.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"n8.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p1.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p1.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p2.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p2.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p3.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p3.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p4.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p4.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p5.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p5.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p6.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p6.png","modified":1,"renderable":1},{"_id":"source/_posts/神经网络简介/p7.png","post":"ck5454tsz0012zsv5c1yzmnay","slug":"p7.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/09.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"09.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c3.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c3.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c4.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c4.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c5.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c5.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c6.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c6.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c7.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"c7.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/c8.png","slug":"c8.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v1.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"v1.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/v11.png","slug":"v11.png","post":"ck5454tsl000qzsv5cr7lezre","modified":1,"renderable":0},{"_id":"source/_posts/深度学习之CNN模型演化/v13.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"v13.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/v3.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"v3.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/v4.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"v4.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x1.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x1.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x2.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x2.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x3.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x3.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x4.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x4.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x5.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x5.png","modified":1,"renderable":1},{"_id":"source/_posts/深度学习之CNN模型演化/x6.png","post":"ck5454tsl000qzsv5cr7lezre","slug":"x6.png","modified":1,"renderable":1},{"_id":"source/_posts/C和C++语法总结/01.png","slug":"01.png","post":"ck5454tuk0034zsv5ijho38pk","modified":1,"renderable":0},{"_id":"source/_posts/C和C++语法总结/03.png","post":"ck5454tuk0034zsv5ijho38pk","slug":"03.png","modified":1,"renderable":1},{"_id":"source/_posts/C和C++语法总结/04.png","post":"ck5454tuk0034zsv5ijho38pk","slug":"04.png","modified":1,"renderable":1},{"_id":"source/_posts/C和C++语法总结/05.png","slug":"05.png","post":"ck5454tuk0034zsv5ijho38pk","modified":1,"renderable":0},{"_id":"source/_posts/C和C++语法总结/06.png","post":"ck5454tuk0034zsv5ijho38pk","slug":"06.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ck5454tro0001zsv5lpyfdiwc","category_id":"ck5454trv0005zsv54or2iw1b","_id":"ck5454tsc000izsv56vriv0gs"},{"post_id":"ck5454trt0003zsv5pykx7evg","category_id":"ck5454ts5000czsv5nf2rp8gh","_id":"ck5454tsh000nzsv5f40j0efl"},{"post_id":"ck5454trx0007zsv5yp03gkt9","category_id":"ck5454tsc000jzsv5pa78y7d8","_id":"ck5454tsq000vzsv5pnqnkb25"},{"post_id":"ck5454ts20009zsv5criei4mw","category_id":"ck5454ts5000czsv5nf2rp8gh","_id":"ck5454tsy0011zsv5stnawrl1"},{"post_id":"ck5454ts5000bzsv54pepxk8o","category_id":"ck5454ts5000czsv5nf2rp8gh","_id":"ck5454tt50014zsv5bwo1w693"},{"post_id":"ck5454ts8000fzsv5130vsj44","category_id":"ck5454tsy0010zsv57q845ngx","_id":"ck5454ttf0018zsv5oafj3ncs"},{"post_id":"ck5454tsb000hzsv5hgxv5hke","category_id":"ck5454tt50015zsv589ouwyv6","_id":"ck5454tti001ezsv51m0nywl1"},{"post_id":"ck5454tse000lzsv5bvsgyyi2","category_id":"ck5454tt50015zsv589ouwyv6","_id":"ck5454ttj001gzsv5t8kkh2jt"},{"post_id":"ck5454tsg000mzsv5mtkyj6zd","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454ttm001lzsv5ei4k6t7g"},{"post_id":"ck5454tsl000qzsv5cr7lezre","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454tto001qzsv54po42vbg"},{"post_id":"ck5454tso000szsv5npcie8vs","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454ttq001vzsv5sl3agdq6"},{"post_id":"ck5454tss000xzsv58rjg2wct","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454ttr001yzsv5xlitgud4"},{"post_id":"ck5454tsv000zzsv574n6a04y","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454tts0020zsv565cdvu8z"},{"post_id":"ck5454tsz0012zsv5c1yzmnay","category_id":"ck5454tti001dzsv58fqvy6e7","_id":"ck5454ttu0024zsv5agzanpy4"},{"post_id":"ck5454tuk0034zsv5ijho38pk","category_id":"ck5454trv0005zsv54or2iw1b","_id":"ck5454tuo0036zsv5di9bfohk"}],"PostTag":[{"post_id":"ck5454tro0001zsv5lpyfdiwc","tag_id":"ck5454trx0006zsv5ds4vajl6","_id":"ck5454tsl000rzsv51yjszglu"},{"post_id":"ck5454tro0001zsv5lpyfdiwc","tag_id":"ck5454ts6000dzsv5mwxfz3e2","_id":"ck5454tso000tzsv5xj96dfee"},{"post_id":"ck5454tro0001zsv5lpyfdiwc","tag_id":"ck5454tsd000kzsv588tg93sg","_id":"ck5454tst000yzsv5b3vsse43"},{"post_id":"ck5454trt0003zsv5pykx7evg","tag_id":"ck5454tsj000pzsv5qn35eyya","_id":"ck5454ttd0017zsv5cex4ofq7"},{"post_id":"ck5454trt0003zsv5pykx7evg","tag_id":"ck5454tsr000wzsv5mbks623x","_id":"ck5454tth001azsv5z9b6igdm"},{"post_id":"ck5454trt0003zsv5pykx7evg","tag_id":"ck5454tt00013zsv5sicv0d0z","_id":"ck5454tti001czsv5qrr0difd"},{"post_id":"ck5454trx0007zsv5yp03gkt9","tag_id":"ck5454tt70016zsv54fvg89mo","_id":"ck5454ttl001jzsv568wmvzwe"},{"post_id":"ck5454trx0007zsv5yp03gkt9","tag_id":"ck5454tth001bzsv5sppcbddm","_id":"ck5454ttm001mzsv5fjcqzdz4"},{"post_id":"ck5454trx0007zsv5yp03gkt9","tag_id":"ck5454ttj001fzsv5e0bwntom","_id":"ck5454ttn001ozsv5rclbucwn"},{"post_id":"ck5454ts20009zsv5criei4mw","tag_id":"ck5454tsj000pzsv5qn35eyya","_id":"ck5454tto001rzsv54p2xfmjt"},{"post_id":"ck5454ts5000bzsv54pepxk8o","tag_id":"ck5454tsj000pzsv5qn35eyya","_id":"ck5454ttp001tzsv5q05mnwmf"},{"post_id":"ck5454ts8000fzsv5130vsj44","tag_id":"ck5454ttp001szsv5hk250a7x","_id":"ck5454ttt0021zsv50t93j1by"},{"post_id":"ck5454ts8000fzsv5130vsj44","tag_id":"ck5454ttq001wzsv5as9p0cqa","_id":"ck5454ttt0022zsv57isentfj"},{"post_id":"ck5454tsb000hzsv5hgxv5hke","tag_id":"ck5454tts001zzsv5tn5wkozq","_id":"ck5454ttw0027zsv5zzbohpl9"},{"post_id":"ck5454tsb000hzsv5hgxv5hke","tag_id":"ck5454ttt0023zsv5y16psnxb","_id":"ck5454ttx0028zsv5lrw7mkau"},{"post_id":"ck5454tsb000hzsv5hgxv5hke","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tty002azsv5jm0ceoh2"},{"post_id":"ck5454tse000lzsv5bvsgyyi2","tag_id":"ck5454ttt0023zsv5y16psnxb","_id":"ck5454ttz002czsv5s1wk2e2t"},{"post_id":"ck5454tse000lzsv5bvsgyyi2","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454ttz002dzsv5eg799otr"},{"post_id":"ck5454tsg000mzsv5mtkyj6zd","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tu2002hzsv592roazed"},{"post_id":"ck5454tsg000mzsv5mtkyj6zd","tag_id":"ck5454ttt0023zsv5y16psnxb","_id":"ck5454tu2002izsv5d3p5xwe6"},{"post_id":"ck5454tsg000mzsv5mtkyj6zd","tag_id":"ck5454tu0002fzsv5g1qct1c8","_id":"ck5454tu3002kzsv5agufiobu"},{"post_id":"ck5454tsl000qzsv5cr7lezre","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tu4002mzsv5tla4mgh7"},{"post_id":"ck5454tsl000qzsv5cr7lezre","tag_id":"ck5454tu0002fzsv5g1qct1c8","_id":"ck5454tu5002nzsv5mhposfiz"},{"post_id":"ck5454tso000szsv5npcie8vs","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tu6002pzsv5jwumdkok"},{"post_id":"ck5454tso000szsv5npcie8vs","tag_id":"ck5454tsj000pzsv5qn35eyya","_id":"ck5454tu6002qzsv55aql9af8"},{"post_id":"ck5454tss000xzsv58rjg2wct","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tu7002tzsv5tl8p8cwd"},{"post_id":"ck5454tss000xzsv58rjg2wct","tag_id":"ck5454tu0002fzsv5g1qct1c8","_id":"ck5454tu8002uzsv5ymeqbid6"},{"post_id":"ck5454tsv000zzsv574n6a04y","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tu9002xzsv5fk38bvde"},{"post_id":"ck5454tsv000zzsv574n6a04y","tag_id":"ck5454tu0002fzsv5g1qct1c8","_id":"ck5454tu9002yzsv5937qcu3d"},{"post_id":"ck5454tsz0012zsv5c1yzmnay","tag_id":"ck5454ttu0025zsv5sae87n0v","_id":"ck5454tua0031zsv5xrjwglbx"},{"post_id":"ck5454tsz0012zsv5c1yzmnay","tag_id":"ck5454ttt0023zsv5y16psnxb","_id":"ck5454tua0032zsv513ve3u9j"},{"post_id":"ck5454tsz0012zsv5c1yzmnay","tag_id":"ck5454tua0030zsv5dotx0lxj","_id":"ck5454tua0033zsv5jcn1uany"},{"post_id":"ck5454tuk0034zsv5ijho38pk","tag_id":"ck5454tum0035zsv5n9r3kcvx","_id":"ck5454tuu0037zsv5a9xnfv8b"},{"post_id":"ck5454tuk0034zsv5ijho38pk","tag_id":"ck5454tsd000kzsv588tg93sg","_id":"ck5454tuw0038zsv5dfs4cei6"}],"Tag":[{"name":"CMake","_id":"ck5454trx0006zsv5ds4vajl6"},{"name":"Make","_id":"ck5454ts6000dzsv5mwxfz3e2"},{"name":"C++","_id":"ck5454tsd000kzsv588tg93sg"},{"name":"python","_id":"ck5454tsj000pzsv5qn35eyya"},{"name":"math","_id":"ck5454tsr000wzsv5mbks623x"},{"name":"sympy","_id":"ck5454tt00013zsv5sicv0d0z"},{"name":"Github","_id":"ck5454tt70016zsv54fvg89mo"},{"name":"Hexo","_id":"ck5454tth001bzsv5sppcbddm"},{"name":"node.js","_id":"ck5454ttj001fzsv5e0bwntom"},{"name":"Tree","_id":"ck5454ttp001szsv5hk250a7x"},{"name":"C/C++","_id":"ck5454ttq001wzsv5as9p0cqa"},{"name":"AI","_id":"ck5454tts001zzsv5tn5wkozq"},{"name":"ML","_id":"ck5454ttt0023zsv5y16psnxb"},{"name":"DL","_id":"ck5454ttu0025zsv5sae87n0v"},{"name":"Python","_id":"ck5454tu0002fzsv5g1qct1c8"},{"name":"ANN","_id":"ck5454tua0030zsv5dotx0lxj"},{"name":"C","_id":"ck5454tum0035zsv5n9r3kcvx"}]}}